{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "print(load_dotenv(find_dotenv(), override=True))\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo-0301'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\"\n",
    "\n",
    "llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "operator.itemgetter('language')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "itemgetter(\"language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': operator.itemgetter('language')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"language\": itemgetter(\"language\")} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['city', 'language'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['city', 'language'], template='what country is the city {city} in? respond in {language}'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(\"what country is the city {city} in? respond in {language}\")\n",
    "prompt2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m {\u001b[39m\"\u001b[39;49m\u001b[39mcity\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mseoul\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlanguage\u001b[39;49m\u001b[39m\"\u001b[39;49m: itemgetter(\u001b[39m\"\u001b[39;49m\u001b[39mlanguage\u001b[39;49m\u001b[39m\"\u001b[39;49m)} \u001b[39m|\u001b[39;49m prompt2\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:260\u001b[0m, in \u001b[0;36mRunnable.__ror__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__ror__\u001b[39m(\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    252\u001b[0m     other: Union[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m     ],\n\u001b[0;32m    258\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Runnable[Other, Output]:\n\u001b[0;32m    259\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compose this runnable with another object to create a RunnableSequence.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m     \u001b[39mreturn\u001b[39;00m RunnableSequence(first\u001b[39m=\u001b[39mcoerce_to_runnable(other), last\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:2525\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[1;34m(thing)\u001b[0m\n\u001b[0;32m   2523\u001b[0m     \u001b[39mreturn\u001b[39;00m RunnableLambda(cast(Callable[[Input], Output], thing))\n\u001b[0;32m   2524\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(thing, \u001b[39mdict\u001b[39m):\n\u001b[1;32m-> 2525\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[0;32m   2526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2527\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   2528\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a Runnable, callable or dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2529\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInstead got an unsupported type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(thing)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2530\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1530\u001b[0m, in \u001b[0;36mRunnableParallel.__init__\u001b[1;34m(self, _RunnableParallel__steps, **kwargs)\u001b[0m\n\u001b[0;32m   1527\u001b[0m merged \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m__steps} \u001b[39mif\u001b[39;00m __steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m   1528\u001b[0m merged\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m-> 1530\u001b[0m     steps\u001b[39m=\u001b[39m{key: coerce_to_runnable(r) \u001b[39mfor\u001b[39;00m key, r \u001b[39min\u001b[39;00m merged\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m   1531\u001b[0m )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1530\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1527\u001b[0m merged \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m__steps} \u001b[39mif\u001b[39;00m __steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m   1528\u001b[0m merged\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m-> 1530\u001b[0m     steps\u001b[39m=\u001b[39m{key: coerce_to_runnable(r) \u001b[39mfor\u001b[39;00m key, r \u001b[39min\u001b[39;00m merged\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m   1531\u001b[0m )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:2527\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[1;34m(thing)\u001b[0m\n\u001b[0;32m   2525\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[0;32m   2526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2527\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   2528\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a Runnable, callable or dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2529\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInstead got an unsupported type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(thing)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2530\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "{\"city\": \"seoul\", \"language\": itemgetter(\"language\")} | prompt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 결과 보니, city의 value가 callable or dict가 아니라 string이라 위에서 오류 발생했던 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m {\u001b[39m\"\u001b[39;49m\u001b[39mcity\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mseoul\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlanguage\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mkorean\u001b[39;49m\u001b[39m'\u001b[39;49m} \u001b[39m|\u001b[39;49m prompt2\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:260\u001b[0m, in \u001b[0;36mRunnable.__ror__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__ror__\u001b[39m(\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    252\u001b[0m     other: Union[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m     ],\n\u001b[0;32m    258\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Runnable[Other, Output]:\n\u001b[0;32m    259\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compose this runnable with another object to create a RunnableSequence.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m     \u001b[39mreturn\u001b[39;00m RunnableSequence(first\u001b[39m=\u001b[39mcoerce_to_runnable(other), last\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:2525\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[1;34m(thing)\u001b[0m\n\u001b[0;32m   2523\u001b[0m     \u001b[39mreturn\u001b[39;00m RunnableLambda(cast(Callable[[Input], Output], thing))\n\u001b[0;32m   2524\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(thing, \u001b[39mdict\u001b[39m):\n\u001b[1;32m-> 2525\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[0;32m   2526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2527\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   2528\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a Runnable, callable or dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2529\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInstead got an unsupported type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(thing)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2530\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1530\u001b[0m, in \u001b[0;36mRunnableParallel.__init__\u001b[1;34m(self, _RunnableParallel__steps, **kwargs)\u001b[0m\n\u001b[0;32m   1527\u001b[0m merged \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m__steps} \u001b[39mif\u001b[39;00m __steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m   1528\u001b[0m merged\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m-> 1530\u001b[0m     steps\u001b[39m=\u001b[39m{key: coerce_to_runnable(r) \u001b[39mfor\u001b[39;00m key, r \u001b[39min\u001b[39;00m merged\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m   1531\u001b[0m )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1530\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1527\u001b[0m merged \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m__steps} \u001b[39mif\u001b[39;00m __steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m   1528\u001b[0m merged\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m-> 1530\u001b[0m     steps\u001b[39m=\u001b[39m{key: coerce_to_runnable(r) \u001b[39mfor\u001b[39;00m key, r \u001b[39min\u001b[39;00m merged\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m   1531\u001b[0m )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:2527\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[1;34m(thing)\u001b[0m\n\u001b[0;32m   2525\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[0;32m   2526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2527\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   2528\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a Runnable, callable or dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2529\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInstead got an unsupported type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(thing)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2530\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "{\"city\": \"seoul\", \"language\": 'korean'} | prompt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예상이 맞네"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  city: RunnableLambda(...)\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['city'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['city'], template='what country is the city {city} in?'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompt = ChatPromptTemplate.from_template(\"what country is the city {city} in?\")\n",
    "{\"city\": itemgetter(\"seoul\")} | test_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현재 고객 문의 메시지만 고려한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = PromptTemplate.from_template(\"\"\"\n",
    "아래 고객의 메시지를 보고, 이 메시지가 \"일반\", \"주문 변경\", \"주문 취소\" 중 어디에 해당되는지 분류해줘.\n",
    "\"일반\"이란 범주는 일반 문의 또는 주문에 관한 내용을 의미해.\n",
    "\n",
    "제시된 범주(\"일반\", \"주문 변경\", \"주문 취소\") 가운데 하나로 분류해야돼.\n",
    "\n",
    "<고객 메시지>\n",
    "{message}\n",
    "</고객 메시지>\n",
    "\n",
    "Classification:\"\"\") | ChatOpenAI(temperature=0, model=llm_model) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'일반'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'message': \"무지개 떡은 가격이 얼마인가요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'일반'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'message': \"판매하는 상품정보 알수있을까요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주문 변경'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'message': \"추가 주문 좀 할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주문 취소'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'message': \"사정이 생겨서 환불해야할 거 같아요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_query_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "order_change_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 변경을 전담하는 종업원이야.\n",
    "고객이 변경한 주문 내용을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 변경 내용이 잘못됐다면, 주문 변경 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 변경 내용을 정확히 파악할 때까지 반복해야돼.\n",
    "고객의 주문 변경을 정확히 파악했다면, 고객에게 고객이 주문을 변경한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 변경된 주문의 총 가격을 알려줘.\n",
    "\n",
    "\n",
    "\n",
    "고객의 주문 변경은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "order_cancel_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 취소를 전담하는 종업원이야.\n",
    "고객이 취소하려는 주문을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 취소 내용이 잘못됐다면, 주문 취소 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 취소 내용을 정확히 파악할 때\n",
    "고객의 주문 취소 내용을 정확히 파악했다면, 고객에게 고객이 주문을 취소한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 취소된 주문의 총 가격을 알려줘.\n",
    "\n",
    "\n",
    "고객이 취소하려는 주문은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "general_chain = PromptTemplate.from_template(\"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RunnableBranch default must be runnable, callable or mapping.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrunnable\u001b[39;00m \u001b[39mimport\u001b[39;00m RunnableBranch\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m branch \u001b[39m=\u001b[39m RunnableBranch(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   (\u001b[39mlambda\u001b[39;49;00m x: \u001b[39m\"\u001b[39;49m\u001b[39m일반\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39min\u001b[39;49;00m x[\u001b[39m\"\u001b[39;49m\u001b[39mtopic\u001b[39;49m\u001b[39m\"\u001b[39;49m], general_query_chain),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   (\u001b[39mlambda\u001b[39;49;00m x: \u001b[39m\"\u001b[39;49m\u001b[39m주문 변경\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39min\u001b[39;49;00m x[\u001b[39m\"\u001b[39;49m\u001b[39mtopic\u001b[39;49m\u001b[39m\"\u001b[39;49m], order_change_chain),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   (\u001b[39mlambda\u001b[39;49;00m x: \u001b[39m\"\u001b[39;49m\u001b[39m주문 취소\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39min\u001b[39;49;00m x[\u001b[39m\"\u001b[39;49m\u001b[39mtopic\u001b[39;49m\u001b[39m\"\u001b[39;49m], order_cancel_chain),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\branch.py:91\u001b[0m, in \u001b[0;36mRunnableBranch.__init__\u001b[1;34m(self, *branches)\u001b[0m\n\u001b[0;32m     86\u001b[0m default \u001b[39m=\u001b[39m branches[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m     89\u001b[0m     default, (Runnable, Callable, Mapping)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     90\u001b[0m ):\n\u001b[1;32m---> 91\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m     92\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRunnableBranch default must be runnable, callable or mapping.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[0;32m     95\u001b[0m default_ \u001b[39m=\u001b[39m cast(\n\u001b[0;32m     96\u001b[0m     Runnable[Input, Output], coerce_to_runnable(cast(RunnableLike, default))\n\u001b[0;32m     97\u001b[0m )\n\u001b[0;32m     99\u001b[0m _branches \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: RunnableBranch default must be runnable, callable or mapping."
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "  (lambda x: \"일반\" in x[\"topic\"], general_query_chain),\n",
    "  (lambda x: \"주문 변경\" in x[\"topic\"], order_change_chain),\n",
    "  (lambda x: \"주문 취소\" in x[\"topic\"], order_cancel_chain),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 저희 가게에서는 떡케익5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있습니다. 어떤 상품을 찾으시나요? 제가 도움을 드릴게요.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_query_chain.invoke({\"message\": \"어떤 상품이 있는지 좀 알 수 있을까요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 어떤 부분을 변경하시겠어요? 변경 내용을 알려주세요.')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_change_chain.invoke({\"message\": \"주문 좀 변경할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='고객님, 주문하신 무지개 백설기를 취소하시겠다고 하셨군요. 확인해보니 해당 상품은 1개 주문하셨고, 가격은 5,000원입니다. 취소된 주문의 총 가격은 5,000원이 되겠습니다. 맞으시면 확인 부탁드립니다.')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환각 답변\n",
    "\n",
    "order_cancel_chain.invoke({\"message\": \"무지개 백설기는 취소할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chain = PromptTemplate.from_template(\"\"\"You are an expert in langchain. \\\n",
    "Always answer questions starting with \"As Harrison Chase told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "anthropic_chain = PromptTemplate.from_template(\"\"\"You are an expert in anthropic. \\\n",
    "Always answer questions starting with \"As Dario Amodei told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "general_chain = PromptTemplate.from_template(\"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "  (lambda x: \"anthropic\" in x[\"topic\"].lower(), anthropic_chain),\n",
    "  (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),\n",
    "  general_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "  (lambda x: \"일반\" in x[\"topic\"], general_query_chain),\n",
    "  (lambda x: \"주문 변경\" in x[\"topic\"], order_change_chain),\n",
    "  (lambda x: \"주문 취소\" in x[\"topic\"], order_cancel_chain),\n",
    "  general_chain # 이게 default branch로 설정되는 듯, 일반을 그냥 기본 chain으로 만들면 될 듯\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = {\n",
    "    \"topic\": chain,\n",
    "    \"message\": lambda x: x[\"message\"]\n",
    "} | branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch = RunnableBranch(\n",
    "  (lambda x: \"주문 변경\" in x[\"topic\"], order_change_chain),\n",
    "  (lambda x: \"주문 취소\" in x[\"topic\"], order_cancel_chain),\n",
    "  general_query_chain,\n",
    ")\n",
    "\n",
    "full_chain = {\n",
    "    \"topic\": chain,\n",
    "    \"message\": lambda x: x[\"message\"]\n",
    "} | branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 동작 시험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 물론입니다. 저희 가게에서는 떡케익 5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있습니다. 각 상품의 기본 판매 수량과 가격은 다음과 같습니다. 떡케익 5호는 1개에 54,000원, 무지개 백설기 케익은 1개에 51,500원, 미니 백설기는 35개에 31,500원, 그리고 개별 모듬팩은 1개에 13,500원입니다. 어떤 상품을 원하시나요?')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"주문 하기 전에 상품 정보 좀 알 수 있을까요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 떡케익 5호는 기본 판매 수량이 1개이며 가격은 54,000원입니다. 개별 모듬팩은 기본 판매 수량이 1개이며 가격은 13,500원입니다. 따라서 떡케익 2개와 개별 모듬팩 4개를 구매하시면 총 가격은 54,000원 x 2 + 13,500원 x 4 = 151,500원이 됩니다. 주문하실까요?')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"떡케익2개, 개별 모듬팩 4개할게요. 얼마죠?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 고객님. 떡케익 5호는 기본 판매 수량이 1개이며 가격은 54,000원입니다. 미니 백설기는 기본 판매 수량이 35개이며 가격은 31,500원입니다. 고객님께서 원하시는 떡케익 2개와 미니 백설기 2개를 주문하시면 총 가격은 152,400원이 됩니다. 주문하시겠어요?')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"아 그냥 떡케익2개, 미니 백설기 2개할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 어떤 부분을 변경하시겠어요? 변경 내용을 알려주세요.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"주문 좀 변경할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='고객님, 이해했습니다. 주문 취소가 완료되었습니다. 취소된 상품의 이름, 수량, 가격은 확인하셨나요? 취소된 주문의 총 가격은 XX원입니다. 감사합니다.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"아까 주문했던 거 그냥 안 할게요\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메모리 추가"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "order_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
