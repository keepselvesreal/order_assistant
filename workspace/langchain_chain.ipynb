{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "print(load_dotenv(find_dotenv(), override=True))\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo-0301'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\"\n",
    "\n",
    "llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "operator.itemgetter('language')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "itemgetter(\"language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': operator.itemgetter('language')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"language\": itemgetter(\"language\")} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['city', 'language'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['city', 'language'], template='what country is the city {city} in? respond in {language}'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(\"what country is the city {city} in? respond in {language}\")\n",
    "prompt2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m {\u001b[39m\"\u001b[39;49m\u001b[39mcity\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mseoul\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlanguage\u001b[39;49m\u001b[39m\"\u001b[39;49m: itemgetter(\u001b[39m\"\u001b[39;49m\u001b[39mlanguage\u001b[39;49m\u001b[39m\"\u001b[39;49m)} \u001b[39m|\u001b[39;49m prompt2\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:260\u001b[0m, in \u001b[0;36mRunnable.__ror__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__ror__\u001b[39m(\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    252\u001b[0m     other: Union[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m     ],\n\u001b[0;32m    258\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Runnable[Other, Output]:\n\u001b[0;32m    259\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compose this runnable with another object to create a RunnableSequence.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m     \u001b[39mreturn\u001b[39;00m RunnableSequence(first\u001b[39m=\u001b[39mcoerce_to_runnable(other), last\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:2525\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[1;34m(thing)\u001b[0m\n\u001b[0;32m   2523\u001b[0m     \u001b[39mreturn\u001b[39;00m RunnableLambda(cast(Callable[[Input], Output], thing))\n\u001b[0;32m   2524\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(thing, \u001b[39mdict\u001b[39m):\n\u001b[1;32m-> 2525\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[0;32m   2526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2527\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   2528\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a Runnable, callable or dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2529\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInstead got an unsupported type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(thing)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2530\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1530\u001b[0m, in \u001b[0;36mRunnableParallel.__init__\u001b[1;34m(self, _RunnableParallel__steps, **kwargs)\u001b[0m\n\u001b[0;32m   1527\u001b[0m merged \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m__steps} \u001b[39mif\u001b[39;00m __steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m   1528\u001b[0m merged\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m-> 1530\u001b[0m     steps\u001b[39m=\u001b[39m{key: coerce_to_runnable(r) \u001b[39mfor\u001b[39;00m key, r \u001b[39min\u001b[39;00m merged\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m   1531\u001b[0m )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1530\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1527\u001b[0m merged \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m__steps} \u001b[39mif\u001b[39;00m __steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m   1528\u001b[0m merged\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m-> 1530\u001b[0m     steps\u001b[39m=\u001b[39m{key: coerce_to_runnable(r) \u001b[39mfor\u001b[39;00m key, r \u001b[39min\u001b[39;00m merged\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m   1531\u001b[0m )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:2527\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[1;34m(thing)\u001b[0m\n\u001b[0;32m   2525\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[0;32m   2526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2527\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   2528\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a Runnable, callable or dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2529\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInstead got an unsupported type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(thing)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2530\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "{\"city\": \"seoul\", \"language\": itemgetter(\"language\")} | prompt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 결과 보니, city의 value가 callable or dict가 아니라 string이라 위에서 오류 발생했던 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m {\u001b[39m\"\u001b[39;49m\u001b[39mcity\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mseoul\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlanguage\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mkorean\u001b[39;49m\u001b[39m'\u001b[39;49m} \u001b[39m|\u001b[39;49m prompt2\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:260\u001b[0m, in \u001b[0;36mRunnable.__ror__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__ror__\u001b[39m(\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    252\u001b[0m     other: Union[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m     ],\n\u001b[0;32m    258\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Runnable[Other, Output]:\n\u001b[0;32m    259\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compose this runnable with another object to create a RunnableSequence.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m     \u001b[39mreturn\u001b[39;00m RunnableSequence(first\u001b[39m=\u001b[39mcoerce_to_runnable(other), last\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:2525\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[1;34m(thing)\u001b[0m\n\u001b[0;32m   2523\u001b[0m     \u001b[39mreturn\u001b[39;00m RunnableLambda(cast(Callable[[Input], Output], thing))\n\u001b[0;32m   2524\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(thing, \u001b[39mdict\u001b[39m):\n\u001b[1;32m-> 2525\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[0;32m   2526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2527\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   2528\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a Runnable, callable or dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2529\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInstead got an unsupported type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(thing)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2530\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1530\u001b[0m, in \u001b[0;36mRunnableParallel.__init__\u001b[1;34m(self, _RunnableParallel__steps, **kwargs)\u001b[0m\n\u001b[0;32m   1527\u001b[0m merged \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m__steps} \u001b[39mif\u001b[39;00m __steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m   1528\u001b[0m merged\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m-> 1530\u001b[0m     steps\u001b[39m=\u001b[39m{key: coerce_to_runnable(r) \u001b[39mfor\u001b[39;00m key, r \u001b[39min\u001b[39;00m merged\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m   1531\u001b[0m )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1530\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1527\u001b[0m merged \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m__steps} \u001b[39mif\u001b[39;00m __steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m   1528\u001b[0m merged\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m-> 1530\u001b[0m     steps\u001b[39m=\u001b[39m{key: coerce_to_runnable(r) \u001b[39mfor\u001b[39;00m key, r \u001b[39min\u001b[39;00m merged\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m   1531\u001b[0m )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:2527\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[1;34m(thing)\u001b[0m\n\u001b[0;32m   2525\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[0;32m   2526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2527\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   2528\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a Runnable, callable or dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2529\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInstead got an unsupported type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(thing)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2530\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "{\"city\": \"seoul\", \"language\": 'korean'} | prompt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예상이 맞네"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  city: RunnableLambda(...)\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['city'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['city'], template='what country is the city {city} in?'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompt = ChatPromptTemplate.from_template(\"what country is the city {city} in?\")\n",
    "{\"city\": itemgetter(\"seoul\")} | test_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현재 고객 문의 메시지만 고려한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = PromptTemplate.from_template(\"\"\"\n",
    "아래 고객의 메시지를 보고, 이 메시지가 \"일반\", \"주문 변경\", \"주문 취소\" 중 어디에 해당되는지 분류해줘.\n",
    "\"일반\"이란 범주는 일반 문의 또는 주문에 관한 내용을 의미해.\n",
    "\n",
    "제시된 범주(\"일반\", \"주문 변경\", \"주문 취소\") 가운데 하나로 분류해야돼.\n",
    "\n",
    "<고객 메시지>\n",
    "{message}\n",
    "</고객 메시지>\n",
    "\n",
    "Classification:\"\"\") | ChatOpenAI(temperature=0, model=llm_model) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'일반'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'message': \"무지개 떡은 가격이 얼마인가요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'일반'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'message': \"판매하는 상품정보 알수있을까요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주문 변경'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'message': \"추가 주문 좀 할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주문 취소'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'message': \"사정이 생겨서 환불해야할 거 같아요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_query_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "order_change_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 변경을 전담하는 종업원이야.\n",
    "고객이 변경한 주문 내용을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 변경 내용이 잘못됐다면, 주문 변경 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 변경 내용을 정확히 파악할 때까지 반복해야돼.\n",
    "고객의 주문 변경을 정확히 파악했다면, 고객에게 고객이 주문을 변경한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 변경된 주문의 총 가격을 알려줘.\n",
    "\n",
    "\n",
    "\n",
    "고객의 주문 변경은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "order_cancel_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 취소를 전담하는 종업원이야.\n",
    "고객이 취소하려는 주문을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 취소 내용이 잘못됐다면, 주문 취소 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 취소 내용을 정확히 파악할 때\n",
    "고객의 주문 취소 내용을 정확히 파악했다면, 고객에게 고객이 주문을 취소한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 취소된 주문의 총 가격을 알려줘.\n",
    "\n",
    "\n",
    "고객이 취소하려는 주문은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "general_chain = PromptTemplate.from_template(\"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RunnableBranch default must be runnable, callable or mapping.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrunnable\u001b[39;00m \u001b[39mimport\u001b[39;00m RunnableBranch\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m branch \u001b[39m=\u001b[39m RunnableBranch(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   (\u001b[39mlambda\u001b[39;49;00m x: \u001b[39m\"\u001b[39;49m\u001b[39m일반\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39min\u001b[39;49;00m x[\u001b[39m\"\u001b[39;49m\u001b[39mtopic\u001b[39;49m\u001b[39m\"\u001b[39;49m], general_query_chain),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   (\u001b[39mlambda\u001b[39;49;00m x: \u001b[39m\"\u001b[39;49m\u001b[39m주문 변경\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39min\u001b[39;49;00m x[\u001b[39m\"\u001b[39;49m\u001b[39mtopic\u001b[39;49m\u001b[39m\"\u001b[39;49m], order_change_chain),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   (\u001b[39mlambda\u001b[39;49;00m x: \u001b[39m\"\u001b[39;49m\u001b[39m주문 취소\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39min\u001b[39;49;00m x[\u001b[39m\"\u001b[39;49m\u001b[39mtopic\u001b[39;49m\u001b[39m\"\u001b[39;49m], order_cancel_chain),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\branch.py:91\u001b[0m, in \u001b[0;36mRunnableBranch.__init__\u001b[1;34m(self, *branches)\u001b[0m\n\u001b[0;32m     86\u001b[0m default \u001b[39m=\u001b[39m branches[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m     89\u001b[0m     default, (Runnable, Callable, Mapping)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     90\u001b[0m ):\n\u001b[1;32m---> 91\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m     92\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRunnableBranch default must be runnable, callable or mapping.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[0;32m     95\u001b[0m default_ \u001b[39m=\u001b[39m cast(\n\u001b[0;32m     96\u001b[0m     Runnable[Input, Output], coerce_to_runnable(cast(RunnableLike, default))\n\u001b[0;32m     97\u001b[0m )\n\u001b[0;32m     99\u001b[0m _branches \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: RunnableBranch default must be runnable, callable or mapping."
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "  (lambda x: \"일반\" in x[\"topic\"], general_query_chain),\n",
    "  (lambda x: \"주문 변경\" in x[\"topic\"], order_change_chain),\n",
    "  (lambda x: \"주문 취소\" in x[\"topic\"], order_cancel_chain),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 저희 가게에서는 떡케익5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있습니다. 어떤 상품을 찾으시나요? 제가 도움을 드릴게요.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_query_chain.invoke({\"message\": \"어떤 상품이 있는지 좀 알 수 있을까요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 어떤 부분을 변경하시겠어요? 변경 내용을 알려주세요.')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_change_chain.invoke({\"message\": \"주문 좀 변경할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='고객님, 주문하신 무지개 백설기를 취소하시겠다고 하셨군요. 확인해보니 해당 상품은 1개 주문하셨고, 가격은 5,000원입니다. 취소된 주문의 총 가격은 5,000원이 되겠습니다. 맞으시면 확인 부탁드립니다.')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환각 답변\n",
    "\n",
    "order_cancel_chain.invoke({\"message\": \"무지개 백설기는 취소할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chain = PromptTemplate.from_template(\"\"\"You are an expert in langchain. \\\n",
    "Always answer questions starting with \"As Harrison Chase told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "anthropic_chain = PromptTemplate.from_template(\"\"\"You are an expert in anthropic. \\\n",
    "Always answer questions starting with \"As Dario Amodei told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "general_chain = PromptTemplate.from_template(\"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "  (lambda x: \"anthropic\" in x[\"topic\"].lower(), anthropic_chain),\n",
    "  (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),\n",
    "  general_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "  (lambda x: \"일반\" in x[\"topic\"], general_query_chain),\n",
    "  (lambda x: \"주문 변경\" in x[\"topic\"], order_change_chain),\n",
    "  (lambda x: \"주문 취소\" in x[\"topic\"], order_cancel_chain),\n",
    "  general_chain # 이게 default branch로 설정되는 듯, 일반을 그냥 기본 chain으로 만들면 될 듯\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = {\n",
    "    \"topic\": chain,\n",
    "    \"message\": lambda x: x[\"message\"]\n",
    "} | branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch = RunnableBranch(\n",
    "  (lambda x: \"주문 변경\" in x[\"topic\"], order_change_chain),\n",
    "  (lambda x: \"주문 취소\" in x[\"topic\"], order_cancel_chain),\n",
    "  general_query_chain,\n",
    ")\n",
    "\n",
    "full_chain = {\n",
    "    \"topic\": chain,\n",
    "    \"message\": lambda x: x[\"message\"]\n",
    "} | branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 동작 시험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 물론입니다. 저희 가게에서는 떡케익 5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있습니다. 각 상품의 기본 판매 수량과 가격은 다음과 같습니다. 떡케익 5호는 1개에 54,000원, 무지개 백설기 케익은 1개에 51,500원, 미니 백설기는 35개에 31,500원, 그리고 개별 모듬팩은 1개에 13,500원입니다. 어떤 상품을 원하시나요?')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"주문 하기 전에 상품 정보 좀 알 수 있을까요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 떡케익 5호는 기본 판매 수량이 1개이며 가격은 54,000원입니다. 개별 모듬팩은 기본 판매 수량이 1개이며 가격은 13,500원입니다. 따라서 떡케익 2개와 개별 모듬팩 4개를 구매하시면 총 가격은 54,000원 x 2 + 13,500원 x 4 = 151,500원이 됩니다. 주문하실까요?')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"떡케익2개, 개별 모듬팩 4개할게요. 얼마죠?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 고객님. 떡케익 5호는 기본 판매 수량이 1개이며 가격은 54,000원입니다. 미니 백설기는 기본 판매 수량이 35개이며 가격은 31,500원입니다. 고객님께서 원하시는 떡케익 2개와 미니 백설기 2개를 주문하시면 총 가격은 152,400원이 됩니다. 주문하시겠어요?')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"아 그냥 떡케익2개, 미니 백설기 2개할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 어떤 부분을 변경하시겠어요? 변경 내용을 알려주세요.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"주문 좀 변경할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='고객님, 이해했습니다. 주문 취소가 완료되었습니다. 취소된 상품의 이름, 수량, 가격은 확인하셨나요? 취소된 주문의 총 가격은 XX원입니다. 감사합니다.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"아까 주문했던 거 그냥 안 할게요\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메모리 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    CombinedMemory,\n",
    "    ConversationSummaryMemory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PromptTemplate.from_template에는 input_variables 파라미터가 없나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_query_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\n",
    "이전 대화는 다음과 같아\n",
    "{chat_history}\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "order_change_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 변경을 전담하는 종업원이야.\n",
    "고객이 변경한 주문 내용을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 변경 내용이 잘못됐다면, 주문 변경 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 변경 내용을 정확히 파악할 때까지 반복해야돼.\n",
    "고객의 주문 변경을 정확히 파악했다면, 고객에게 고객이 주문을 변경한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 변경된 주문의 총 가격을 알려줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "이전 대화는 다음과 같아\n",
    "{chat_history}\n",
    "\n",
    "고객의 주문 변경은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "order_cancel_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 취소를 전담하는 종업원이야.\n",
    "고객이 취소하려는 주문을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 취소 내용이 잘못됐다면, 주문 취소 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 취소 내용을 정확히 파악할 때\n",
    "고객의 주문 취소 내용을 정확히 파악했다면, 고객에게 고객이 주문을 취소한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 취소된 주문의 총 가격을 알려줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "이전 대화는 다음과 같아\n",
    "{chat_history}\n",
    "\n",
    "고객이 취소하려는 주문은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "# general_chain = PromptTemplate.from_template(\"\"\"Respond to the following question:\n",
    "\n",
    "# Question: {question}\n",
    "# Answer:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "branch = RunnableBranch(\n",
    "  (lambda x: \"주문 변경\" in x[\"topic\"], order_change_chain),\n",
    "  (lambda x: \"주문 취소\" in x[\"topic\"], order_cancel_chain),\n",
    "  general_query_chain,\n",
    ")\n",
    "\n",
    "full_chain = {\n",
    "    \"topic\": chain,\n",
    "    \"message\": lambda x: x[\"message\"]\n",
    "} | branch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\n",
    "이전 대화는 다음과 같아\n",
    "{chat_history}\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\"\n",
    "\n",
    "general_query_prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"message\"], template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['chat_history', 'message'], template='너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\\n가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\\n자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\\n이전 대화를 고려하여 답변해야 해.\\n\\n가게에서 판매하는 상품 목록.\\n1. 상품: 떡케익5호\\n   기본 판매 수량: 1개\\n   기본 판매 수량의 가격: 54,000원\\n2. 상품: 무지개 백설기 케익\\n   기본 판매 수량: 1개\\n   기본 판매 수량의 가격: 51,500원\\n3. 상품: 미니 백설기\\n   기본 판매 수량: 35개\\n   기본 판매 수량의 가격: 31,500원\\n4. 상품: 개별 모듬팩\\n   기본 판매 수량: 1개\\n   기본 판매 수량의 가격: 13,500원\\n\\n이전 대화는 다음과 같아\\n{chat_history}\\n\\n고객이 문의는 다음과 같아:\\n{message}\\n답변:')\n",
       "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0301', temperature=0.0, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_query_chain = general_query_prompt | ChatOpenAI(temperature=0, model=llm_model)\n",
    "general_query_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "runnable과 LLMChain은 결합 불가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "llm = OpenAI()\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=general_query_prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
      "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
      "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
      "이전 대화를 고려하여 답변해야 해.\n",
      "\n",
      "가게에서 판매하는 상품 목록.\n",
      "1. 상품: 떡케익5호\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 54,000원\n",
      "2. 상품: 무지개 백설기 케익\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 51,500원\n",
      "3. 상품: 미니 백설기\n",
      "   기본 판매 수량: 35개\n",
      "   기본 판매 수량의 가격: 31,500원\n",
      "4. 상품: 개별 모듬팩\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 13,500원\n",
      "\n",
      "이전 대화는 다음과 같아\n",
      "\n",
      "\n",
      "고객이 문의는 다음과 같아:\n",
      "판매 상품 좀 알 수 있을까요?\n",
      "답변:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 안녕하세요. 저희 가게에서는 떡케익5호, 무지개 백설기 케익, 미니 백설기, 개별 모듬팩을 판매하고 있습니다. 궁금하신 상품에 대해 자세히 알고 싶으시다면, 무엇을 알고 싶으신지 알려주시면 자세하게 알려드리겠습'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(message=\"판매 상품 좀 알 수 있을까요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
      "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
      "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
      "이전 대화를 고려하여 답변해야 해.\n",
      "\n",
      "가게에서 판매하는 상품 목록.\n",
      "1. 상품: 떡케익5호\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 54,000원\n",
      "2. 상품: 무지개 백설기 케익\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 51,500원\n",
      "3. 상품: 미니 백설기\n",
      "   기본 판매 수량: 35개\n",
      "   기본 판매 수량의 가격: 31,500원\n",
      "4. 상품: 개별 모듬팩\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 13,500원\n",
      "\n",
      "이전 대화는 다음과 같아\n",
      "Human: 판매 상품 좀 알 수 있을까요?\n",
      "AI:  안녕하세요. 저희 가게에서는 떡케익5호, 무지개 백설기 케익, 미니 백설기, 개별 모듬팩을 판매하고 있습니다. 궁금하신 상품에 대해 자세히 알고 싶으시다면, 무엇을 알고 싶으신지 알려주시면 자세하게 알려드리겠습\n",
      "\n",
      "고객이 문의는 다음과 같아:\n",
      "무지개 백설기 2개랑 미니 백설기 2개 주문할게요\n",
      "답변:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  안녕하세요. 무지개 백설기 케익(1개당 51,500원)과 미니 백설기(35개당 31,500원)를 2개씩 주문하시면 되겠습니다. 이 상품들은 모두 배송 비용이 포함되어 있어 총 금액은 103,000원 입니다. 감사합니다.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(message=\"무지개 백설기 2개랑 미니 백설기 2개 주문할게요\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL에 memory 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", system),\n",
    "#     MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "#     (\"human\", \"{message}\")\n",
    "# ])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
      "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
      "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
      "이전 대화를 고려하여 답변해야 해.\n",
      "\n",
      "가게에서 판매하는 상품 목록.\n",
      "1. 상품: 떡케익5호\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 54,000원\n",
      "2. 상품: 무지개 백설기 케익\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 51,500원\n",
      "3. 상품: 미니 백설기\n",
      "   기본 판매 수량: 35개\n",
      "   기본 판매 수량의 가격: 31,500원\n",
      "4. 상품: 개별 모듬팩\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 13,500원\n",
      "\n",
      "Human: Hi there my friend\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "system = \"\"\"너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", system),\n",
    "#     MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "#     (\"human\", \"{message}\")\n",
    "# ])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=system),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{human_input}\"),\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "chat_llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "chat_llm_chain.predict(human_input=\"Hi there my friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'chat_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 51\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y141sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m llm \u001b[39m=\u001b[39m ChatOpenAI()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y141sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m chain \u001b[39m=\u001b[39m RunnablePassthrough\u001b[39m.\u001b[39massign(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y141sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     memory\u001b[39m=\u001b[39mRunnableLambda(memory\u001b[39m.\u001b[39mload_memory_variables) \u001b[39m|\u001b[39m itemgetter(\u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y141sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m ) \u001b[39m|\u001b[39m prompt \u001b[39m|\u001b[39m llm \n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y141sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mhuman_input\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mHi there my friend\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1121\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1122\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m             patch_config(\n\u001b[0;32m   1125\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1126\u001b[0m             ),\n\u001b[0;32m   1127\u001b[0m         )\n\u001b[0;32m   1128\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:57\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;49;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_prompt(\n\u001b[0;32m     59\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{key: inner_input[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:640\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    634\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    635\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[0;32m    636\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[0;32m    637\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    638\u001b[0m )\n\u001b[0;32m    639\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 640\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m    641\u001b[0m         func, \u001b[39minput\u001b[39;49m, run_manager, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    642\u001b[0m     )\n\u001b[0;32m    643\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    644\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\config.py:201\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    200\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[1;32m--> 201\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:59\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[1;32m---> 59\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:59\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[1;32m---> 59\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'chat_history'"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=system),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{human_input}\"),\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "chain = RunnablePassthrough.assign(\n",
    "    memory=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\")\n",
    ") | prompt | llm \n",
    "\n",
    "chain.invoke({\"human_input\": \"Hi there my friend\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': []}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "variable chat_history should be a list of base messages, got {'chat_history': []}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 52\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y142sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m memory \u001b[39m=\u001b[39m ConversationBufferMemory(memory_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m, return_messages\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y142sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m chain \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m: RunnableLambda(memory\u001b[39m.\u001b[39mload_memory_variables), \u001b[39m'\u001b[39m\u001b[39mhuman_input\u001b[39m\u001b[39m'\u001b[39m: RunnablePassthrough()} \u001b[39m|\u001b[39m prompt \u001b[39m|\u001b[39m llm \n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y142sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mhuman_input\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mHi there my friend\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1121\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1122\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m             patch_config(\n\u001b[0;32m   1125\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1126\u001b[0m             ),\n\u001b[0;32m   1127\u001b[0m         )\n\u001b[0;32m   1128\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:57\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;49;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_prompt(\n\u001b[0;32m     59\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{key: inner_input[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:640\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    634\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    635\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[0;32m    636\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[0;32m    637\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    638\u001b[0m )\n\u001b[0;32m    639\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 640\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m    641\u001b[0m         func, \u001b[39minput\u001b[39;49m, run_manager, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    642\u001b[0m     )\n\u001b[0;32m    643\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    644\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\config.py:201\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    200\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[1;32m--> 201\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:58\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[1;32m---> 58\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_prompt(\n\u001b[0;32m     59\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{key: inner_input[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\prompts\\chat.py:339\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format_prompt\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_prompt\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m    331\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[39m    Format prompt. Should return a PromptValue.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39m        PromptValue.\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_messages(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    340\u001b[0m     \u001b[39mreturn\u001b[39;00m ChatPromptValue(messages\u001b[39m=\u001b[39mmessages)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\prompts\\chat.py:586\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    579\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[0;32m    580\u001b[0m ):\n\u001b[0;32m    581\u001b[0m     rel_params \u001b[39m=\u001b[39m {\n\u001b[0;32m    582\u001b[0m         k: v\n\u001b[0;32m    583\u001b[0m         \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    584\u001b[0m         \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m message_template\u001b[39m.\u001b[39minput_variables\n\u001b[0;32m    585\u001b[0m     }\n\u001b[1;32m--> 586\u001b[0m     message \u001b[39m=\u001b[39m message_template\u001b[39m.\u001b[39;49mformat_messages(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrel_params)\n\u001b[0;32m    587\u001b[0m     result\u001b[39m.\u001b[39mextend(message)\n\u001b[0;32m    588\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\prompts\\chat.py:99\u001b[0m, in \u001b[0;36mMessagesPlaceholder.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m value \u001b[39m=\u001b[39m kwargs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariable_name]\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mlist\u001b[39m):\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    100\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvariable \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariable_name\u001b[39m}\u001b[39;00m\u001b[39m should be a list of base messages, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m     )\n\u001b[0;32m    103\u001b[0m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m value:\n\u001b[0;32m    104\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(v, BaseMessage):\n",
      "\u001b[1;31mValueError\u001b[0m: variable chat_history should be a list of base messages, got {'chat_history': []}"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=system),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{human_input}\"),\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "chain = {\"chat_history\": RunnableLambda(memory.load_memory_variables), 'human_input': RunnablePassthrough()} | prompt | llm \n",
    "\n",
    "chain.invoke({\"human_input\": \"Hi there my friend\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  memory: RunnableLambda(...)\n",
       "          | RunnableLambda(...)\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': typing.List[typing.Union[langchain.schema.messages.AIMessage, langchain.schema.messages.HumanMessage, langchain.schema.messages.ChatMessage, langchain.schema.messages.SystemMessage, langchain.schema.messages.FunctionMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful chatbot')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain = RunnablePassthrough.assign(\n",
    "#     memory=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\")\n",
    "# ) | prompt | model\n",
    "\n",
    "chain = RunnablePassthrough.assign(\n",
    "    memory=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    ") | prompt | model\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 53\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y105sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m주문 좀 하려고요\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y105sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m response \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49minvoke(inputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y105sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m response\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1121\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1122\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m             patch_config(\n\u001b[0;32m   1125\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1126\u001b[0m             ),\n\u001b[0;32m   1127\u001b[0m         )\n\u001b[0;32m   1128\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:57\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;49;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_prompt(\n\u001b[0;32m     59\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{key: inner_input[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:640\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    634\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    635\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[0;32m    636\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[0;32m    637\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    638\u001b[0m )\n\u001b[0;32m    639\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 640\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m    641\u001b[0m         func, \u001b[39minput\u001b[39;49m, run_manager, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    642\u001b[0m     )\n\u001b[0;32m    643\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    644\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\config.py:201\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    200\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[1;32m--> 201\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:59\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[1;32m---> 59\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:59\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[1;32m---> 59\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'history'"
     ]
    }
   ],
   "source": [
    "inputs = {\"input\": \"주문 좀 하려고요\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful chatbot\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  memory: RunnableLambda(...)\n",
       "          | RunnableLambda(...)\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnablePassthrough.assign(\n",
    "    memory=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': typing.List[typing.Union[langchain.schema.messages.AIMessage, langchain.schema.messages.HumanMessage, langchain.schema.messages.ChatMessage, langchain.schema.messages.SystemMessage, langchain.schema.messages.FunctionMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful chatbot')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  memory: RunnableLambda(...)\n",
       "          | RunnableLambda(...)\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnablePassthrough.assign(\n",
    "    memory=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnablePassthrough.assign(\n",
    "    memory=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    ") | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  history: RunnableAssign(mapper={\n",
       "             memory: RunnableLambda(...)\n",
       "           })\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': typing.List[typing.Union[langchain.schema.messages.AIMessage, langchain.schema.messages.HumanMessage, langchain.schema.messages.ChatMessage, langchain.schema.messages.SystemMessage, langchain.schema.messages.FunctionMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful chatbot')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 59\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y115sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mhi im bob\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y115sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m response \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49minvoke(inputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y115sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m response\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1121\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1122\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m             patch_config(\n\u001b[0;32m   1125\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1126\u001b[0m             ),\n\u001b[0;32m   1127\u001b[0m         )\n\u001b[0;32m   1128\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:57\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;49;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_prompt(\n\u001b[0;32m     59\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{key: inner_input[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:640\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    634\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    635\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[0;32m    636\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[0;32m    637\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    638\u001b[0m )\n\u001b[0;32m    639\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 640\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m    641\u001b[0m         func, \u001b[39minput\u001b[39;49m, run_manager, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    642\u001b[0m     )\n\u001b[0;32m    643\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    644\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\config.py:201\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    200\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[1;32m--> 201\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:59\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[1;32m---> 59\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:59\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[1;32m---> 59\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'history'"
     ]
    }
   ],
   "source": [
    "inputs = {\"input\": \"hi im bob\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful chatbot\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnablePassthrough.assign(\n",
    "    memory=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    ") | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': RunnableAssign(mapper={\n",
       "   memory: RunnableLambda(...)\n",
       " })}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"history\": RunnablePassthrough.assign(memory=RunnableLambda(memory.load_memory_variables))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {\"history\": RunnablePassthrough.assign(memory=RunnableLambda(memory.load_memory_variables))} | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 66\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y126sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mhi im bob\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y126sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m response \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49minvoke(inputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y126sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m response\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1121\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1122\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m             patch_config(\n\u001b[0;32m   1125\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1126\u001b[0m             ),\n\u001b[0;32m   1127\u001b[0m         )\n\u001b[0;32m   1128\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:57\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;49;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_prompt(\n\u001b[0;32m     59\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{key: inner_input[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:640\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    634\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    635\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[0;32m    636\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[0;32m    637\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    638\u001b[0m )\n\u001b[0;32m    639\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 640\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m    641\u001b[0m         func, \u001b[39minput\u001b[39;49m, run_manager, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    642\u001b[0m     )\n\u001b[0;32m    643\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    644\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\config.py:201\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    200\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[1;32m--> 201\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:59\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[1;32m---> 59\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:59\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[1;32m---> 59\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'input'"
     ]
    }
   ],
   "source": [
    "inputs = {\"input\": \"hi im bob\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- 다시다시! 중간에 {}를 메모리에 넣는 걸 빼먹어서 그런 듯 - 아니네 ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful chatbot\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': ''}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# memory_key는 반환값의 키 이름을 결정\n",
    "ConversationBufferMemory(memory_key=\"chat_history\").load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  memory: RunnableLambda(...)\n",
       "          | RunnableLambda(...)\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': typing.List[typing.Union[langchain.schema.messages.AIMessage, langchain.schema.messages.HumanMessage, langchain.schema.messages.ChatMessage, langchain.schema.messages.SystemMessage, langchain.schema.messages.FunctionMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful chatbot')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnablePassthrough.assign(\n",
    "    memory=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    ") | prompt | model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  history: RunnableLambda(...)\n",
       "           | RunnableLambda(...)\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': typing.List[typing.Union[langchain.schema.messages.AIMessage, langchain.schema.messages.HumanMessage, langchain.schema.messages.ChatMessage, langchain.schema.messages.SystemMessage, langchain.schema.messages.FunctionMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful chatbot')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnablePassthrough.assign(\n",
    "    history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    ") | prompt | model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Bob! How can I assist you today?')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"input\": \"hi im bob\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  memory: RunnableLambda(...)\n",
       "})\n",
       "| RunnableLambda(...)\n",
       "| ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': typing.List[typing.Union[langchain.schema.messages.AIMessage, langchain.schema.messages.HumanMessage, langchain.schema.messages.ChatMessage, langchain.schema.messages.SystemMessage, langchain.schema.messages.FunctionMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful chatbot')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnablePassthrough.assign(\n",
    "    memory=RunnableLambda(memory.load_memory_variables)) | itemgetter(\"history\") | prompt | model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 79\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y151sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mhi im bob\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y151sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m response \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49minvoke(inputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y151sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m response\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1121\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1122\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m             patch_config(\n\u001b[0;32m   1125\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1126\u001b[0m             ),\n\u001b[0;32m   1127\u001b[0m         )\n\u001b[0;32m   1128\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:57\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;49;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_prompt(\n\u001b[0;32m     59\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{key: inner_input[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:640\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    634\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    635\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[0;32m    636\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[0;32m    637\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    638\u001b[0m )\n\u001b[0;32m    639\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 640\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m    641\u001b[0m         func, \u001b[39minput\u001b[39;49m, run_manager, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    642\u001b[0m     )\n\u001b[0;32m    643\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    644\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\config.py:201\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    200\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[1;32m--> 201\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:59\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[1;32m---> 59\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:59\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[1;32m---> 59\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'history'"
     ]
    }
   ],
   "source": [
    "inputs = {\"input\": \"hi im bob\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {'history': RunnableLambda(memory.load_memory_variables), \"input\": RunnablePassthrough()} | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "variable history should be a list of base messages, got {'history': []}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 81\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y155sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mhi im bob\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y155sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m response \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49minvoke(inputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y155sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m response\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1121\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1122\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m             patch_config(\n\u001b[0;32m   1125\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1126\u001b[0m             ),\n\u001b[0;32m   1127\u001b[0m         )\n\u001b[0;32m   1128\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:57\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[0;32m     58\u001b[0m         \u001b[39mlambda\u001b[39;49;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_prompt(\n\u001b[0;32m     59\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{key: inner_input[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:640\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    634\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    635\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[0;32m    636\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[0;32m    637\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    638\u001b[0m )\n\u001b[0;32m    639\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 640\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m    641\u001b[0m         func, \u001b[39minput\u001b[39;49m, run_manager, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    642\u001b[0m     )\n\u001b[0;32m    643\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    644\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\config.py:201\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    200\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[1;32m--> 201\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\prompt_template.py:58\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[1;32m---> 58\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_prompt(\n\u001b[0;32m     59\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{key: inner_input[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_variables}\n\u001b[0;32m     60\u001b[0m         ),\n\u001b[0;32m     61\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m     62\u001b[0m         config,\n\u001b[0;32m     63\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\prompts\\chat.py:339\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format_prompt\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_prompt\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m    331\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[39m    Format prompt. Should return a PromptValue.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39m        PromptValue.\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_messages(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    340\u001b[0m     \u001b[39mreturn\u001b[39;00m ChatPromptValue(messages\u001b[39m=\u001b[39mmessages)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\prompts\\chat.py:586\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    579\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[0;32m    580\u001b[0m ):\n\u001b[0;32m    581\u001b[0m     rel_params \u001b[39m=\u001b[39m {\n\u001b[0;32m    582\u001b[0m         k: v\n\u001b[0;32m    583\u001b[0m         \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    584\u001b[0m         \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m message_template\u001b[39m.\u001b[39minput_variables\n\u001b[0;32m    585\u001b[0m     }\n\u001b[1;32m--> 586\u001b[0m     message \u001b[39m=\u001b[39m message_template\u001b[39m.\u001b[39;49mformat_messages(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrel_params)\n\u001b[0;32m    587\u001b[0m     result\u001b[39m.\u001b[39mextend(message)\n\u001b[0;32m    588\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\prompts\\chat.py:99\u001b[0m, in \u001b[0;36mMessagesPlaceholder.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m value \u001b[39m=\u001b[39m kwargs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariable_name]\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mlist\u001b[39m):\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    100\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvariable \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariable_name\u001b[39m}\u001b[39;00m\u001b[39m should be a list of base messages, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m     )\n\u001b[0;32m    103\u001b[0m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m value:\n\u001b[0;32m    104\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(v, BaseMessage):\n",
      "\u001b[1;31mValueError\u001b[0m: variable history should be a list of base messages, got {'history': []}"
     ]
    }
   ],
   "source": [
    "inputs = {\"input\": \"hi im bob\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해결됐으니 다시 적용!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", system),\n",
    "#     MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "#     (\"human\", \"{message}\")\n",
    "# ])\n",
    "\n",
    "general_query_chain = prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{message}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnableLambda만 바꿔서는 안 되고 memeory의 memory_key의 이름도 바꿔야 함\n",
    "chain = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\")\n",
    ") | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 주문하려는 상품이 어떻게 되시나요? 제가 도와드릴게요.')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"message\": \"주문 좀 하려고요\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "general_query_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\n",
    "이전 대화는 다음과 같아\n",
    "{chat_history}\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "order_change_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 변경을 전담하는 종업원이야.\n",
    "고객이 변경한 주문 내용을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 변경 내용이 잘못됐다면, 주문 변경 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 변경 내용을 정확히 파악할 때까지 반복해야돼.\n",
    "고객의 주문 변경을 정확히 파악했다면, 고객에게 고객이 주문을 변경한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 변경된 주문의 총 가격을 알려줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "이전 대화는 다음과 같아\n",
    "{chat_history}\n",
    "\n",
    "고객의 주문 변경은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "order_cancel_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 취소를 전담하는 종업원이야.\n",
    "고객이 취소하려는 주문을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 취소 내용이 잘못됐다면, 주문 취소 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 취소 내용을 정확히 파악할 때\n",
    "고객의 주문 취소 내용을 정확히 파악했다면, 고객에게 고객이 주문을 취소한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 취소된 주문의 총 가격을 알려줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "이전 대화는 다음과 같아\n",
    "{chat_history}\n",
    "\n",
    "고객이 취소하려는 주문은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "# general_chain = PromptTemplate.from_template(\"\"\"Respond to the following question:\n",
    "\n",
    "# Question: {question}\n",
    "# Answer:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "branch = RunnableBranch(\n",
    "  (lambda x: \"주문 변경\" in x[\"topic\"], order_change_chain),\n",
    "  (lambda x: \"주문 취소\" in x[\"topic\"], order_cancel_chain),\n",
    "  general_query_chain,\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "full_chain = {\n",
    "    \"topic\": chain,\n",
    "    \"message\": lambda x: x[\"message\"],\n",
    "    \"chat_history\": RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\"))\n",
    "} | branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 물론입니다. 저희 가게에서 판매하는 상품은 떡케익5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩이 있습니다. 각 상품의 기본 판매 수량과 가격은 다음과 같습니다. 떡케익5호는 1개에 54,000원, 무지개 백설기 케익은 1개에 51,500원, 미니 백설기는 35개에 31,500원, 그리고 개별 모듬팩은 1개에 13,500원입니다. 어떤 상품을 원하시나요?')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"주문 하기 전에 상품 정보 좀 알 수 있을까요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 고객님. 이전에 주문하신 떡케익 1개와 미니 백설기 2개를 주문하시겠다는 건가요? 그렇다면 총 가격은 94,500원이 되겠습니다. 주문하시겠어요?')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"그럼 떡케익1개, 미니 백설기 2개 주문할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 떡케익은 안 하시겠다고 하셨으니 다른 상품으로 주문하시면 됩니다. 저희 가게에서는 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있습니다. 원하시는 상품을 말씀해주시면 수량과 함께 안내해드리겠습니다.')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"아 그냥 떡케익은 안 할게요. 그럼 제 주문은 어떻게 되죠?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 알겠습니다. 이전에 주문하신 내역을 확인해보면 떡케익을 제외하고는 원래 주문하신 대로 모두 준비되어 있습니다. 떡케익 대신에 다른 상품으로 변경하실 건가요? 저희 가게에서는 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있습니다. 어떤 상품으로 변경하실 건가요?')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"아 그냥 떡케익만 안 하고 나머지는 원래 주문대로 할게요. 제 주문 내역을 알려주시겠어요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = {\n",
    "    \"topic\": chain,\n",
    "    \"message\": lambda x: x[\"message\"],\n",
    "    \"chat_history\": RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\")\n",
    "} | branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 떡케익 1호를 주문하시는 건가요? 미니 백설기는 35개 단위로 판매되는데, 2개를 주문하시면 70개가 되는데요. 혹시 35개 단위로 추가 주문하시겠어요? 떡케익 1호 1개와 미니 백설기 35개를 함께 주문하시면 총 86,500원이 됩니다. 주문하시겠어요?')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"떡케익1개, 미니 백설기 2개 주문할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 떡케익 1개를 빼시면 주문 금액은 54,000원에서 1개의 떡케익 가격인 54,000원을 뺀 0원이 됩니다. 다른 상품으로 대체하실 건가요?')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"떡케익1개을 빼면 제 주문 금액이 어떻게 되죠?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMChain with memory 사용\n",
    "- LLMChain은 prompt, memory 등을 연결한 거라 그런지 prompt는 필수 입력값이네"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LLMChain\nprompt\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 103\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y231sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m LLMChain(llm\u001b[39m=\u001b[39;49mllm)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\load\\serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for LLMChain\nprompt\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "LLMChain(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PromptTemplate.from_template과 ChatPromptTemplate.from_messages의 차이는(특히 메소드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': typing.List[typing.Union[langchain.schema.messages.AIMessage, langchain.schema.messages.HumanMessage, langchain.schema.messages.ChatMessage, langchain.schema.messages.SystemMessage, langchain.schema.messages.FunctionMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful chatbot')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful chatbot\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_query_template = PromptTemplate.from_template(\"\"\"\n",
    "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\n",
    "이전 대화는 다음과 같아\n",
    "{chat_history}\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs, {\"output\": response.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  chat_history: RunnableLambda(...)\n",
       "                | RunnableLambda(...)\n",
       "})\n",
       "| PromptTemplate(input_variables=['chat_history', 'message'], template='\\n너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\\n가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\\n자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\\n이전 대화를 고려하여 답변해야 해.\\n\\n가게에서 판매하는 상품 목록.\\n1. 상품: 떡케익5호\\n   기본 판매 수량: 1개\\n   기본 판매 수량의 가격: 54,000원\\n2. 상품: 무지개 백설기 케익\\n   기본 판매 수량: 1개\\n   기본 판매 수량의 가격: 51,500원\\n3. 상품: 미니 백설기\\n   기본 판매 수량: 35개\\n   기본 판매 수량의 가격: 31,500원\\n4. 상품: 개별 모듬팩\\n   기본 판매 수량: 1개\\n   기본 판매 수량의 가격: 13,500원\\n\\n이전 대화는 다음과 같아\\n{chat_history}\\n\\n고객이 문의는 다음과 같아:\\n{message}\\n답변:')\n",
       "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_query_chain = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\")\n",
    ") | general_query_template | model\n",
    "general_query_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 물론이죠. 저희 가게에서 판매하는 상품은 다음과 같아요.\\n\\n1. 떡케익5호: 기본 판매 수량은 1개이고 가격은 54,000원이에요.\\n2. 무지개 백설기 케익: 기본 판매 수량은 1개이고 가격은 51,500원이에요.\\n3. 미니 백설기: 기본 판매 수량은 35개이고 가격은 31,500원이에요.\\n4. 개별 모듬팩: 기본 판매 수량은 1개이고 가격은 13,500원이에요.\\n\\n어떤 상품에 대해 더 자세한 정보가 필요하신가요?')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_query_chain.invoke({\"message\": \"주문 하기 전에 상품 정보 좀 알 수 있을까요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "general_query_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\n",
    "이전 대화는 다음과 같아\n",
    "{chat_history}\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "chain = PromptTemplate.from_template(\"\"\"\n",
    "아래 고객의 메시지를 보고, 이 메시지가 \"일반\", \"주문 변경\", \"주문 취소\" 중 어디에 해당되는지 분류해줘.\n",
    "\"일반\"이란 범주는 일반 문의 또는 주문에 관한 내용을 의미해.\n",
    "\n",
    "제시된 범주(\"일반\", \"주문 변경\", \"주문 취소\") 가운데 하나로 분류해야돼.\n",
    "\n",
    "<고객 메시지>\n",
    "{message}\n",
    "</고객 메시지>\n",
    "\n",
    "Classification:\"\"\") | ChatOpenAI(temperature=0, model=llm_model) | StrOutputParser()\n",
    "\n",
    "general_query_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\n",
    "이전 대화는 다음과 같아\n",
    "{chat_history}\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "order_change_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 변경을 전담하는 종업원이야.\n",
    "고객이 변경한 주문 내용을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 변경 내용이 잘못됐다면, 주문 변경 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 변경 내용을 정확히 파악할 때까지 반복해야돼.\n",
    "고객의 주문 변경을 정확히 파악했다면, 고객에게 고객이 주문을 변경한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 변경된 주문의 총 가격을 알려줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "이전 대화는 다음과 같아\n",
    "{chat_history}\n",
    "\n",
    "고객의 주문 변경은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "order_cancel_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 취소를 전담하는 종업원이야.\n",
    "고객이 취소하려는 주문을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 취소 내용이 잘못됐다면, 주문 취소 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 취소 내용을 정확히 파악할 때\n",
    "고객의 주문 취소 내용을 정확히 파악했다면, 고객에게 고객이 주문을 취소한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 취소된 주문의 총 가격을 알려줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "이전 대화는 다음과 같아\n",
    "{chat_history}\n",
    "\n",
    "고객이 취소하려는 주문은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "# general_chain = PromptTemplate.from_template(\"\"\"Respond to the following question:\n",
    "\n",
    "# Question: {question}\n",
    "# Answer:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "branch = RunnableBranch(\n",
    "  (lambda x: \"주문 변경\" in x[\"topic\"], order_change_chain),\n",
    "  (lambda x: \"주문 취소\" in x[\"topic\"], order_cancel_chain),\n",
    "  general_query_chain,\n",
    ")\n",
    "\n",
    "full_chain = {\n",
    "    \"topic\": chain,\n",
    "    \"message\": lambda x: x[\"message\"]\n",
    "} | branch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMChain 사용 다시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatPromptTemplate.from_messages vs PromptTemplate.from_template 차이(특히 메소드 측면)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\n",
    "이전 대화는 다음과 같아\n",
    "{chat_history}\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful chatbot\"),\n",
    "#     MessagesPlaceholder(variable_name=\"history\"),\n",
    "#     (\"human\", \"{input}\")\n",
    "# ])\n",
    "\n",
    "general_query_prompt = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", system_prompt),\n",
    "   MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "   (\"human\", \"{message}\")\n",
    "   ])\n",
    "\n",
    "general_query_chain = LLMChain(\n",
    "    prompt=general_query_prompt,\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유저 입력이 중복 포함됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
      "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
      "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
      "이전 대화를 고려하여 답변해야 해.\n",
      "\n",
      "가게에서 판매하는 상품 목록.\n",
      "1. 상품: 떡케익5호\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 54,000원\n",
      "2. 상품: 무지개 백설기 케익\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 51,500원\n",
      "3. 상품: 미니 백설기\n",
      "   기본 판매 수량: 35개\n",
      "   기본 판매 수량의 가격: 31,500원\n",
      "4. 상품: 개별 모듬팩\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 13,500원\n",
      "\n",
      "이전 대화는 다음과 같아\n",
      "[]\n",
      "\n",
      "고객이 문의는 다음과 같아:\n",
      "주문 하기 전에 상품 정보 좀 알 수 있을까요?\n",
      "답변:\n",
      "Human: 주문 하기 전에 상품 정보 좀 알 수 있을까요?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': '주문 하기 전에 상품 정보 좀 알 수 있을까요?',\n",
       " 'chat_history': [HumanMessage(content='주문 하기 전에 상품 정보 좀 알 수 있을까요?'),\n",
       "  AIMessage(content='네, 물론이죠! 저희 가게에서는 다양한 상품을 판매하고 있어요. 현재 판매 중인 상품은 떡케익5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩이 있어요. \\n\\n떡케익5호는 1개를 기본 판매 수량으로 하고 있고, 가격은 54,000원이에요. 무지개 백설기 케익도 1개를 기본 판매 수량으로 하고 있고, 가격은 51,500원이에요. \\n\\n미니 백설기는 35개를 기본 판매 수량으로 하고 있고, 가격은 31,500원이에요. 마지막으로 개별 모듬팩은 1개를 기본 판매 수량으로 하고 있고, 가격은 13,500원이에요.\\n\\n어떤 상품에 대해 더 자세한 정보를 알고 싶으신가요?')],\n",
       " 'text': '네, 물론이죠! 저희 가게에서는 다양한 상품을 판매하고 있어요. 현재 판매 중인 상품은 떡케익5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩이 있어요. \\n\\n떡케익5호는 1개를 기본 판매 수량으로 하고 있고, 가격은 54,000원이에요. 무지개 백설기 케익도 1개를 기본 판매 수량으로 하고 있고, 가격은 51,500원이에요. \\n\\n미니 백설기는 35개를 기본 판매 수량으로 하고 있고, 가격은 31,500원이에요. 마지막으로 개별 모듬팩은 1개를 기본 판매 수량으로 하고 있고, 가격은 13,500원이에요.\\n\\n어떤 상품에 대해 더 자세한 정보를 알고 싶으신가요?'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_query_chain({'message': '주문 하기 전에 상품 정보 좀 알 수 있을까요?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful chatbot\"),\n",
    "#     MessagesPlaceholder(variable_name=\"history\"),\n",
    "#     (\"human\", \"{input}\")\n",
    "# ])\n",
    "\n",
    "general_query_prompt = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", system_prompt),\n",
    "   MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "   (\"human\", \"{message}\")\n",
    "   ])\n",
    "\n",
    "general_query_chain = LLMChain(\n",
    "    prompt=general_query_prompt,\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
      "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
      "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
      "이전 대화를 고려하여 답변해야 해.\n",
      "\n",
      "가게에서 판매하는 상품 목록.\n",
      "1. 상품: 떡케익5호\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 54,000원\n",
      "2. 상품: 무지개 백설기 케익\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 51,500원\n",
      "3. 상품: 미니 백설기\n",
      "   기본 판매 수량: 35개\n",
      "   기본 판매 수량의 가격: 31,500원\n",
      "4. 상품: 개별 모듬팩\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 13,500원\n",
      "\n",
      "고객이 문의는 다음과 같아:\n",
      "주문 하기 전에 상품 정보 좀 알 수 있을까요?\n",
      "답변:\n",
      "Human: 주문 하기 전에 상품 정보 좀 알 수 있을까요?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': '주문 하기 전에 상품 정보 좀 알 수 있을까요?',\n",
       " 'chat_history': [HumanMessage(content='주문 하기 전에 상품 정보 좀 알 수 있을까요?'),\n",
       "  AIMessage(content='네, 물론입니다! 어떤 상품에 대해 알고 싶으신가요? 저희 가게에서는 떡케익 5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있습니다. 각 상품의 기본 판매 수량과 가격을 알려드릴게요. 떡케익 5호는 1개 판매 시 54,000원이며, 무지개 백설기 케익은 1개 판매 시 51,500원입니다. 미니 백설기는 35개가 한 세트로 판매되며, 세트당 31,500원입니다. 마지막으로, 개별 모듬팩은 1개 판매 시 13,500원입니다. 어떤 상품에 대해 더 자세한 정보가 필요하신가요?')],\n",
       " 'text': '네, 물론입니다! 어떤 상품에 대해 알고 싶으신가요? 저희 가게에서는 떡케익 5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있습니다. 각 상품의 기본 판매 수량과 가격을 알려드릴게요. 떡케익 5호는 1개 판매 시 54,000원이며, 무지개 백설기 케익은 1개 판매 시 51,500원입니다. 미니 백설기는 35개가 한 세트로 판매되며, 세트당 31,500원입니다. 마지막으로, 개별 모듬팩은 1개 판매 시 13,500원입니다. 어떤 상품에 대해 더 자세한 정보가 필요하신가요?'}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_query_chain({'message': '주문 하기 전에 상품 정보 좀 알 수 있을까요?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
      "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
      "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
      "이전 대화를 고려하여 답변해야 해.\n",
      "\n",
      "가게에서 판매하는 상품 목록.\n",
      "1. 상품: 떡케익5호\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 54,000원\n",
      "2. 상품: 무지개 백설기 케익\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 51,500원\n",
      "3. 상품: 미니 백설기\n",
      "   기본 판매 수량: 35개\n",
      "   기본 판매 수량의 가격: 31,500원\n",
      "4. 상품: 개별 모듬팩\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 13,500원\n",
      "\n",
      "고객이 문의는 다음과 같아:\n",
      "그럼 떡케익1개, 미니 백설기 2개 주문하면 얼마죠?\n",
      "답변:\n",
      "Human: 주문 하기 전에 상품 정보 좀 알 수 있을까요?\n",
      "AI: 네, 물론입니다! 어떤 상품에 대해 알고 싶으신가요? 저희 가게에서는 떡케익 5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있습니다. 각 상품의 기본 판매 수량과 가격을 알려드릴게요. 떡케익 5호는 1개 판매 시 54,000원이며, 무지개 백설기 케익은 1개 판매 시 51,500원입니다. 미니 백설기는 35개가 한 세트로 판매되며, 세트당 31,500원입니다. 마지막으로, 개별 모듬팩은 1개 판매 시 13,500원입니다. 어떤 상품에 대해 더 자세한 정보가 필요하신가요?\n",
      "Human: 그럼 떡케익1개, 미니 백설기 2개 주문하면 얼마죠?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': '그럼 떡케익1개, 미니 백설기 2개 주문하면 얼마죠?',\n",
       " 'chat_history': [HumanMessage(content='주문 하기 전에 상품 정보 좀 알 수 있을까요?'),\n",
       "  AIMessage(content='네, 물론입니다! 어떤 상품에 대해 알고 싶으신가요? 저희 가게에서는 떡케익 5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있습니다. 각 상품의 기본 판매 수량과 가격을 알려드릴게요. 떡케익 5호는 1개 판매 시 54,000원이며, 무지개 백설기 케익은 1개 판매 시 51,500원입니다. 미니 백설기는 35개가 한 세트로 판매되며, 세트당 31,500원입니다. 마지막으로, 개별 모듬팩은 1개 판매 시 13,500원입니다. 어떤 상품에 대해 더 자세한 정보가 필요하신가요?'),\n",
       "  HumanMessage(content='그럼 떡케익1개, 미니 백설기 2개 주문하면 얼마죠?'),\n",
       "  AIMessage(content='떡케익 1개와 미니 백설기 2개를 주문하시면 어떤 상품을 원하시는지 정확히 알려주셔야 정확한 가격을 안내해 드릴 수 있어요. 저희 가게에는 떡케익 5호와 미니 백설기 두 가지 종류가 있기 때문에, 원하시는 상품을 알려주시면 해당 상품의 가격을 알려드릴게요. 어떤 상품을 주문하시려고 하시는 건가요?')],\n",
       " 'text': '떡케익 1개와 미니 백설기 2개를 주문하시면 어떤 상품을 원하시는지 정확히 알려주셔야 정확한 가격을 안내해 드릴 수 있어요. 저희 가게에는 떡케익 5호와 미니 백설기 두 가지 종류가 있기 때문에, 원하시는 상품을 알려주시면 해당 상품의 가격을 알려드릴게요. 어떤 상품을 주문하시려고 하시는 건가요?'}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_query_chain({'message': '그럼 떡케익1개, 미니 백설기 2개 주문하면 얼마죠?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
      "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
      "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
      "이전 대화를 고려하여 답변해야 해.\n",
      "\n",
      "가게에서 판매하는 상품 목록.\n",
      "1. 상품: 떡케익5호\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 54,000원\n",
      "2. 상품: 무지개 백설기 케익\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 51,500원\n",
      "3. 상품: 미니 백설기\n",
      "   기본 판매 수량: 35개\n",
      "   기본 판매 수량의 가격: 31,500원\n",
      "4. 상품: 개별 모듬팩\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 13,500원\n",
      "\n",
      "고객이 문의는 다음과 같아:\n",
      "그럼 떡케익 종류가 한 가지 뿐이지 않나요?\n",
      "답변:\n",
      "Human: 주문 하기 전에 상품 정보 좀 알 수 있을까요?\n",
      "AI: 네, 물론입니다! 어떤 상품에 대해 알고 싶으신가요? 저희 가게에서는 떡케익 5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있습니다. 각 상품의 기본 판매 수량과 가격을 알려드릴게요. 떡케익 5호는 1개 판매 시 54,000원이며, 무지개 백설기 케익은 1개 판매 시 51,500원입니다. 미니 백설기는 35개가 한 세트로 판매되며, 세트당 31,500원입니다. 마지막으로, 개별 모듬팩은 1개 판매 시 13,500원입니다. 어떤 상품에 대해 더 자세한 정보가 필요하신가요?\n",
      "Human: 그럼 떡케익1개, 미니 백설기 2개 주문하면 얼마죠?\n",
      "AI: 떡케익 1개와 미니 백설기 2개를 주문하시면 어떤 상품을 원하시는지 정확히 알려주셔야 정확한 가격을 안내해 드릴 수 있어요. 저희 가게에는 떡케익 5호와 미니 백설기 두 가지 종류가 있기 때문에, 원하시는 상품을 알려주시면 해당 상품의 가격을 알려드릴게요. 어떤 상품을 주문하시려고 하시는 건가요?\n",
      "Human: 그럼 떡케익 종류가 한 가지 뿐이지 않나요?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': '그럼 떡케익 종류가 한 가지 뿐이지 않나요?',\n",
       " 'chat_history': [HumanMessage(content='주문 하기 전에 상품 정보 좀 알 수 있을까요?'),\n",
       "  AIMessage(content='네, 물론입니다! 어떤 상품에 대해 알고 싶으신가요? 저희 가게에서는 떡케익 5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있습니다. 각 상품의 기본 판매 수량과 가격을 알려드릴게요. 떡케익 5호는 1개 판매 시 54,000원이며, 무지개 백설기 케익은 1개 판매 시 51,500원입니다. 미니 백설기는 35개가 한 세트로 판매되며, 세트당 31,500원입니다. 마지막으로, 개별 모듬팩은 1개 판매 시 13,500원입니다. 어떤 상품에 대해 더 자세한 정보가 필요하신가요?'),\n",
       "  HumanMessage(content='그럼 떡케익1개, 미니 백설기 2개 주문하면 얼마죠?'),\n",
       "  AIMessage(content='떡케익 1개와 미니 백설기 2개를 주문하시면 어떤 상품을 원하시는지 정확히 알려주셔야 정확한 가격을 안내해 드릴 수 있어요. 저희 가게에는 떡케익 5호와 미니 백설기 두 가지 종류가 있기 때문에, 원하시는 상품을 알려주시면 해당 상품의 가격을 알려드릴게요. 어떤 상품을 주문하시려고 하시는 건가요?'),\n",
       "  HumanMessage(content='그럼 떡케익 종류가 한 가지 뿐이지 않나요?'),\n",
       "  AIMessage(content='네, 맞아요. 저희 가게에서는 현재 떡케익 종류는 \"떡케익 5호\"만 판매하고 있어요. 떡케익 5호 1개의 가격은 54,000원이에요. 그럼 떡케익 5호를 1개 주문하시겠어요?')],\n",
       " 'text': '네, 맞아요. 저희 가게에서는 현재 떡케익 종류는 \"떡케익 5호\"만 판매하고 있어요. 떡케익 5호 1개의 가격은 54,000원이에요. 그럼 떡케익 5호를 1개 주문하시겠어요?'}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_query_chain({'message': '그럼 떡케익 종류가 한 가지 뿐이지 않나요?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모두 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = PromptTemplate.from_template(\"\"\"\n",
    "아래 고객의 메시지를 보고, 이 메시지가 \"일반\", \"주문 변경\", \"주문 취소\" 중 어디에 해당되는지 분류해줘.\n",
    "\"일반\"이란 범주는 일반 문의 또는 주문에 관한 내용을 의미해.\n",
    "\n",
    "제시된 범주(\"일반\", \"주문 변경\", \"주문 취소\") 가운데 하나로 분류해야돼.\n",
    "\n",
    "<고객 메시지>\n",
    "{message}\n",
    "</고객 메시지>\n",
    "\n",
    "Classification:\"\"\") | ChatOpenAI(temperature=0, model=llm_model) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\"\n",
    "\n",
    "general_query_prompt = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", system_prompt),\n",
    "   MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "   (\"human\", \"{message}\")\n",
    "   ])\n",
    "\n",
    "general_query_chain = LLMChain(\n",
    "    prompt=general_query_prompt,\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_change_system_prompt = \"\"\"\n",
    "너는 주문 변경을 전담하는 종업원이야.\n",
    "고객이 변경한 주문 내용을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 변경 내용이 잘못됐다면, 주문 변경 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 변경 내용을 정확히 파악할 때까지 반복해야돼.\n",
    "고객의 주문 변경을 정확히 파악했다면, 고객에게 고객이 주문을 변경한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 변경된 주문의 총 가격을 알려줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "고객의 주문 변경은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\"\n",
    "order_change_prompt = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", order_change_system_prompt),\n",
    "   MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "   (\"human\", \"{message}\")\n",
    "   ])\n",
    "order_change_chain = LLMChain(\n",
    "    prompt=order_change_prompt,\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    )\n",
    "\n",
    "\n",
    "order_cancel_system_prompt = \"\"\"\n",
    "너는 주문 취소를 전담하는 종업원이야.\n",
    "고객이 취소하려는 주문을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 취소 내용이 잘못됐다면, 주문 취소 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 취소 내용을 정확히 파악할 때\n",
    "고객의 주문 취소 내용을 정확히 파악했다면, 고객에게 고객이 주문을 취소한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 취소된 주문의 총 가격을 알려줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "고객이 취소하려는 주문은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\"\n",
    "order_chancel_prompt = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", order_cancel_system_prompt),\n",
    "   MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "   (\"human\", \"{message}\")\n",
    "   ])\n",
    "order_cancel_chain = LLMChain(\n",
    "    prompt=order_chancel_prompt,\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch = RunnableBranch(\n",
    "  (lambda x: \"주문 변경\" in x[\"topic\"], order_change_chain),\n",
    "  (lambda x: \"주문 취소\" in x[\"topic\"], order_cancel_chain),\n",
    "  general_query_chain,\n",
    ")\n",
    "\n",
    "full_chain = {\n",
    "    \"topic\": chain,\n",
    "    \"message\": lambda x: x[\"message\"]\n",
    "} | branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatPromptTemplate 사용하는 것과 관련 있나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
      "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
      "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
      "이전 대화를 고려하여 답변해야 해.\n",
      "\n",
      "가게에서 판매하는 상품 목록.\n",
      "1. 상품: 떡케익5호\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 54,000원\n",
      "2. 상품: 무지개 백설기 케익\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 51,500원\n",
      "3. 상품: 미니 백설기\n",
      "   기본 판매 수량: 35개\n",
      "   기본 판매 수량의 가격: 31,500원\n",
      "4. 상품: 개별 모듬팩\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 13,500원\n",
      "\n",
      "고객이 문의는 다음과 같아:\n",
      "주문 하기 전에 상품 정보 좀 알 수 있을까요?\n",
      "답변:\n",
      "Human: 주문 하기 전에 상품 정보 좀 알 수 있을까요?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One input key expected got ['topic', 'message']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 130\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y303sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m full_chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mmessage\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39m주문 하기 전에 상품 정보 좀 알 수 있을까요?\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1121\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1122\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m             patch_config(\n\u001b[0;32m   1125\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1126\u001b[0m             ),\n\u001b[0;32m   1127\u001b[0m         )\n\u001b[0;32m   1128\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\branch.py:194\u001b[0m, in \u001b[0;36mRunnableBranch.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 194\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefault\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m    195\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    196\u001b[0m             config\u001b[39m=\u001b[39;49mpatch_config(\n\u001b[0;32m    197\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(tag\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbranch:default\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    198\u001b[0m             ),\n\u001b[0;32m    199\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    200\u001b[0m         )\n\u001b[0;32m    201\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\chains\\base.py:85\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m     79\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     80\u001b[0m     \u001b[39minput\u001b[39m: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m     81\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     82\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m     83\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m     84\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m---> 85\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(\n\u001b[0;32m     86\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m     87\u001b[0m         callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     88\u001b[0m         tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     89\u001b[0m         metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     90\u001b[0m         run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     91\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m     92\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\chains\\base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    309\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m--> 310\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_outputs(\n\u001b[0;32m    311\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    312\u001b[0m )\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m include_run_info:\n\u001b[0;32m    314\u001b[0m     final_outputs[RUN_KEY] \u001b[39m=\u001b[39m RunInfo(run_id\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mrun_id)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\chains\\base.py:406\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[1;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_outputs(outputs)\n\u001b[0;32m    405\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49msave_context(inputs, outputs)\n\u001b[0;32m    407\u001b[0m \u001b[39mif\u001b[39;00m return_only_outputs:\n\u001b[0;32m    408\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\memory\\chat_memory.py:35\u001b[0m, in \u001b[0;36mBaseChatMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_context\u001b[39m(\u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Any], outputs: Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     input_str, output_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_input_output(inputs, outputs)\n\u001b[0;32m     36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_memory\u001b[39m.\u001b[39madd_user_message(input_str)\n\u001b[0;32m     37\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_memory\u001b[39m.\u001b[39madd_ai_message(output_str)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\memory\\chat_memory.py:22\u001b[0m, in \u001b[0;36mBaseChatMemory._get_input_output\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_input_output\u001b[39m(\n\u001b[0;32m     19\u001b[0m     \u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Any], outputs: Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]\n\u001b[0;32m     20\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m     21\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m         prompt_input_key \u001b[39m=\u001b[39m get_prompt_input_key(inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory_variables)\n\u001b[0;32m     23\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m         prompt_input_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\memory\\utils.py:19\u001b[0m, in \u001b[0;36mget_prompt_input_key\u001b[1;34m(inputs, memory_variables)\u001b[0m\n\u001b[0;32m     17\u001b[0m prompt_input_keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(inputs)\u001b[39m.\u001b[39mdifference(memory_variables \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[0;32m     18\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(prompt_input_keys) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOne input key expected got \u001b[39m\u001b[39m{\u001b[39;00mprompt_input_keys\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m prompt_input_keys[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: One input key expected got ['topic', 'message']"
     ]
    }
   ],
   "source": [
    "full_chain.invoke({\"message\": \"주문 하기 전에 상품 정보 좀 알 수 있을까요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_query_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "order_change_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 변경을 전담하는 종업원이야.\n",
    "고객이 변경한 주문 내용을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 변경 내용이 잘못됐다면, 주문 변경 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 변경 내용을 정확히 파악할 때까지 반복해야돼.\n",
    "고객의 주문 변경을 정확히 파악했다면, 고객에게 고객이 주문을 변경한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 변경된 주문의 총 가격을 알려줘.\n",
    "\n",
    "고객의 주문 변경은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "order_cancel_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 취소를 전담하는 종업원이야.\n",
    "고객이 취소하려는 주문을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 취소 내용이 잘못됐다면, 주문 취소 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 취소 내용을 정확히 파악할 때\n",
    "고객의 주문 취소 내용을 정확히 파악했다면, 고객에게 고객이 주문을 취소한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 취소된 주문의 총 가격을 알려줘.\n",
    "\n",
    "고객이 취소하려는 주문은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='네, 물론입니다. 저희 가게에서 판매하는 상품은 떡케익5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩이 있습니다. 각 상품의 기본 판매 수량과 가격은 다음과 같습니다. 떡케익5호는 1개에 54,000원, 무지개 백설기 케익은 1개에 51,500원, 미니 백설기는 35개에 31,500원, 그리고 개별 모듬팩은 1개에 13,500원입니다. 어떤 상품을 원하시나요?')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branch = RunnableBranch(\n",
    "  (lambda x: \"주문 변경\" in x[\"topic\"], order_change_chain),\n",
    "  (lambda x: \"주문 취소\" in x[\"topic\"], order_cancel_chain),\n",
    "  general_query_chain,\n",
    ")\n",
    "\n",
    "full_chain = {\n",
    "    \"topic\": chain,\n",
    "    \"message\": lambda x: x[\"message\"]\n",
    "} | branch\n",
    "\n",
    "full_chain.invoke({\"message\": \"주문 하기 전에 상품 정보 좀 알 수 있을까요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_query_prompt = PromptTemplate.from_template(\"\"\"\n",
    "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "이전 대화를 고려하여 답변해야 해.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "   \n",
    "이전 대화는 다음과 같아:\n",
    "{chat_history}\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\")\n",
    "\n",
    "# general_query_prompt = ChatPromptTemplate.from_messages([\n",
    "#    (\"system\", system_prompt),\n",
    "#    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "#    (\"human\", \"{message}\")\n",
    "#    ])\n",
    "\n",
    "general_query_chain = LLMChain(\n",
    "    prompt=general_query_prompt,\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    )\n",
    "\n",
    "order_change_prompt = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 변경을 전담하는 종업원이야.\n",
    "고객이 변경한 주문 내용을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 변경 내용이 잘못됐다면, 주문 변경 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 변경 내용을 정확히 파악할 때까지 반복해야돼.\n",
    "고객의 주문 변경을 정확히 파악했다면, 고객에게 고객이 주문을 변경한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 변경된 주문의 총 가격을 알려줘.\n",
    "이전 대화를 고려해 답변해야 해.\n",
    "\n",
    "이전 대화는 다음과 같아:\n",
    "{chat_history}\n",
    "\n",
    "고객의 주문 변경은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\")\n",
    "order_change_chain = LLMChain(\n",
    "    prompt=order_change_prompt ,\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    )\n",
    "\n",
    "order_cancel_prompt = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 취소를 전담하는 종업원이야.\n",
    "고객이 취소하려는 주문을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 취소 내용이 잘못됐다면, 주문 취소 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 취소 내용을 정확히 파악할 때\n",
    "고객의 주문 취소 내용을 정확히 파악했다면, 고객에게 고객이 주문을 취소한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 취소된 주문의 총 가격을 알려줘.\n",
    "이전 대화를 고려해 답변해야 해.\n",
    "\n",
    "이전 대화는 다음과 같아:\n",
    "{chat_history}\n",
    "\n",
    "고객이 취소하려는 주문은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\")\n",
    "order_cancel_chain = LLMChain(\n",
    "    prompt=order_cancel_prompt,\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMChain 사용과 관련 있는 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
      "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
      "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
      "이전 대화를 고려하여 답변해야 해.\n",
      "\n",
      "가게에서 판매하는 상품 목록.\n",
      "1. 상품: 떡케익5호\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 54,000원\n",
      "2. 상품: 무지개 백설기 케익\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 51,500원\n",
      "3. 상품: 미니 백설기\n",
      "   기본 판매 수량: 35개\n",
      "   기본 판매 수량의 가격: 31,500원\n",
      "4. 상품: 개별 모듬팩\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 13,500원\n",
      "   \n",
      "이전 대화는 다음과 같아:\n",
      "[]\n",
      "\n",
      "고객이 문의는 다음과 같아:\n",
      "주문 하기 전에 상품 정보 좀 알 수 있을까요?\n",
      "답변:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One input key expected got ['topic', 'message']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chain.ipynb Cell 135\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y313sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m branch \u001b[39m=\u001b[39m RunnableBranch(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y313sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m   (\u001b[39mlambda\u001b[39;00m x: \u001b[39m\"\u001b[39m\u001b[39m주문 변경\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m x[\u001b[39m\"\u001b[39m\u001b[39mtopic\u001b[39m\u001b[39m\"\u001b[39m], order_change_chain),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y313sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m   (\u001b[39mlambda\u001b[39;00m x: \u001b[39m\"\u001b[39m\u001b[39m주문 취소\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m x[\u001b[39m\"\u001b[39m\u001b[39mtopic\u001b[39m\u001b[39m\"\u001b[39m], order_cancel_chain),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y313sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   general_query_chain,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y313sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y313sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m full_chain \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y313sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtopic\u001b[39m\u001b[39m\"\u001b[39m: chain,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y313sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlambda\u001b[39;00m x: x[\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y313sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m } \u001b[39m|\u001b[39m branch\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chain.ipynb#Y313sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m full_chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mmessage\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39m주문 하기 전에 상품 정보 좀 알 수 있을까요?\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1121\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1122\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m             patch_config(\n\u001b[0;32m   1125\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1126\u001b[0m             ),\n\u001b[0;32m   1127\u001b[0m         )\n\u001b[0;32m   1128\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\branch.py:194\u001b[0m, in \u001b[0;36mRunnableBranch.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 194\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefault\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m    195\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    196\u001b[0m             config\u001b[39m=\u001b[39;49mpatch_config(\n\u001b[0;32m    197\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(tag\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbranch:default\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    198\u001b[0m             ),\n\u001b[0;32m    199\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    200\u001b[0m         )\n\u001b[0;32m    201\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\chains\\base.py:85\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m     79\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     80\u001b[0m     \u001b[39minput\u001b[39m: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m     81\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     82\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m     83\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m     84\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m---> 85\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(\n\u001b[0;32m     86\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m     87\u001b[0m         callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     88\u001b[0m         tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     89\u001b[0m         metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     90\u001b[0m         run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     91\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m     92\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\chains\\base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    309\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m--> 310\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_outputs(\n\u001b[0;32m    311\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    312\u001b[0m )\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m include_run_info:\n\u001b[0;32m    314\u001b[0m     final_outputs[RUN_KEY] \u001b[39m=\u001b[39m RunInfo(run_id\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mrun_id)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\chains\\base.py:406\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[1;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_outputs(outputs)\n\u001b[0;32m    405\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49msave_context(inputs, outputs)\n\u001b[0;32m    407\u001b[0m \u001b[39mif\u001b[39;00m return_only_outputs:\n\u001b[0;32m    408\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\memory\\chat_memory.py:35\u001b[0m, in \u001b[0;36mBaseChatMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_context\u001b[39m(\u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Any], outputs: Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     input_str, output_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_input_output(inputs, outputs)\n\u001b[0;32m     36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_memory\u001b[39m.\u001b[39madd_user_message(input_str)\n\u001b[0;32m     37\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_memory\u001b[39m.\u001b[39madd_ai_message(output_str)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\memory\\chat_memory.py:22\u001b[0m, in \u001b[0;36mBaseChatMemory._get_input_output\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_input_output\u001b[39m(\n\u001b[0;32m     19\u001b[0m     \u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Any], outputs: Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]\n\u001b[0;32m     20\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m     21\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m         prompt_input_key \u001b[39m=\u001b[39m get_prompt_input_key(inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory_variables)\n\u001b[0;32m     23\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m         prompt_input_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\memory\\utils.py:19\u001b[0m, in \u001b[0;36mget_prompt_input_key\u001b[1;34m(inputs, memory_variables)\u001b[0m\n\u001b[0;32m     17\u001b[0m prompt_input_keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(inputs)\u001b[39m.\u001b[39mdifference(memory_variables \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[0;32m     18\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(prompt_input_keys) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOne input key expected got \u001b[39m\u001b[39m{\u001b[39;00mprompt_input_keys\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m prompt_input_keys[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: One input key expected got ['topic', 'message']"
     ]
    }
   ],
   "source": [
    "branch = RunnableBranch(\n",
    "  (lambda x: \"주문 변경\" in x[\"topic\"], order_change_chain),\n",
    "  (lambda x: \"주문 취소\" in x[\"topic\"], order_cancel_chain),\n",
    "  general_query_chain,\n",
    ")\n",
    "\n",
    "full_chain = {\n",
    "    \"topic\": chain,\n",
    "    \"message\": lambda x: x[\"message\"]\n",
    "} | branch\n",
    "\n",
    "full_chain.invoke({\"message\": \"주문 하기 전에 상품 정보 좀 알 수 있을까요?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "order_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
