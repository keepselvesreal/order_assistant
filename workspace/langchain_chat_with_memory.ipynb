{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "print(load_dotenv(find_dotenv(), override=True))\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI 프로젝트를 성공적으로 수행하기 위해서는 몇 가지 중요한 단계를 따라야 합니다. 아래는 AI 프로젝트를 잘 수행하기 위한 몇 가지 팁입니다:\\n\\n1. 목표 설정: 프로젝트의 목표를 잘 설정해야 합니다. 명확한 목표를 가지고 시작하여 프로젝트의 방향성을 제시해야 합니다. 이를 통해 팀원들의 동기부여와 집중력을 유지할 수 있습니다.\\n\\n2. 데이터 수집 및 전처리: AI 모델을 훈련시키기 위해서는 좋은 품질의 데이터가 필요합니다. 데이터 수집 및 전처리 단계에서 품질 좋은 데이터를 확보하기 위해 노력해야 합니다. 데이터의 라벨링, 정제, 이상치 처리 등을 포함한 전처리 작업도 중요합니다.\\n\\n3. 적절한 알고리즘 선택: 프로젝트의 목표와 데이터에 맞는 적절한 AI 알고리즘을 선택해야 합니다. 예를 들어, 이미지 분류를 수행하는 경우에는 CNN (Convolutional Neural Network)을 사용하는 것이 일반적입니다. 알고리즘 선택은 프로젝트의 성능과 결과에 큰 영향을 미칩니다.\\n\\n4. 모델 훈련 및 평가: 선택한 알고리즘을 사용하여 모델을 훈련시키고 검증하는 과정을 거쳐야 합니다. 모델 훈련 시에는 하이퍼파라미터 튜닝, 교차 검증 등의 기법을 사용하여 모델의 성능을 최대화해야 합니다.\\n\\n5. 결과 분석 및 개선: 모델의 결과를 분석하고 평가하여 개선할 수 있는 부분을 찾아야 합니다. 모델의 성능이 만족스럽지 않을 경우, 하이퍼파라미터 조정, 데이터 추가 수집, 모델 구조 변경 등을 고려해야 합니다.\\n\\n6. 지속적인 개발 및 유지보수: AI 모델은 환경이나 데이터의 변화에 따라 지속적으로 개선되어야 합니다. 프로젝트가 종료된 후에도 모델의 유지보수를 위해 시스템을 지속적으로 모니터링하고 필요한 업데이트를 수행해야 합니다.\\n\\n또한, 팀원들의 역량을 고려하여 역할 분담을 잘 정리하고, 커뮤니케이션을 원활하게 유지하는 것도 중요합니다. AI 프로젝트는 복잡한 과정이므로 팀원들 간의 협업과 지속적인 학습을 통해 성공적으로 진행될 수 있습니다.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI()\n",
    "chat([HumanMessage(content=\"AI 프로젝트 진행 중인데, 어떻게 하면 잘 할 수 있을까요?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='프로젝트 기반 학습은 실제 문제를 해결하기 위해 프로젝트를 수행하며 학습하는 방법입니다. 이 방법은 이론적인 개념을 학습한 후, 그 개념을 실제 문제에 적용해보는 과정을 거치면서 실제적인 능력을 향상시킬 수 있습니다.\\n\\n프로젝트 기반 학습은 다음과 같은 단계로 구성됩니다:\\n\\n1. 주제 설정: 프로젝트를 시작하기 전에 해결하고자 하는 문제나 목표를 설정합니다. 이는 학습하고자 하는 분야와 관련된 주제여야 합니다.\\n\\n2. 자료 수집: 주제에 필요한 자료를 수집하고 분석합니다. 이를 통해 문제에 대한 이해를 높일 수 있습니다.\\n\\n3. 설계 및 계획: 프로젝트를 수행하기 위한 계획을 세우고 설계합니다. 이 단계에서는 어떤 알고리즘, 모델, 도구 등을 사용할지 결정하게 됩니다.\\n\\n4. 구현: 설계한 알고리즘이나 모델을 구현하고, 데이터를 처리하거나 모델을 학습시킵니다. 이 단계에서 오류를 해결하고 결과를 분석하여 개선할 수 있습니다.\\n\\n5. 평가: 구현한 모델을 평가하고 결과를 분석합니다. 이 결과를 통해 모델의 성능을 개선하거나 다른 방법을 시도할 수 있습니다.\\n\\n6. 발표 및 문서화: 프로젝트 결과를 발표하고 문서화하여 다른 사람들과 공유합니다. 이를 통해 다른 사람들의 피드백을 받을 수 있고, 자신의 학습 내용을 정리할 수 있습니다.\\n\\n프로젝트 기반 학습은 이론적인 개념을 실제 문제에 적용해보며 실력을 향상시킬 수 있는 장점이 있습니다. 또한, 실제 문제 해결 과정을 경험하면서 문제 해결 능력과 창의력을 키울 수 있습니다.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"넌 나를 딥러닝 마스터로 이끌어줄 인공지능이야.\"),\n",
    "    HumanMessage(content=\"프로젝트 기반 학습 방법에 관해 알려줘.\")\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Love is more valuable than anything else and therefore also dangerous.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=chat)  \n",
    "conversation.run(\"Translate this sentence from korean to english: 사랑은 그 무엇보다 가치있고 그래서 또 위험한 법\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'愛は他の何よりも価値があり、それゆえに危険でもあります。 (Ai wa hoka no nani yori mo kachi ga ari, sore yue ni kiken de mo arimasu.)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Translate it to Japanese.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: not much you\\nAI: not much'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: hi\\nAI: whats up\\nHuman: not much you\\nAI: not much'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Human: hi\\nAI: whats up', 'Human: not much you\\nAI: not much']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.split(r'\\n(?=H)', memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.prompts.chat.HumanMessagePromptTemplate"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ts\n",
    "\n",
    "HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ts\n",
    "\n",
    "HumanMessagePromptTemplate.from_template(\"{question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(return_messages=True, memory_key='chat_history')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ts\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True)\n",
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내언정 - MessagesPlaceholder(variable_name=\"chat_history\")와 ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True)\n",
    "- MessagesPlaceholder가 비워둔 자리에 ConversationBufferMemory에 담긴 메시지를 넣는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: hi\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'hi',\n",
       " 'chat_history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content='Hello! How can I assist you today?')],\n",
       " 'text': 'Hello! How can I assist you today?'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Prompt \n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
    "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
    "conversation({\"question\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: hi\n",
      "AI: Hello! How can I assist you today?\n",
      "Human: 한국말도 할 줄 알아?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': '한국말도 할 줄 알아?',\n",
       " 'chat_history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content='Hello! How can I assist you today?'),\n",
       "  HumanMessage(content='한국말도 할 줄 알아?'),\n",
       "  AIMessage(content='네, 저는 한국어도 할 줄 압니다. 무엇을 도와드릴까요?')],\n",
       " 'text': '네, 저는 한국어도 할 줄 압니다. 무엇을 도와드릴까요?'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ts\n",
    "\n",
    "conversation({\"question\": \"한국말도 할 줄 알아?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationBufferMemory()의 return_message=False로 설정\n",
    "- 내언정: 보관하고 있는 걸 뱉는 거, 달리 말하면 반환하는 설정인 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "variable chat_history should be a list of base messages, got ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\langchain_chat_with_memory.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chat_with_memory.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m conversation \u001b[39m=\u001b[39m LLMChain(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chat_with_memory.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     llm\u001b[39m=\u001b[39mllm,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chat_with_memory.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     prompt\u001b[39m=\u001b[39mprompt,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chat_with_memory.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chat_with_memory.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     memory\u001b[39m=\u001b[39mmemory\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chat_with_memory.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chat_with_memory.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/langchain_chat_with_memory.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m conversation({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mhi\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\chains\\base.py:308\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    307\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 308\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    309\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    310\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    311\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    312\u001b[0m )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\chains\\base.py:302\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    295\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    296\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    297\u001b[0m     inputs,\n\u001b[0;32m    298\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[0;32m    299\u001b[0m )\n\u001b[0;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 302\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    303\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    304\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    306\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    307\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\chains\\llm.py:93\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[0;32m     89\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     90\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m     91\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     92\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m---> 93\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m     94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\chains\\llm.py:102\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\n\u001b[0;32m     97\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     98\u001b[0m     input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]],\n\u001b[0;32m     99\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    100\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    101\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    104\u001b[0m         prompts,\n\u001b[0;32m    105\u001b[0m         stop,\n\u001b[0;32m    106\u001b[0m         callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    107\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs,\n\u001b[0;32m    108\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\chains\\llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.prep_prompts\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mfor\u001b[39;00m inputs \u001b[39min\u001b[39;00m input_list:\n\u001b[0;32m    137\u001b[0m     selected_inputs \u001b[39m=\u001b[39m {k: inputs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt\u001b[39m.\u001b[39minput_variables}\n\u001b[1;32m--> 138\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprompt\u001b[39m.\u001b[39;49mformat_prompt(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mselected_inputs)\n\u001b[0;32m    139\u001b[0m     _colored_text \u001b[39m=\u001b[39m get_colored_text(prompt\u001b[39m.\u001b[39mto_string(), \u001b[39m\"\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    140\u001b[0m     _text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPrompt after formatting:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m _colored_text\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\prompts\\chat.py:339\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format_prompt\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_prompt\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m    331\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[39m    Format prompt. Should return a PromptValue.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39m        PromptValue.\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_messages(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    340\u001b[0m     \u001b[39mreturn\u001b[39;00m ChatPromptValue(messages\u001b[39m=\u001b[39mmessages)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\prompts\\chat.py:586\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    579\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[0;32m    580\u001b[0m ):\n\u001b[0;32m    581\u001b[0m     rel_params \u001b[39m=\u001b[39m {\n\u001b[0;32m    582\u001b[0m         k: v\n\u001b[0;32m    583\u001b[0m         \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    584\u001b[0m         \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m message_template\u001b[39m.\u001b[39minput_variables\n\u001b[0;32m    585\u001b[0m     }\n\u001b[1;32m--> 586\u001b[0m     message \u001b[39m=\u001b[39m message_template\u001b[39m.\u001b[39;49mformat_messages(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrel_params)\n\u001b[0;32m    587\u001b[0m     result\u001b[39m.\u001b[39mextend(message)\n\u001b[0;32m    588\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\prompts\\chat.py:99\u001b[0m, in \u001b[0;36mMessagesPlaceholder.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m value \u001b[39m=\u001b[39m kwargs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariable_name]\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mlist\u001b[39m):\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    100\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvariable \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariable_name\u001b[39m}\u001b[39;00m\u001b[39m should be a list of base messages, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m     )\n\u001b[0;32m    103\u001b[0m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m value:\n\u001b[0;32m    104\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(v, BaseMessage):\n",
      "\u001b[1;31mValueError\u001b[0m: variable chat_history should be a list of base messages, got "
     ]
    }
   ],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Prompt \n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
    "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=False)\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
    "conversation({\"question\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: hi\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'hi',\n",
       " 'chat_history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content='Hello! How can I assist you today?')],\n",
       " 'text': 'Hello! How can I assist you today?'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Prompt \n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
    "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
    "conversation({\"question\": \"hi\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "order_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
