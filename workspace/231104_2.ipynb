{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 분류기 \n",
    "\n",
    "from typing import Literal\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from langchain.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output parser \n",
    "\n",
    "from typing import List\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "print(load_dotenv(find_dotenv(), override=True))\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# memory 테스트\n",
    "- 함수 안에서 memory 객체 참조할 수 없는 듯. gpt4에 따르면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conversation(dict):\n",
    "    memory.save_context({\"inputs\": dict[\"customer_message\"]}, {\"output\": dict[\"ai_response\"]})\n",
    "    return dict[\"ai_response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "<이전 대화>와 <현재 사용자 입력을>를 모두 고려해서 적절히 답변해줘.\n",
    "\n",
    "<이전 대화>:\n",
    "{history}\n",
    "\n",
    "<현재 사용자 입력>:\n",
    "{customer_message}\n",
    "답변:\"\"\")\n",
    "\n",
    "response_chain = (\n",
    "    RunnablePassthrough.assign(ai_response=prompt | ChatOpenAI(model=\"gpt-3.5-turbo-0613\") | StrOutputParser()) | \n",
    "    RunnableLambda(save_conversation)\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatHandler(threading.Thread):\n",
    "    def __init__(self, new_message_list):\n",
    "        super().__init__()\n",
    "        self.new_message_list = new_message_list\n",
    "        self.keep_running = True\n",
    "        self.memory = ConversationBufferMemory(return_messages=True)\n",
    "        self.cnt = 0\n",
    "        \n",
    "    def run(self):\n",
    "        while self.keep_running:\n",
    "            # 새 메시지를 처리합니다\n",
    "            if self.cnt >= len(self.new_message_list):\n",
    "                break\n",
    "            new_message = self.new_messge_list[self.cnt]\n",
    "            response = self.generate_response(new_message, self.memory)\n",
    "            self.post_response(new_message, response)\n",
    "            # 일정 시간마다 새로운 메시지를 확인합니다. (예: 5초마다)\n",
    "            time.sleep(1)\n",
    "            self.cnt += 1\n",
    "\n",
    "    def generate_response(self, new_message, memory):\n",
    "        # 여기에 LLM을 사용하여 응답을 생성하는 로직을 구현합니다.\n",
    "        # 예시: return f\"Response to {message}\"\n",
    "        memory = self.memory\n",
    "        chat_history = memory.load_memory_variables({})\n",
    "        response = response_chain(customer_message=new_message, history=chat_history)\n",
    "        return response\n",
    "\n",
    "    def post_response(self, new_message, response):\n",
    "        # 여기에 생성된 응답을 웹상의 메시지 창에 입력하는 로직을 구현합니다.\n",
    "        # 예시: post_message_to_web(response)\n",
    "        print(f\"메시지: {new_message}\\nAI 응답: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Chain 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conversation(dict):\n",
    "    # memory = dict['memory']\n",
    "    memory.save_context({\"inputs\": dict[\"customer_message\"]}, {\"output\": dict[\"ai_response\"]})\n",
    "    return dict[\"ai_response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history(before)-> {'history': [HumanMessage(content='안녕 내 이름은 나들이라고 해'), AIMessage(content='안녕 나들이! 반가워. 어떤 일로 나와 함께 놀러 갈까?'), HumanMessage(content='나들이가 아니라 \"나들\"이야'), AIMessage(content='나들이가 아니라 \"나들\"이야? 그렇군요. 죄송하지만, \"나들이\"라는 단어를 사용하고 싶으시면 어떤 일로 함께 놀러 갈지 알려주세요. 함께 즐거운 시간을 보내기 위해 기다리고 있을게요!'), HumanMessage(content='나들이랑 아무 상관 없고 그냥 이름이 나들이란 말이야'), AIMessage(content='그렇군요! 이름이 \"나들이\"라는 말이 맘에 들지 않으시다는 거군요. 죄송하지만, \"나들이\"라는 단어를 사용하고 싶으시면 어떤 활동으로 함께 놀러 갈지 알려주세요. 함께 즐거운 시간을 보내기 위해 기다리고 있을게요!'), HumanMessage(content=\"나들이란 말과 아무 상관 없다니깐. 그냥 이름이 '나들'이라고~~\"), AIMessage(content=\"알겠습니다. 이름이 '나들'이라고 하셨군요. 그렇다면 어떤 활동으로 함께 놀러 갈까요? 함께 즐거운 시간을 보내기 위해 기다리고 있을게요!\")]}\n",
      "-----------------------------------------------------------------------------\n",
      "당신의 이름은 '나들'이라고 하셨죠! 함께 어떤 활동으로 놀러 갈까요? 즐거운 시간을 보내기 위해 기다리고 있을게요!\n",
      "-----------------------------------------------------------------------------\n",
      "history(after)-> {'history': [HumanMessage(content='안녕 내 이름은 나들이라고 해'), AIMessage(content='안녕 나들이! 반가워. 어떤 일로 나와 함께 놀러 갈까?'), HumanMessage(content='나들이가 아니라 \"나들\"이야'), AIMessage(content='나들이가 아니라 \"나들\"이야? 그렇군요. 죄송하지만, \"나들이\"라는 단어를 사용하고 싶으시면 어떤 일로 함께 놀러 갈지 알려주세요. 함께 즐거운 시간을 보내기 위해 기다리고 있을게요!'), HumanMessage(content='나들이랑 아무 상관 없고 그냥 이름이 나들이란 말이야'), AIMessage(content='그렇군요! 이름이 \"나들이\"라는 말이 맘에 들지 않으시다는 거군요. 죄송하지만, \"나들이\"라는 단어를 사용하고 싶으시면 어떤 활동으로 함께 놀러 갈지 알려주세요. 함께 즐거운 시간을 보내기 위해 기다리고 있을게요!'), HumanMessage(content=\"나들이란 말과 아무 상관 없다니깐. 그냥 이름이 '나들'이라고~~\"), AIMessage(content=\"알겠습니다. 이름이 '나들'이라고 하셨군요. 그렇다면 어떤 활동으로 함께 놀러 갈까요? 함께 즐거운 시간을 보내기 위해 기다리고 있을게요!\"), HumanMessage(content='내 이름이 뭐라고?'), AIMessage(content=\"당신의 이름은 '나들'이라고 하셨죠! 함께 어떤 활동으로 놀러 갈까요? 즐거운 시간을 보내기 위해 기다리고 있을게요!\")]}\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "<이전 대화>와 <현재 사용자 입력을>를 모두 고려해서 적절히 답변해줘.\n",
    "\n",
    "<이전 대화>:\n",
    "{history}\n",
    "\n",
    "<현재 사용자 입력>:\n",
    "{customer_message}\n",
    "답변:\"\"\")\n",
    "\n",
    "# RunnablePassthrough.assign(ai_response=general_prompt | ChatOpenAI(model=\"gpt-3.5-turbo-0613\") | StrOutputParser()) | RunnableLambda(save_conversation)\n",
    "response_chain = (\n",
    "    RunnablePassthrough.assign(ai_response=prompt | ChatOpenAI(model=\"gpt-3.5-turbo-0613\") | StrOutputParser()) | \n",
    "    RunnableLambda(save_conversation)\n",
    "    ) \n",
    "\n",
    "# 동작 점검 \n",
    "history = memory.load_memory_variables({})\n",
    "print('history(before)->', history)\n",
    "print('-'*77)\n",
    "response = response_chain.invoke({\"customer_message\": \"내 이름이 뭐라고?\", \"history\": history})\n",
    "print(response)\n",
    "print('-'*77)\n",
    "print('history(after)->', history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memory를 chain에 전달해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이전 대화 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\231104_2.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/231104_2.ipynb#X60sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m memory\n",
      "\u001b[1;31mNameError\u001b[0m: name 'memory' is not defined"
     ]
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (960249448.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    delete memory\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# memory 변수가 없어서 오류 발생하는 거?\n",
    "delete memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구 버전\n",
    "# response_chain = (\n",
    "#     RunnablePassthrough.assign(ai_response=prompt | ChatOpenAI(model=\"gpt-3.5-turbo-0613\") | StrOutputParser()) | \n",
    "#     RunnableLambda(save_conversation)\n",
    "#     ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_memory(dict):\n",
    "    memory = dict['memory']\n",
    "    chat_history = memory.load_memory_variables({})\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conversation(dict):\n",
    "    memory = dict['memory']\n",
    "    memory.save_context({\"inputs\": dict[\"customer_message\"]}, {\"output\": dict[\"ai_response\"]})\n",
    "    return dict[\"ai_response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"안녕 '나들'이! 반가워. 무엇을 도와줄까?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "<이전 대화>와 <현재 사용자 입력을>를 모두 고려해서 적절히 답변해줘.\n",
    "\n",
    "<이전 대화>:\n",
    "{history}\n",
    "\n",
    "<현재 사용자 입력>:\n",
    "{customer_message}\n",
    "답변:\"\"\")\n",
    "\n",
    "t_chain = (\n",
    "    RunnablePassthrough.assign(history=RunnableLambda(load_memory)) |\n",
    "    RunnablePassthrough.assign(ai_response=prompt | ChatOpenAI(model=\"gpt-3.5-turbo-0613\") | StrOutputParser()) |\n",
    "    RunnableLambda(save_conversation)\n",
    ")\n",
    "\n",
    "# 동작 점검\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "t_chain.invoke({\"customer_message\": \"안녕 나는 '나들'이라고 해\", \"memory\": memory})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  history: RunnableLambda(...)\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    ai_response: PromptTemplate(input_variables=['customer_message', 'history'], template='\\n<이전 대화>와 <현재 사용자 입력을>를 모두 고려해서 적절히 답변해줘.\\n\\n<이전 대화>:\\n{history}\\n\\n<현재 사용자 입력>:\\n{customer_message}\\n답변:')\n",
       "                 | ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0613', openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')\n",
       "                 | StrOutputParser()\n",
       "  })\n",
       "| RunnableLambda(...)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "<이전 대화>와 <현재 사용자 입력을>를 모두 고려해서 적절히 답변해줘.\n",
    "\n",
    "<이전 대화>:\n",
    "{history}\n",
    "\n",
    "<현재 사용자 입력>:\n",
    "{customer_message}\n",
    "답변:\"\"\")\n",
    "\n",
    "response_chain = (\n",
    "    RunnablePassthrough.assign(history=RunnableLambda(load_memory)) |\n",
    "    RunnablePassthrough.assign(ai_response=prompt | ChatOpenAI(model=\"gpt-3.5-turbo-0613\") | StrOutputParser()) |\n",
    "    RunnableLambda(save_conversation)\n",
    ")\n",
    "response_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\231104_2.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/231104_2.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdel\u001b[39;00m memory\n",
      "\u001b[1;31mNameError\u001b[0m: name 'memory' is not defined"
     ]
    }
   ],
   "source": [
    "del memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(new_message, memory):\n",
    "        response = response_chain.invoke({\"customer_message\": new_message, \"memory\": memory})\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history(before)->  {'history': [HumanMessage(content=\"안녕 나는 '나들'이라고 해\"), AIMessage(content=\"안녕 '나들'! 반가워. 무엇을 도와줄까?\"), HumanMessage(content='넌 이름이 뭐야?'), AIMessage(content=\"나는 '나들'이라고 해. 어떤 도움이 필요한거야?\")]}\n",
      "-----------------------------------------------------------------------------\n",
      "\"안녕 나들이! 내 이름은 '나들'이야. 무엇을 도와줄까?\"\n",
      "-----------------------------------------------------------------------------\n",
      "history(before)->  {'history': [HumanMessage(content=\"안녕 나는 '나들'이라고 해\"), AIMessage(content=\"안녕 '나들'! 반가워. 무엇을 도와줄까?\"), HumanMessage(content='넌 이름이 뭐야?'), AIMessage(content=\"나는 '나들'이라고 해. 어떤 도움이 필요한거야?\"), HumanMessage(content='아니 내 이름이 나들이고 너의 이름이 뭐냐고?'), AIMessage(content='\"안녕 나들이! 내 이름은 \\'나들\\'이야. 무엇을 도와줄까?\"')]}\n"
     ]
    }
   ],
   "source": [
    "print('history(before)-> ', memory.load_memory_variables({}))\n",
    "print('-'*77)\n",
    "response = generate_response(\"아니 내 이름이 나들이고 너의 이름이 뭐냐고?\", memory)\n",
    "print(response)\n",
    "print('-'*77)\n",
    "print('history(before)-> ', memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content=\"안녕 나는 '나들'이라고 해\"), AIMessage(content=\"안녕 '나들'! 반가워. 무엇을 도와줄까?\")]), return_messages=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Class 구성\n",
    "- db는 어떻게 관리?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "class ChatHandler(threading.Thread):\n",
    "    def __init__(self, new_message_list):\n",
    "        super().__init__()\n",
    "        self.new_message_list = new_message_list\n",
    "        self.keep_running = True\n",
    "        self.user_name = '나들'\n",
    "        # self.order_ids = {} # db 이용하니 이렇게 관리 안해도 될 듯. 더 복잡하고 비효율적일 듯\n",
    "        self.memory = ConversationBufferMemory(return_messages=True)\n",
    "        self.cnt = 0\n",
    "        \n",
    "    def run(self):\n",
    "        while self.keep_running:\n",
    "            # 새 메시지를 처리합니다.\n",
    "            try:\n",
    "                new_message = self.new_message_list[self.cnt]\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "                print(\"3초 동안 입력 메시지가 주어지지 않아 대화를 종료합니다.\")\n",
    "                break\n",
    "            response = self.generate_response(new_message, self.memory)\n",
    "            # if respone.get('end_flag'):\n",
    "            #     self.keep_running = False\n",
    "            #     break \n",
    "            self.post_response(new_message, response)\n",
    "            # 일정 시간마다 새로운 메시지를 확인합니다. (예: 5초마다)\n",
    "            time.sleep(1)\n",
    "            self.cnt += 1\n",
    "\n",
    "    def generate_response(self, new_message, memory): # 바로 response_chain.invoke() 사용이 나으려나? (gpt에) 물어보기\n",
    "        # 여기에 LLM을 사용하여 응답을 생성하는 로직을 구현합니다.\n",
    "        # 예시: return f\"Response to {message}\"\n",
    "        response = response_chain.invoke({\"customer_message\": new_message, \"memory\":memory})\n",
    "        return response\n",
    "\n",
    "    def post_response(self, new_message, response):\n",
    "        # 여기에 생성된 응답을 웹상의 메시지 창에 입력하는 로직을 구현합니다.\n",
    "        # 예시: post_message_to_web(response)\n",
    "        print(f\"메시지: {new_message}\\nAI 응답: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메시지: 상품 정보 좀 알 수 있을까요\n",
      "AI 응답: 물론입니다. 어떤 상품에 대해 알고 싶으신가요? 상품명이나 카테고리를 알려주세요.\n",
      "3초 동안 입력 메시지가 주어지지 않아 대화를 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "chatroom1 = ChatHandler([\"상품 정보 좀 알 수 있을까요\"])\n",
    "chatroom1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(new_messsage, memory):\n",
    "    status = chat_status_chain.invoke({\"customer_message\": new_message, \"memory\": memory})\n",
    "    if status[\"conv_status\"] == \"대화 중\":\n",
    "        response = chat_chain.invoke({\"customer_message\": new_message, 'history': memory})\n",
    "    else:\n",
    "        response = chat_end_chain.invoke({\"customer_message\": new_message, 'history': memory})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 나중에 새로운 답변 입력 안한 채 일정 시간 지나면 메시지 작성하고 루프 종료되게 하는 부분 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class ChatHandler(threading.Thread):\n",
    "    def __init__(self, new_message_element):\n",
    "        super().__init__()\n",
    "        self.new_message_element = None\n",
    "        self.keep_running = True\n",
    "        self.user_name = '나들'\n",
    "        self.order_ids = {}\n",
    "        self.memory = ConversationBufferMemory(return_messages=True)\n",
    "        \n",
    "    def run(self):\n",
    "        while self.keep_running:\n",
    "            # 새 메시지를 처리합니다.\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                pass\n",
    "            # new_message = self.extract_message(self.new_message_element)\n",
    "            except:\n",
    "                \n",
    "            if new_message: # if문 없애도 될 듯?\n",
    "                response = self.generate_response(new_message)\n",
    "                if respone.get('end_flag'):\n",
    "                    chat_history = history= self.memory.load_memory_variables({})[\"history\"]\n",
    "                    handle_order(chat_history, self.id)\n",
    "                    self.keep_running = False\n",
    "                    break\n",
    "                self.post_response(response)\n",
    "\n",
    "            # 일정 시간마다 새로운 메시지를 확인합니다. (예: 5초마다)\n",
    "            time.sleep(5)\n",
    "    def handle_order(self,chat_history):\n",
    "        # 여기에 주문을 처리하는 로직을 구현합니다.\n",
    "        \n",
    "        return \"주문 처리 완료\"\n",
    "\n",
    "    def extract_message(self, element):\n",
    "        # 여기에 웹 요소로부터 메시지를 추출하는 로직을 구현합니다.\n",
    "        # 예시: return extract_text_from_element(element)\n",
    "        return \"새 메시지 내용\"\n",
    "\n",
    "    def generate_response(self, message):\n",
    "        # 여기에 LLM을 사용하여 응답을 생성하는 로직을 구현합니다.\n",
    "        # 예시: return f\"Response to {message}\"\n",
    "        handle_chain(message)\n",
    "        return \"생성된 응답\"\n",
    "\n",
    "    def post_response(self, response):\n",
    "        # 여기에 생성된 응답을 웹상의 메시지 창에 입력하는 로직을 구현합니다.\n",
    "        # 예시: post_message_to_web(response)\n",
    "        print(f\"응답 전송: {response}\")\n",
    "\n",
    "    def stop(self):\n",
    "        self.keep_running = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aync 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단순 버전\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "from threading import Event\n",
    "\n",
    "class ChatHandler(threading.Thread):\n",
    "    def __init__(self, user_name, new_message_list):\n",
    "        super().__init__()\n",
    "        self.new_message_list = new_message_list\n",
    "        self.keep_running = True\n",
    "        self.user_name = user_name\n",
    "        # self.order_ids = {} # db 이용하니 이렇게 관리 안해도 될 듯. 더 복잡하고 비효율적일 듯\n",
    "        self.memory = ConversationBufferMemory(return_messages=True)\n",
    "        self.cnt = 0\n",
    "        self.error_event = Event()\n",
    "        \n",
    "    def run(self):\n",
    "        while self.keep_running:\n",
    "            # 새 메시지를 처리합니다.\n",
    "            try:\n",
    "                new_message = self.new_message_list[self.cnt]\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "                print(\"3초 동안 입력 메시지가 주어지지 않아 대화를 종료합니다.\")\n",
    "                break\n",
    "            response = self.generate_response(new_message, self.memory)\n",
    "            # if respone.get('end_flag'):\n",
    "            #     self.keep_running = False\n",
    "            #     break \n",
    "            self.post_response(new_message, response)\n",
    "            # 일정 시간마다 새로운 메시지를 확인합니다. (예: 5초마다)\n",
    "            time.sleep(1)\n",
    "            self.cnt += 1\n",
    "\n",
    "    def generate_response(self, new_message, memory): # 바로 response_chain.invoke() 사용이 나으려나? (gpt에) 물어보기\n",
    "        # 여기에 LLM을 사용하여 응답을 생성하는 로직을 구현합니다.\n",
    "        # 예시: return f\"Response to {message}\"\n",
    "        response = response_chain.invoke({\"customer_message\": new_message, \"memory\":memory})\n",
    "        return response\n",
    "\n",
    "    def post_response(self, new_message, response):\n",
    "        # 여기에 생성된 응답을 웹상의 메시지 창에 입력하는 로직을 구현합니다.\n",
    "        # 예시: post_message_to_web(response)\n",
    "        print(f\"메시지: {new_message}\\nAI 응답: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------\n",
      "메시지: 떡케익 2개, 개별 모듬팩 3개 할게요\n",
      "AI 응답: 떡케익 2개와 개별 모듬팩 3개를 주문하시는 건가요? 어떤 종류의 떡케익과 개별 모듬팩을 원하시나요? 제가 메뉴를 안내해 드릴게요.\n",
      "3초 동안 입력 메시지가 주어지지 않아 대화를 종료합니다.\n",
      "메시지: 지금 주문하면 언제 받아볼 수 있나요?\n",
      "AI 응답: 주문하신 상품에 따라서 배송일정이 달라질 수 있습니다. 일반적으로는 주문 후 1~3일 내에 상품을 받아보실 수 있으나, 상황에 따라 배송일정이 변경될 수도 있습니다. 주문하시는 상품의 배송일정은 상품 페이지에서 확인하실 수 있으며, 주문 완료 후에는 배송 추적 번호를 통해 실시간으로 배송 상황을 확인하실 수 있습니다.\n",
      "3초 동안 입력 메시지가 주어지지 않아 대화를 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "# 여러 대화방에 대한 스레드 생성 및 시작.\n",
    "user_names = ['user1', 'user2', 'user3']  # 실제 대화방 ID를 리스트 형태로 관리.\n",
    "messages = [['상품 정보 좀 알 수 있을까요'], ['떡케익 2개, 개별 모듬팩 3개 할게요'], ['지금 주문하면 언제 받아볼 수 있나요?']]\n",
    "threads = []\n",
    "\n",
    "for user_name, message in zip(user_names, messages):\n",
    "    chat_bot = ChatHandler(user_name, message)\n",
    "    chat_bot.start()\n",
    "    threads.append(chat_bot)\n",
    "    print('-'*77)\n",
    "\n",
    "# 모든 스레드가 완료될 때까지 기다림.\n",
    "for thread in threads:\n",
    "    thread.join(timeout=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 실행되지 않았던 스레드가 실행됐음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ktsfr\\Anaconda3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\ktsfr\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\ktsfr\\AppData\\Local\\Temp\\ipykernel_20336\\4081997892.py\", line 6, in monitor_chat_bots\n",
      "AttributeError: 'ChatHandler' object has no attribute 'error_event'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메시지: 상품 정보 좀 알 수 있을까요\n",
      "AI 응답: 저희 가게에서는 다양한 상품을 판매하고 있습니다. 어떤 상품에 관심이 있으신가요? 카테고리나 특정 상품에 대한 정보를 알려주세요. 그럴게요. 어떤 상품에 관심이 있으신가요? 카테고리나 특정 상품에 대한 정보를 알려주세요. 그럴게요. 어떤 상품에 관심이 있으신가요? 카테고리나 특정 상품에 대한 정보를 알려주세요. 그럴게요. 어떤 상품에 관심이 있으신가요? 카테고리나 특정 상품에 대한 정보를 알려주세요. 그럴게요. 어떤 상품에 관심이 있으신가요? 카테고리나 특정 상품에 대한 정보를 알려주세요. 그럴게요. 어떤 상품에 관심이 있으신가요? 카테고리나 특정 상품에 대한 정보를 알려주세요. 그럴게요. 어떤 상품에 관심이 있으신가요? 카테고리나 특정 상품에 대한 정보를 알려주세요. 그럴게요. 어떤 상품에 관심이 있으신가요? 카테고리나 특정 상품에 대한 정보를 알려주세요.\n",
      "3초 동안 입력 메시지가 주어지지 않아 대화를 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread, Event\n",
    "\n",
    "def monitor_chat_bots(chat_bots):\n",
    "    while True:\n",
    "        for bot in chat_bots:\n",
    "            if bot.error_event.is_set():\n",
    "                log_error(f\"Restarting bot in room {bot.room_id} due to error.\")\n",
    "                bot.error_event.clear()  # 오류 상태 초기화\n",
    "                bot.start()  # 스레드를 다시 시작\n",
    "        time.sleep(10)  # 10초 마다 감시\n",
    "        \n",
    "# 감시 스레드 생성 및 시작\n",
    "monitor_thread = Thread(target=monitor_chat_bots, args=(threads,))\n",
    "monitor_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread list에 추가 -----------------------------------------------------------------------------\n",
      "thread list에 추가 -----------------------------------------------------------------------------\n",
      "thread list에 추가 -----------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메시지: 상품 정보 좀 알 수 있을까요\n",
      "AI 응답: 어떤 상품에 대한 정보를 원하시나요? 상품의 이름이나 종류를 알려주시면 더 자세한 도움을 드릴 수 있습니다.\n",
      "메시지: 떡케익 2개, 개별 모듬팩 3개 할게요\n",
      "AI 응답: 주문 감사합니다! 떡케익 2개와 개별 모듬팩 3개를 준비해드리겠습니다. 손님의 맛있는 식사를 기대해주세요!\n",
      "3초 동안 입력 메시지가 주어지지 않아 대화를 종료합니다.\n",
      "thread.join에 실행 -----------------------------------------------------------------------------\n",
      "3초 동안 입력 메시지가 주어지지 않아 대화를 종료합니다.\n",
      "thread.join에 실행 -----------------------------------------------------------------------------\n",
      "메시지: 지금 주문하면 언제 받아볼 수 있나요?\n",
      "AI 응답: 주문하신 제품은 배송지 및 현재 재고 상황에 따라 다를 수 있습니다. 보통 주문 후 1~3일 이내에 받아보실 수 있으나, 지연될 수도 있습니다. 정확한 배송일정은 주문 시스템에서 확인하실 수 있습니다.\n",
      "3초 동안 입력 메시지가 주어지지 않아 대화를 종료합니다.\n",
      "thread.join에 실행 -----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 여러 대화방에 대한 스레드 생성 및 시작.\n",
    "user_names = ['user1', 'user2', 'user3']  # 실제 대화방 ID를 리스트 형태로 관리.\n",
    "messages = [['상품 정보 좀 알 수 있을까요'], ['떡케익 2개, 개별 모듬팩 3개 할게요'], ['지금 주문하면 언제 받아볼 수 있나요?']]\n",
    "threads = []\n",
    "\n",
    "for user_name, message in zip(user_names, messages):\n",
    "    chat_bot = ChatHandler(user_name, message)\n",
    "    chat_bot.start()\n",
    "    threads.append(chat_bot)\n",
    "    print('thread list에 추가', '-'*77)\n",
    "\n",
    "# 모든 스레드가 완료될 때까지 기다림.\n",
    "for thread in threads:\n",
    "    thread.join(timeout=30)\n",
    "    print('thread.join에 실행', '-'*77)\n",
    "    \n",
    "def monitor_chat_bots(chat_bots):\n",
    "    while True:\n",
    "        for bot in chat_bots:\n",
    "            if bot.error_event.is_set():\n",
    "                log_error(f\"Restarting bot in room {bot.room_id} due to error.\")\n",
    "                bot.error_event.clear()  # 오류 상태 초기화\n",
    "                bot.start()  # 스레드를 다시 시작\n",
    "        time.sleep(10)  # 10초 마다 감시\n",
    "        \n",
    "# 감시 스레드 생성 및 시작\n",
    "monitor_thread = Thread(target=monitor_chat_bots, args=(threads,))\n",
    "monitor_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<threading.Event object at 0x0000017DE6C30F40>\n",
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(threads[0].error_event)\n",
    "print(threads[0].error_event.is_set())\n",
    "print(threads[0].error_event.clear())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "class ChatHandler(threading.Thread):\n",
    "    def __init__(self, new_message_list):\n",
    "        super().__init__()\n",
    "        self.new_message_list = new_message_list\n",
    "        self.keep_running = True\n",
    "        self.user_name = '나들'\n",
    "        # self.order_ids = {} # db 이용하니 이렇게 관리 안해도 될 듯. 더 복잡하고 비효율적일 듯\n",
    "        self.memory = ConversationBufferMemory(return_messages=True)\n",
    "        self.cnt = 0\n",
    "        \n",
    "    def run(self):\n",
    "        while self.keep_running:\n",
    "            # 새 메시지를 처리합니다.\n",
    "            try:\n",
    "                new_message = self.new_message_list[self.cnt]\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "                print(\"3초 동안 입력 메시지가 주어지지 않아 대화를 종료합니다.\")\n",
    "                break\n",
    "            except NetworkError as ne::\n",
    "                # 네트워크 오류 처리\n",
    "                log_error(f\"Network error for room {self.room_id}: {ne}\")\n",
    "                continue  # 다음 반복으로 넘어가서 재시도.\n",
    "            \n",
    "            try:\n",
    "                response = self.generate_response(new_message, self.memory)\n",
    "                # if respone.get('end_flag'):\n",
    "                #     self.keep_running = False\n",
    "                #     break\n",
    "            except ResponseError as re:\n",
    "                # 응답 생성 오류 처리\n",
    "                log_error(f\"Response generation error in room {self.room_id}: {re}\")\n",
    "                # 해당 메시지에 대해 오류 응답을 보내거나 건너뛰기 등의 처리를 할 수 있습니다.\n",
    "                self.handle_invalid_input(message)\n",
    "\n",
    "            self.post_response(new_message, response)\n",
    "            # 일정 시간마다 새로운 메시지를 확인합니다. (예: 5초마다)\n",
    "            time.sleep(1)\n",
    "            self.cnt += 1\n",
    "\n",
    "    def generate_response(self, new_message, memory): # 바로 response_chain.invoke() 사용이 나으려나? (gpt에) 물어보기\n",
    "        # 여기에 LLM을 사용하여 응답을 생성하는 로직을 구현합니다.\n",
    "        # 예시: return f\"Response to {message}\"\n",
    "        response = response_chain.invoke({\"customer_message\": new_message, \"memory\":memory})\n",
    "        return response\n",
    "\n",
    "    def post_response(self, new_message, response):\n",
    "        # 여기에 생성된 응답을 웹상의 메시지 창에 입력하는 로직을 구현합니다.\n",
    "        # 예시: post_message_to_web(response)\n",
    "        print(f\"메시지: {new_message}\\nAI 응답: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 대화방에 대한 스레드 생성 및 시작.\n",
    "room_ids = ['room1', 'room2', 'room3']  # 실제 대화방 ID를 리스트 형태로 관리.\n",
    "threads = []\n",
    "\n",
    "for room_id in room_ids:\n",
    "    chat_bot = ChatBotHandler(room_id)\n",
    "    chat_bot.start()\n",
    "    threads.append(chat_bot)\n",
    "\n",
    "# 모든 스레드가 완료될 때까지 기다림.\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "밑에 코드가 위에보다 예외에 안전 timeout=30 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import threading\n",
    "from chat_bot_handler import ChatBotHandler  # ChatBotHandler 클래스가 정의된 모듈을 임포트.\n",
    "\n",
    "def main():\n",
    "    # 가정: 여기에 실제 대화방 ID들을 리스트로 가져오는 로직이 포함되어 있음.\n",
    "    room_ids = get_chat_room_ids()  # 실제 대화방 ID들을 가져오는 함수 가정.\n",
    "    threads = []\n",
    "\n",
    "    # 각 대화방 ID별로 ChatBotHandler 스레드 생성 및 시작.\n",
    "    for room_id in room_ids:\n",
    "        chat_bot = ChatBotHandler(room_id)\n",
    "        chat_bot.start()\n",
    "        threads.append(chat_bot)\n",
    "\n",
    "    # 메인 스레드에서는 생성된 모든 스레드들이 완료될 때까지 기다린다.\n",
    "    for thread in threads:\n",
    "        thread.join(timeout=30)  # 30초 후에 반환하도록 타임아웃 설정.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "감시 스레드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread, Event\n",
    "\n",
    "class ChatBotHandler(Thread):\n",
    "    # ...\n",
    "\n",
    "    def __init__(self, room_id):\n",
    "        # ...\n",
    "        self.error_event = Event()  # 오류 상태를 나타내는 이벤트\n",
    "\n",
    "    def run(self):\n",
    "        while self.keep_running:\n",
    "            try:\n",
    "                # ...\n",
    "            except Exception as e:\n",
    "                self.error_event.set()  # 오류가 발생하면 이벤트를 설정하여 오류 상태를 표시\n",
    "                break\n",
    "\n",
    "def monitor_chat_bots(chat_bots):\n",
    "    while True:\n",
    "        for bot in chat_bots:\n",
    "            if bot.error_event.is_set():\n",
    "                log_error(f\"Restarting bot in room {bot.room_id} due to error.\")\n",
    "                bot.error_event.clear()  # 오류 상태 초기화\n",
    "                bot.start()  # 스레드를 다시 시작\n",
    "        time.sleep(10)  # 10초 마다 감시\n",
    "\n",
    "# main.py에서 ChatBotHandler 인스턴스 생성 후 감시 스레드 시작\n",
    "chat_bots = [ChatBotHandler(room_id) for room_id in room_ids]\n",
    "for bot in chat_bots:\n",
    "    bot.start()\n",
    "\n",
    "# 감시 스레드 생성 및 시작\n",
    "monitor_thread = Thread(target=monitor_chat_bots, args=(chat_bots,))\n",
    "monitor_thread.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "order_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
