{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "print(load_dotenv(find_dotenv(), override=True))\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo-0301'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\"\n",
    "\n",
    "llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시도1 - Adding memory 예시 + Route between multiple Runnables 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adding memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful chatbot\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 주문 접수를 도와주는 챗봇이야\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "order_cancel_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 주문 취소 도와주는 챗봇이야\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(return_messages=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  history: RunnableLambda(...)\n",
       "           | RunnableLambda(...)\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['history', 'question'], input_types={'history': typing.List[typing.Union[langchain.schema.messages.AIMessage, langchain.schema.messages.HumanMessage, langchain.schema.messages.ChatMessage, langchain.schema.messages.SystemMessage, langchain.schema.messages.FunctionMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful chatbot')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])\n",
       "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_chain = RunnablePassthrough.assign(\n",
    "    history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    ") | prompt | model\n",
    "general_chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  history: RunnableLambda(...)\n",
       "           | RunnableLambda(...)\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['history', 'question'], input_types={'history': typing.List[typing.Union[langchain.schema.messages.AIMessage, langchain.schema.messages.HumanMessage, langchain.schema.messages.ChatMessage, langchain.schema.messages.SystemMessage, langchain.schema.messages.FunctionMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='너는 주문 접수를 도와주는 챗봇이야')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])\n",
       "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain1 = RunnablePassthrough.assign(\n",
    "    history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    ") | query_prompt | model\n",
    "chain1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  history: RunnableLambda(...)\n",
       "           | RunnableLambda(...)\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['history', 'question'], input_types={'history': typing.List[typing.Union[langchain.schema.messages.AIMessage, langchain.schema.messages.HumanMessage, langchain.schema.messages.ChatMessage, langchain.schema.messages.SystemMessage, langchain.schema.messages.FunctionMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='너는 주문 취소 도와주는 챗봇이야')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])\n",
       "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2 = RunnablePassthrough.assign(\n",
    "    history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    ") | order_cancel_prompt | model\n",
    "chain2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Bob! How can I assist you today?')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"input\": \"hi im bob\"}\n",
    "response = chain1.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Route between multiple Runnables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route 예시\n",
    "\n",
    "general_prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Answer the question as accurately as you can.\\n\\n{input}\"\n",
    ")\n",
    "prompt_branch = RunnableBranch(\n",
    "  (lambda x: x[\"topic\"] == \"math\", math_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"physics\", physics_prompt),\n",
    "  general_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = PromptTemplate.from_template(\"\"\"\n",
    "아래 고객의 메시지를 보고, 이 메시지가 \"일반\", \"주문 변경\", \"주문 취소\" 중 어디에 해당되는지 분류해줘.\n",
    "\"일반\"이란 범주는 일반 문의 또는 주문에 관한 내용을 의미해.\n",
    "\n",
    "제시된 범주(\"일반\", \"주문 변경\", \"주문 취소\") 가운데 하나로 분류해야돼.\n",
    "\n",
    "<고객 메시지>\n",
    "{question}\n",
    "</고객 메시지>\n",
    "\n",
    "Classification:\"\"\") | ChatOpenAI(temperature=0, model=llm_model) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "  (lambda x: \"일반\" in x[\"topic\"].lower(), chain1),\n",
    "  (lambda x: \"주문 취소\" in x[\"topic\"].lower(), chain2),\n",
    "  general_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = {\n",
    "    \"topic\": chain,\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "} | branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='저는 주문 접수를 도와주는 챗봇입니다. 주문을 받아서 처리하는 역할을 하고 있어요. 어떤 주문을 도와드릴까요?')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"너는 무슨 봇이야?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain.invoke({\"question\": \"안녕하세요 제 이름은 나들이고 주문 취소 좀 하는데요, 당신은 무슨 챗봇인가요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시도 2- Router예시(프름프트 routing) + LLMChain wihth memory 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Router예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "physics_prompt = PromptTemplate.from_template(physics_template)\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "math_prompt = PromptTemplate.from_template(math_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from langchain.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Answer the question as accurately as you can.\\n\\n{input}\"\n",
    ")\n",
    "prompt_branch = RunnableBranch(\n",
    "  (lambda x: x[\"topic\"] == \"math\", math_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"physics\", physics_prompt),\n",
    "  general_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicClassifier(BaseModel):\n",
    "    \"Classify the topic of the user question\"\n",
    "    \n",
    "    topic: Literal[\"math\", \"physics\", \"general\"]\n",
    "    \"The topic of the user question. One of 'math', 'phsyics' or 'general'.\"\n",
    "\n",
    "\n",
    "classifier_function = convert_pydantic_to_openai_function(TopicClassifier)\n",
    "llm = ChatOpenAI().bind(functions=[classifier_function], function_call={\"name\": \"TopicClassifier\"}) \n",
    "parser = PydanticAttrOutputFunctionsParser(pydantic_schema=TopicClassifier, attr_name=\"topic\")\n",
    "classifier_chain = llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'TopicClassifier',\n",
       " 'description': 'Classify the topic of the user question',\n",
       " 'parameters': {'title': 'TopicClassifier',\n",
       "  'description': 'Classify the topic of the user question',\n",
       "  'type': 'object',\n",
       "  'properties': {'topic': {'title': 'Topic',\n",
       "    'enum': ['math', 'physics', 'general'],\n",
       "    'type': 'string'}},\n",
       "  'required': ['topic']}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adding memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | classifier_chain) \\\n",
    "    | prompt_branch | ChatOpenAI()| StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunnablePassthrough.assign(topic=itemgetter(\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\231027.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/231027.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m final_chain\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/231027.ipynb#Y103sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     {\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mWhat is the first prime number greater than 40 such that one plus the prime number is divisible by 3?\u001b[39;49m\u001b[39m\"\u001b[39;49m}\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/231027.ipynb#Y103sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1121\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1122\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m             patch_config(\n\u001b[0;32m   1125\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1126\u001b[0m             ),\n\u001b[0;32m   1127\u001b[0m         )\n\u001b[0;32m   1128\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\passthrough.py:315\u001b[0m, in \u001b[0;36mRunnableAssign.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m    305\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    306\u001b[0m     \u001b[39minput\u001b[39m: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m    307\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    309\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m    310\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    311\u001b[0m         \u001b[39minput\u001b[39m, \u001b[39mdict\u001b[39m\n\u001b[0;32m    312\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m    314\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m,\n\u001b[1;32m--> 315\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapper\u001b[39m.\u001b[39;49minvoke(\u001b[39minput\u001b[39;49m, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs),\n\u001b[0;32m    316\u001b[0m     }\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1631\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[39mwith\u001b[39;00m get_executor_for_config(config) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m   1618\u001b[0m         futures \u001b[39m=\u001b[39m [\n\u001b[0;32m   1619\u001b[0m             executor\u001b[39m.\u001b[39msubmit(\n\u001b[0;32m   1620\u001b[0m                 step\u001b[39m.\u001b[39minvoke,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[39mfor\u001b[39;00m key, step \u001b[39min\u001b[39;00m steps\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1630\u001b[0m         ]\n\u001b[1;32m-> 1631\u001b[0m         output \u001b[39m=\u001b[39m {key: future\u001b[39m.\u001b[39mresult() \u001b[39mfor\u001b[39;00m key, future \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   1632\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1631\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[39mwith\u001b[39;00m get_executor_for_config(config) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m   1618\u001b[0m         futures \u001b[39m=\u001b[39m [\n\u001b[0;32m   1619\u001b[0m             executor\u001b[39m.\u001b[39msubmit(\n\u001b[0;32m   1620\u001b[0m                 step\u001b[39m.\u001b[39minvoke,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[39mfor\u001b[39;00m key, step \u001b[39min\u001b[39;00m steps\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1630\u001b[0m         ]\n\u001b[1;32m-> 1631\u001b[0m         output \u001b[39m=\u001b[39m {key: future\u001b[39m.\u001b[39;49mresult() \u001b[39mfor\u001b[39;00m key, future \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   1632\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py:444\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    443\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 444\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    445\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    388\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 389\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    390\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    392\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\thread.py:57\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m     59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1121\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1122\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m             patch_config(\n\u001b[0;32m   1125\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1126\u001b[0m             ),\n\u001b[0;32m   1127\u001b[0m         )\n\u001b[0;32m   1128\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1121\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1122\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m             patch_config(\n\u001b[0;32m   1125\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1126\u001b[0m             ),\n\u001b[0;32m   1127\u001b[0m         )\n\u001b[0;32m   1128\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:2386\u001b[0m, in \u001b[0;36mRunnableBinding.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m   2381\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   2382\u001b[0m     \u001b[39minput\u001b[39m: Input,\n\u001b[0;32m   2383\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2384\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2385\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Output:\n\u001b[1;32m-> 2386\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbound\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   2387\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   2388\u001b[0m         merge_configs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig, config),\n\u001b[0;32m   2389\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs},\n\u001b[0;32m   2390\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\chat_models\\base.py:150\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m    137\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    138\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    143\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessageChunk:\n\u001b[0;32m    144\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[0;32m    146\u001b[0m         BaseMessageChunk,\n\u001b[0;32m    147\u001b[0m         cast(\n\u001b[0;32m    148\u001b[0m             ChatGeneration,\n\u001b[0;32m    149\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_prompt(\n\u001b[1;32m--> 150\u001b[0m                 [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[0;32m    151\u001b[0m                 stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    152\u001b[0m                 callbacks\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    153\u001b[0m                 tags\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtags\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    154\u001b[0m                 metadata\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    155\u001b[0m                 run_name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    156\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    157\u001b[0m             )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[0;32m    158\u001b[0m         )\u001b[39m.\u001b[39mmessage,\n\u001b[0;32m    159\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\chat_models\\base.py:131\u001b[0m, in \u001b[0;36mBaseChatModel._convert_input\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[39mreturn\u001b[39;00m ChatPromptValue(messages\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m)\n\u001b[0;32m    130\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid input type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39minput\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "final_chain.invoke(\n",
    "    {\"input\": \"What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = RunnablePassthrough.assign(topic=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")) | classifier_chain \\\n",
    "    | prompt_branch | ChatOpenAI()| StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\231027.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/231027.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m final_chain\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/231027.ipynb#Y112sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     {\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mWhat is the first prime number greater than 40 such that one plus the prime number is divisible by 3?\u001b[39;49m\u001b[39m\"\u001b[39;49m}\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/231027.ipynb#Y112sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1121\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1122\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m             patch_config(\n\u001b[0;32m   1125\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1126\u001b[0m             ),\n\u001b[0;32m   1127\u001b[0m         )\n\u001b[0;32m   1128\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1121\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1122\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m             patch_config(\n\u001b[0;32m   1125\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1126\u001b[0m             ),\n\u001b[0;32m   1127\u001b[0m         )\n\u001b[0;32m   1128\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:2386\u001b[0m, in \u001b[0;36mRunnableBinding.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m   2381\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   2382\u001b[0m     \u001b[39minput\u001b[39m: Input,\n\u001b[0;32m   2383\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2384\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2385\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Output:\n\u001b[1;32m-> 2386\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbound\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   2387\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   2388\u001b[0m         merge_configs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig, config),\n\u001b[0;32m   2389\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs},\n\u001b[0;32m   2390\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\chat_models\\base.py:150\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m    137\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    138\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    143\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessageChunk:\n\u001b[0;32m    144\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[0;32m    146\u001b[0m         BaseMessageChunk,\n\u001b[0;32m    147\u001b[0m         cast(\n\u001b[0;32m    148\u001b[0m             ChatGeneration,\n\u001b[0;32m    149\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_prompt(\n\u001b[1;32m--> 150\u001b[0m                 [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[0;32m    151\u001b[0m                 stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    152\u001b[0m                 callbacks\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    153\u001b[0m                 tags\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtags\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    154\u001b[0m                 metadata\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    155\u001b[0m                 run_name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    156\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    157\u001b[0m             )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[0;32m    158\u001b[0m         )\u001b[39m.\u001b[39mmessage,\n\u001b[0;32m    159\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\chat_models\\base.py:131\u001b[0m, in \u001b[0;36mBaseChatModel._convert_input\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[39mreturn\u001b[39;00m ChatPromptValue(messages\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m)\n\u001b[0;32m    130\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid input type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39minput\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "final_chain.invoke(\n",
    "    {\"input\": \"What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시도3 - Route between multiple Runnables 예시 + 프롬프트에 memory 내용 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding memory\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template='Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`.\\n                                     \\nDo not respond with more than one word.\\n\\n<question>\\n{question}\\n</question>\\n\\nClassification:')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ts 추가\n",
    "\n",
    "PromptTemplate.from_template(\"\"\"Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`.\n",
    "                                     \n",
    "Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['history', 'input'], input_types={'history': typing.List[typing.Union[langchain.schema.messages.AIMessage, langchain.schema.messages.HumanMessage, langchain.schema.messages.ChatMessage, langchain.schema.messages.SystemMessage, langchain.schema.messages.FunctionMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful chatbot')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ts 추가\n",
    "# Router 예제 중\n",
    "\n",
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful chatbot\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['history', 'question'], template='Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`.\\n                                     \\nDo not respond with more than one word.\\n\\n<previous conversation>\\n{history}\\n</previous conversation>\\n\\n<question>\\n{question}\\n</question>\\n\\nClassification:')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ts 수정\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`.\n",
    "                                     \n",
    "Do not respond with more than one word.\n",
    "\n",
    "<previous conversation>\n",
    "{history}\n",
    "</previous conversation>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\")\n",
    "prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain =  RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))| prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"랭체인에 대해 알려줘?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['history', 'question'], template='\\n너는 친절한 챗봇이야.\\n이전 대화를 고려해서 유저와 대화해줘.\\n\\n<previous conversation>\\n{history}\\n</previous conversation>\\n\\n<question>\\n{question}\\n</question>\\n\\nClassification:')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# memory 추가 여부 확인하기 위해 프롬프트 수정\n",
    "\n",
    "# ts 수정\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "너는 친절한 챗봇이야.\n",
    "이전 대화를 고려해서 유저와 대화해줘.\n",
    "\n",
    "<previous conversation>\n",
    "{history}\n",
    "</previous conversation>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\")\n",
    "prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain =  RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))| prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'인사'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"내 이름은 나들이야\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain =  RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))| prompt | ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='인사')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parser 제거 시\n",
    "\n",
    "chain.invoke({\"question\": \"내 이름은 나들이야\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain =  RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))| prompt | ChatOpenAI(temperature=0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='greeting\\n\\n<Response>\\n안녕하세요, 나들이님! 저는 친절한 챗봇입니다. 무엇을 도와드릴까요?\\n</Response>')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chain.invoke({\"question\": \"내 이름은 나들이야\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Memory recall')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"좀 전에 내가 보낸 메시지를 기억해?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맨 마지막이 classification으로 돼 있었음!\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "너는 친절한 챗봇이야.\n",
    "이전 대화를 고려해서 유저와 대화해줘.\n",
    "\n",
    "<previous conversation>\n",
    "{history}\n",
    "</previous conversation>\n",
    "\n",
    "<message>\n",
    "{message}\n",
    "</message>\n",
    "\n",
    "answer:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain =  RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))| prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 나들이님, 반갑습니다. 저는 친절한 챗봇이라고 해요. 무엇을 도와드릴까요?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"message\": \"내 이름은 나들이야\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'저는 아직 당신의 이름을 알지 못합니다. 이전 대화에서 당신의 이름을 언급하지 않았던 것 같습니다. 제가 도움을 드릴 수 있는 다른 질문이 있으시다면 말씀해주세요.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"message\": \"좀 전에 내 이름이 뭐라고 했지?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain =  RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))| prompt | ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='안녕 나들이! 반가워요. 어떤 것에 대해 이야기하고 싶으세요?'\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"message\": \"안녕 나는 나들이라고 해\"}\n",
    "response = chain.invoke(inputs)\n",
    "print(response)\n",
    "memory.save_context(inputs, {\"output\": response.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_memory_variables() missing 1 required positional argument: 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\231027.ipynb Cell 65\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/231027.ipynb#Y156sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m memory\u001b[39m.\u001b[39;49mload_memory_variables()\n",
      "\u001b[1;31mTypeError\u001b[0m: load_memory_variables() missing 1 required positional argument: 'inputs'"
     ]
    }
   ],
   "source": [
    "memory.load_memory_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='안녕 나는 나들이라고 해'),\n",
       "  AIMessage(content='안녕 나들이! 반가워요. 어떤 것에 대해 이야기하고 싶으세요?')]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메모리 불러오는데 왜 inputs 필요하게 만든 거지?\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 메모리 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain =  RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))| prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 메시지와 모델의 출력 메시지를 메모리에 저장하는 함수 이름\n",
    "\n",
    "def save_conversation(dict):\n",
    "    print('customer_message: ', dict[\"customer_message\"])\n",
    "    print('ai_response: ', dict[\"ai_response\"])\n",
    "    memory.save_context({\"inputs\": dict[\"customer_message\"]}, {\"output\": dict[\"ai_response\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableLambda(save_conversation)로 끝나면 모델 출력이 반환 안되나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_memory = {\"customer_message\": RunnablePassthrough(), \"ai_response\": chain} |  RunnableLambda(save_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  {'message': '나는 나들이라고 해'}\n",
      "ai_response:  안녕 나들이! 다시 만나서 반가워요. 어떤 것에 대해 이야기하고 싶으세요?\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HumanMessage\ncontent\n  str type expected (type=type_error.str)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\231027.ipynb Cell 72\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/231027.ipynb#Y203sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m chain_with_memory\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mmessage\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39m나는 나들이라고 해\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:1121\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1121\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1122\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1124\u001b[0m             patch_config(\n\u001b[0;32m   1125\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1126\u001b[0m             ),\n\u001b[0;32m   1127\u001b[0m         )\n\u001b[0;32m   1128\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:2171\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m   2170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfunc\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 2171\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[0;32m   2172\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_invoke,\n\u001b[0;32m   2173\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   2174\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_config(config, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc),\n\u001b[0;32m   2175\u001b[0m     )\n\u001b[0;32m   2176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2177\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   2178\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2179\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse `ainvoke` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2180\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:640\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    634\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    635\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[0;32m    636\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[0;32m    637\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    638\u001b[0m )\n\u001b[0;32m    639\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 640\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m    641\u001b[0m         func, \u001b[39minput\u001b[39;49m, run_manager, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    642\u001b[0m     )\n\u001b[0;32m    643\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    644\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\config.py:201\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    200\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[1;32m--> 201\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\base.py:2104\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[1;34m(self, input, run_manager, config)\u001b[0m\n\u001b[0;32m   2098\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_invoke\u001b[39m(\n\u001b[0;32m   2099\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   2100\u001b[0m     \u001b[39minput\u001b[39m: Input,\n\u001b[0;32m   2101\u001b[0m     run_manager: CallbackManagerForChainRun,\n\u001b[0;32m   2102\u001b[0m     config: RunnableConfig,\n\u001b[0;32m   2103\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Output:\n\u001b[1;32m-> 2104\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, \u001b[39minput\u001b[39;49m, run_manager, config)\n\u001b[0;32m   2105\u001b[0m     \u001b[39m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[0;32m   2106\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\runnable\\config.py:201\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    200\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[1;32m--> 201\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\231027.ipynb Cell 72\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/231027.ipynb#Y203sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mcustomer_message: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mdict\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mcustomer_message\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/231027.ipynb#Y203sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mai_response: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mdict\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mai_response\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/231027.ipynb#Y203sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m memory\u001b[39m.\u001b[39;49msave_context({\u001b[39m\"\u001b[39;49m\u001b[39minputs\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mdict\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcustomer_message\u001b[39;49m\u001b[39m\"\u001b[39;49m]}, {\u001b[39m\"\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mdict\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mai_response\u001b[39;49m\u001b[39m\"\u001b[39;49m]})\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\memory\\chat_memory.py:36\u001b[0m, in \u001b[0;36mBaseChatMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m input_str, output_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_input_output(inputs, outputs)\n\u001b[1;32m---> 36\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_memory\u001b[39m.\u001b[39;49madd_user_message(input_str)\n\u001b[0;32m     37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_memory\u001b[39m.\u001b[39madd_ai_message(output_str)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\schema\\chat_history.py:46\u001b[0m, in \u001b[0;36mBaseChatMessageHistory.add_user_message\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_user_message\u001b[39m(\u001b[39mself\u001b[39m, message: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Convenience method for adding a human message string to the store.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \n\u001b[0;32m     43\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m        message: The string contents of a human message.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_message(HumanMessage(content\u001b[39m=\u001b[39;49mmessage))\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\langchain\\load\\serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for HumanMessage\ncontent\n  str type expected (type=type_error.str)"
     ]
    }
   ],
   "source": [
    "# RunnablePassthrough()는 prompt가 아니라 입력 자체를 그대로 전달하는 듯?\n",
    "\n",
    "chain_with_memory.invoke({\"message\": \"나는 나들이라고 해\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_memory = {\"customer_message\": itemgetter(\"message\"), \"ai_response\": chain} |  RunnableLambda(save_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  나는 나들이라고 해\n",
      "ai_response:  안녕 나들이! 반가워요. 이전에 이야기한 내용을 다시 한 번 상기시켜주세요. 어떤 것에 대해 대화를 나누고 싶으세요?\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"나는 나들이라고 해\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  내 이름이 뭐라고?\n",
      "ai_response:  너의 이름은 '나들이'야.\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"내 이름이 뭐라고?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 print문으로 찍는 것도 간편한 방법이네(일단 이렇게 하자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conversation(dict):\n",
    "    # print('customer_message: ', dict[\"customer_message\"])\n",
    "    # print('ai_response: ', dict[\"ai_response\"])\n",
    "    memory.save_context({\"inputs\": dict[\"customer_message\"]}, {\"output\": dict[\"ai_response\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_memory = {\"customer_message\": itemgetter(\"message\"), \"ai_response\": chain} |  RunnableLambda(save_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_memory.invoke({\"message\": \"내 이름은 '나들'이야\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 방법으로 메모리 주입하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_chain = PromptTemplate.from_template(\"\"\"\n",
    "아래 고객의 메시지를 보고, 이 메시지가 \"일반\", \"주문 변경\", \"주문 취소\" 중 어디에 해당되는지 분류해줘.\n",
    "\"일반\"이란 범주는 일반 문의 또는 주문에 관한 내용을 의미해.\n",
    "\n",
    "제시된 범주(\"일반\", \"주문 변경\", \"주문 취소\") 가운데 하나로 분류해야돼.\n",
    "\n",
    "<고객 메시지>\n",
    "{message}\n",
    "</고객 메시지>\n",
    "\n",
    "Classification:\"\"\") | ChatOpenAI(temperature=0, model=llm_model) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain =  RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))| prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_memory = {\"customer_message\": itemgetter(\"message\"), \"ai_response\": chain} |  RunnableLambda(save_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_query_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "order_change_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 변경을 전담하는 종업원이야.\n",
    "고객이 변경한 주문 내용을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 변경 내용이 잘못됐다면, 주문 변경 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 변경 내용을 정확히 파악할 때까지 반복해야돼.\n",
    "고객의 주문 변경을 정확히 파악했다면, 고객에게 고객이 주문을 변경한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 변경된 주문의 총 가격을 알려줘.\n",
    "\n",
    "\n",
    "\n",
    "고객의 주문 변경은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "order_cancel_chain = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 취소를 전담하는 종업원이야.\n",
    "고객이 취소하려는 주문을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 취소 내용이 잘못됐다면, 주문 취소 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 취소 내용을 정확히 파악할 때\n",
    "고객의 주문 취소 내용을 정확히 파악했다면, 고객에게 고객이 주문을 취소한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 취소된 주문의 총 가격을 알려줘.\n",
    "\n",
    "\n",
    "고객이 취소하려는 주문은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\") | ChatOpenAI(temperature=0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 version\n",
    "\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "  (lambda x: \"주문 변경\" in x[\"topic\"], order_change_chain),\n",
    "  (lambda x: \"주문 취소\" in x[\"topic\"], order_cancel_chain),\n",
    "  general_query_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 version\n",
    "\n",
    "full_chain = {\n",
    "    \"topic\": chain,\n",
    "    \"message\": lambda x: x[\"message\"]\n",
    "} | branch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = {\n",
    "    \"topic\": chain,\n",
    "    \"message\": lambda x: x[\"message\"]\n",
    "} | {\"customer_message\": itemgetter(\"message\"), \n",
    "     \"ai_response\": chain\n",
    "     } | RunnableLambda(save_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_memory = {\"customer_message\": itemgetter(\"message\"), \"ai_response\": chain} |  RunnableLambda(save_conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### router 이용하면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_change_prompt = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 변경을 전담하는 종업원이야.\n",
    "고객이 변경한 주문 내용을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 변경 내용이 잘못됐다면, 주문 변경 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 변경 내용을 정확히 파악할 때까지 반복해야돼.\n",
    "고객의 주문 변경을 정확히 파악했다면, 고객에게 고객이 주문을 변경한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 변경된 주문의 총 가격을 알려줘.\n",
    "이전 대화 내용을 고려해서 답변해야 해.\n",
    "\n",
    "이전 대화 내용은 다음과 같아:\n",
    "{history}\n",
    "\n",
    "\n",
    "고객의 주문 변경은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\")\n",
    "\n",
    "order_cancel_prompt = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 취소를 전담하는 종업원이야.\n",
    "고객이 취소하려는 주문을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 취소 내용이 잘못됐다면, 주문 취소 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 취소 내용을 정확히 파악할 때\n",
    "고객의 주문 취소 내용을 정확히 파악했다면, 고객에게 고객이 주문을 취소한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 취소된 주문의 총 가격을 알려줘.\n",
    "이전 대화 내용을 고려해서 답변해야 해.\n",
    "\n",
    "이전 대화 내용은 다음과 같아:\n",
    "{history}\n",
    "\n",
    "고객이 취소하려는 주문은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_prompt = PromptTemplate.from_template(\n",
    "    \"당신은 뛰어난 주문 로봇이야. 질문에 가능한 한 정확하게 답해줘.\\n\\n{message}\"\n",
    ")\n",
    "prompt_branch = RunnableBranch(\n",
    "  (lambda x: x[\"topic\"] == \"주문 변경\", order_change_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"주문 취소\", order_cancel_prompt),\n",
    "  general_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_prompt = PromptTemplate.from_template(\"\"\"\n",
    "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "   \n",
    "이전 대화 내용을 고려해서 답변해야 해.\n",
    "이전 대화 내용은 다음과 같아:\n",
    "{history}\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\")\n",
    "\n",
    "prompt_branch = RunnableBranch(\n",
    "  (lambda x: x[\"topic\"] == \"주문 변경\", order_change_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"주문 취소\", order_cancel_prompt),\n",
    "  general_prompt\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from langchain.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "\n",
    "\n",
    "class TopicClassifier(BaseModel):\n",
    "    \"사용자 문의의 주제를 분류해줘.\"\n",
    "    \n",
    "    topic: Literal[\"일반 문의\", \"주문 변경\", \"주문 취소\"]\n",
    "    \"사용자 문의의 주제는 '일반 문의', '주문 변경', '주문 취소' 중 하나야.\"\n",
    "\n",
    "\n",
    "classifier_function = convert_pydantic_to_openai_function(TopicClassifier)\n",
    "llm = ChatOpenAI().bind(functions=[classifier_function], function_call={\"name\": \"TopicClassifier\"}) \n",
    "parser = PydanticAttrOutputFunctionsParser(pydantic_schema=TopicClassifier, attr_name=\"topic\")\n",
    "classifier_chain = llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = (\n",
    "    RunnablePassthrough.assign(topic=itemgetter(\"message\") | classifier_chain) \n",
    "    | prompt_branch \n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 주문을 도와드릴게요. 어떤 상품을 주문하시려고 하시나요? 제가 판매하는 상품 목록을 알려드릴게요.\\n\\n1. 상품: 떡케익5호\\n   기본 판매 수량: 1개\\n   기본 판매 수량의 가격: 54,000원\\n\\n2. 상품: 무지개 백설기 케익\\n   기본 판매 수량: 1개\\n   기본 판매 수량의 가격: 51,500원\\n\\n3. 상품: 미니 백설기\\n   기본 판매 수량: 35개\\n   기본 판매 수량의 가격: 31,500원\\n\\n4. 상품: 개별 모듬팩\\n   기본 판매 수량: 1개\\n   기본 판매 수량의 가격: 13,500원\\n\\n원하시는 상품을 알려주시면, 주문을 도와드릴게요!'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain.invoke({\"message\": \"주문 좀 하려고요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 주문을 도와드릴게요. 어떤 상품을 주문하시고 싶으신가요? 저희 가게에서는 떡케익5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있어요. 어떤 상품을 선택하시겠어요?'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = (\n",
    "    RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))|\n",
    "    RunnablePassthrough.assign(topic=itemgetter(\"message\") | classifier_chain) \n",
    "    | prompt_branch \n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_chain.invoke({\"message\": \"주문 좀 하려고요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'고객님이 주문을 변경하셨네요. 변경된 주문은 \"무지개 백설기\" 1개가 되었습니다. 변경된 주문의 가격은 총 10,000원입니다. 변경 내용이 맞는지 한 번 더 확인해주세요.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain.invoke({\"message\": \"무지개 백설기 1개 할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conversation(dict):\n",
    "    print('customer_message: ', dict[\"customer_message\"])\n",
    "    print('ai_response: ', dict[\"ai_response\"])\n",
    "    memory.save_context({\"inputs\": dict[\"customer_message\"]}, {\"output\": dict[\"ai_response\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  위에서 사용한 prompt\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "너는 친절한 챗봇이야.\n",
    "이전 대화를 고려해서 유저와 대화해줘.\n",
    "\n",
    "<previous conversation>\n",
    "{history}\n",
    "</previous conversation>\n",
    "\n",
    "<message>\n",
    "{message}\n",
    "</message>\n",
    "\n",
    "answer:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 의도대로 동작했던 구조\n",
    "\n",
    "chain =  RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))| prompt | ChatOpenAI() | StrOutputParser()\n",
    "chain_with_memory = {\"customer_message\": itemgetter(\"message\"), \"ai_response\": chain} |  RunnableLambda(save_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내 작업에 맞게 수정\n",
    "\n",
    "RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))| prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dictionary가 어떻게 프롬프트에 주입되는 거지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': '내 이름은 나들이야',\n",
       " 'history': [HumanMessage(content='주문 좀 하려고요'),\n",
       "  AIMessage(content='물론이죠! 저는 주문에 관련된 질문에 가능한 한 정확하게 답해드릴 수 있습니다. 어떤 주문을 도와드릴까요?'),\n",
       "  HumanMessage(content='주문 좀 하려고요'),\n",
       "  AIMessage(content='물론입니다! 어떤 주문을 도와드릴까요?'),\n",
       "  HumanMessage(content='주문 좀 하려고요'),\n",
       "  AIMessage(content='네, 무엇을 주문하시려고 하시나요? 어떤 종류의 상품이나 서비스를 원하시는지 알려주세요.'),\n",
       "  HumanMessage(content='판매 상품 좀 알려주세요'),\n",
       "  AIMessage(content='안녕하세요! 저희 가게에서 판매하는 상품에 대해 알려드릴게요.\\n\\n1. 떡케익5호: 기본 판매 수량은 1개이며, 가격은 54,000원입니다.\\n2. 무지개 백설기 케익: 기본 판매 수량은 1개이며, 가격은 51,500원입니다.\\n3. 미니 백설기: 기본 판매 수량은 35개이며, 가격은 31,500원입니다.\\n4. 개별 모듬팩: 기본 판매 수량은 1개이며, 가격은 13,500원입니다.\\n\\n더 궁금하신 점이 있으신가요? 어떤 상품에 관심이 있으신지 알려주시면 더 자세한 정보를 알려드릴 수 있어요.'),\n",
       "  HumanMessage(content='떡케익2개랑 미니 백설기 3개 할게요'),\n",
       "  AIMessage(content='고객님이 주문을 변경하셨군요. 변경된 주문 내용을 확인해보겠습니다. 떡케익 2개와 미니 백설기 3개로 주문이 변경되었나요?'),\n",
       "  HumanMessage(content='아니요 처음 주문하는 건데요;'),\n",
       "  AIMessage(content='처음 주문하시는 거군요. 어떤 상품을 원하시는지 알려주시면 제가 도움을 드릴게요. 저희 가게에서는 떡케익 5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있어요. 어떤 상품을 찾으시나요?'),\n",
       "  HumanMessage(content='떡케익2개랑 미니 백설기 3개 주문한다니깐요'),\n",
       "  AIMessage(content='고객님의 주문 변경 내용을 확인해보겠습니다. 변경 내용은 떡케익 2개와 미니 백설기 3개 주문이 맞나요?'),\n",
       "  HumanMessage(content='네'),\n",
       "  AIMessage(content='안녕하세요! 고객님의 문의에 답변해드리겠습니다.\\n\\n고객님이 문의하신 상품은 어떤 것인가요? 저희 가게에서는 떡케익 5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있어요. \\n\\n떡케익 5호는 기본 판매 수량이 1개이며, 가격은 54,000원입니다. 이 제품은 크기가 크고 다양한 맛을 즐길 수 있어 많은 분들이 좋아하십니다.\\n\\n무지개 백설기 케익은 기본 판매 수량이 1개이며, 가격은 51,500원입니다. 이 케익은 백설기의 다양한 맛과 무지개색의 아름다운 모양으로 유명하며, 파티나 기념일에 많이 이용되고 있습니다.\\n\\n미니 백설기는 기본 판매 수량이 35개이며, 가격은 31,500원입니다. 작은 크기의 백설기로, 소규모 모임이나 선물용으로 인기가 많아요.\\n\\n또한 개별 모듬팩은 기본 판매 수량이 1개이며, 가격은 13,500원입니다. 다양한 종류의 작은 떡들을 한 번에 즐길 수 있어서 소량으로 구매하고 싶은 분들에게 추천드려요.\\n\\n고객님께서 어떤 상품을 원하시는지 알려주시면, 주문 절차를 안내해드리도록 하겠습니다.')]}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))).invoke({\"message\": \"내 이름은 나들이야\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\n너는 친절한 챗봇이야.\\n이전 대화를 고려해서 유저와 대화해줘.\\n\\n<previous conversation>\\n[HumanMessage(content='주문 좀 하려고요'), AIMessage(content='물론이죠! 저는 주문에 관련된 질문에 가능한 한 정확하게 답해드릴 수 있습니다. 어떤 주문을 도와드릴까요?'), HumanMessage(content='주문 좀 하려고요'), AIMessage(content='물론입니다! 어떤 주문을 도와드릴까요?'), HumanMessage(content='주문 좀 하려고요'), AIMessage(content='네, 무엇을 주문하시려고 하시나요? 어떤 종류의 상품이나 서비스를 원하시는지 알려주세요.'), HumanMessage(content='판매 상품 좀 알려주세요'), AIMessage(content='안녕하세요! 저희 가게에서 판매하는 상품에 대해 알려드릴게요.\\\\n\\\\n1. 떡케익5호: 기본 판매 수량은 1개이며, 가격은 54,000원입니다.\\\\n2. 무지개 백설기 케익: 기본 판매 수량은 1개이며, 가격은 51,500원입니다.\\\\n3. 미니 백설기: 기본 판매 수량은 35개이며, 가격은 31,500원입니다.\\\\n4. 개별 모듬팩: 기본 판매 수량은 1개이며, 가격은 13,500원입니다.\\\\n\\\\n더 궁금하신 점이 있으신가요? 어떤 상품에 관심이 있으신지 알려주시면 더 자세한 정보를 알려드릴 수 있어요.'), HumanMessage(content='떡케익2개랑 미니 백설기 3개 할게요'), AIMessage(content='고객님이 주문을 변경하셨군요. 변경된 주문 내용을 확인해보겠습니다. 떡케익 2개와 미니 백설기 3개로 주문이 변경되었나요?'), HumanMessage(content='아니요 처음 주문하는 건데요;'), AIMessage(content='처음 주문하시는 거군요. 어떤 상품을 원하시는지 알려주시면 제가 도움을 드릴게요. 저희 가게에서는 떡케익 5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있어요. 어떤 상품을 찾으시나요?'), HumanMessage(content='떡케익2개랑 미니 백설기 3개 주문한다니깐요'), AIMessage(content='고객님의 주문 변경 내용을 확인해보겠습니다. 변경 내용은 떡케익 2개와 미니 백설기 3개 주문이 맞나요?'), HumanMessage(content='네'), AIMessage(content='안녕하세요! 고객님의 문의에 답변해드리겠습니다.\\\\n\\\\n고객님이 문의하신 상품은 어떤 것인가요? 저희 가게에서는 떡케익 5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있어요. \\\\n\\\\n떡케익 5호는 기본 판매 수량이 1개이며, 가격은 54,000원입니다. 이 제품은 크기가 크고 다양한 맛을 즐길 수 있어 많은 분들이 좋아하십니다.\\\\n\\\\n무지개 백설기 케익은 기본 판매 수량이 1개이며, 가격은 51,500원입니다. 이 케익은 백설기의 다양한 맛과 무지개색의 아름다운 모양으로 유명하며, 파티나 기념일에 많이 이용되고 있습니다.\\\\n\\\\n미니 백설기는 기본 판매 수량이 35개이며, 가격은 31,500원입니다. 작은 크기의 백설기로, 소규모 모임이나 선물용으로 인기가 많아요.\\\\n\\\\n또한 개별 모듬팩은 기본 판매 수량이 1개이며, 가격은 13,500원입니다. 다양한 종류의 작은 떡들을 한 번에 즐길 수 있어서 소량으로 구매하고 싶은 분들에게 추천드려요.\\\\n\\\\n고객님께서 어떤 상품을 원하시는지 알려주시면, 주문 절차를 안내해드리도록 하겠습니다.')]\\n</previous conversation>\\n\\n<message>\\n내 이름은 나들이야\\n</message>\\n\\nanswer:\")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))| prompt).invoke({\"message\": \"내 이름은 나들이야\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_memory = {\"customer_message\": itemgetter(\"message\"), \"ai_response\": final_chain} |  RunnableLambda(save_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  판매 상품 좀 알려주세요\n",
      "ai_response:  안녕하세요! 저희 가게에서 판매하는 상품에 대해 알려드릴게요.\n",
      "\n",
      "1. 떡케익5호: 기본 판매 수량은 1개이며, 가격은 54,000원입니다.\n",
      "2. 무지개 백설기 케익: 기본 판매 수량은 1개이며, 가격은 51,500원입니다.\n",
      "3. 미니 백설기: 기본 판매 수량은 35개이며, 가격은 31,500원입니다.\n",
      "4. 개별 모듬팩: 기본 판매 수량은 1개이며, 가격은 13,500원입니다.\n",
      "\n",
      "더 궁금하신 점이 있으신가요? 어떤 상품에 관심이 있으신지 알려주시면 더 자세한 정보를 알려드릴 수 있어요.\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"판매 상품 좀 알려주세요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  떡케익2개랑 미니 백설기 3개 할게요\n",
      "ai_response:  고객님이 주문을 변경하셨군요. 변경된 주문 내용을 확인해보겠습니다. 떡케익 2개와 미니 백설기 3개로 주문이 변경되었나요?\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"떡케익2개랑 미니 백설기 3개 할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  아니요 처음 주문하는 건데요;\n",
      "ai_response:  처음 주문하시는 거군요. 어떤 상품을 원하시는지 알려주시면 제가 도움을 드릴게요. 저희 가게에서는 떡케익 5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있어요. 어떤 상품을 찾으시나요?\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"아니요 처음 주문하는 건데요;\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  떡케익2개랑 미니 백설기 3개 주문한다니깐요\n",
      "ai_response:  고객님의 주문 변경 내용을 확인해보겠습니다. 변경 내용은 떡케익 2개와 미니 백설기 3개 주문이 맞나요?\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"떡케익2개랑 미니 백설기 3개 주문한다니깐요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  네\n",
      "ai_response:  안녕하세요! 고객님의 문의에 답변해드리겠습니다.\n",
      "\n",
      "고객님이 문의하신 상품은 어떤 것인가요? 저희 가게에서는 떡케익 5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있어요. \n",
      "\n",
      "떡케익 5호는 기본 판매 수량이 1개이며, 가격은 54,000원입니다. 이 제품은 크기가 크고 다양한 맛을 즐길 수 있어 많은 분들이 좋아하십니다.\n",
      "\n",
      "무지개 백설기 케익은 기본 판매 수량이 1개이며, 가격은 51,500원입니다. 이 케익은 백설기의 다양한 맛과 무지개색의 아름다운 모양으로 유명하며, 파티나 기념일에 많이 이용되고 있습니다.\n",
      "\n",
      "미니 백설기는 기본 판매 수량이 35개이며, 가격은 31,500원입니다. 작은 크기의 백설기로, 소규모 모임이나 선물용으로 인기가 많아요.\n",
      "\n",
      "또한 개별 모듬팩은 기본 판매 수량이 1개이며, 가격은 13,500원입니다. 다양한 종류의 작은 떡들을 한 번에 즐길 수 있어서 소량으로 구매하고 싶은 분들에게 추천드려요.\n",
      "\n",
      "고객님께서 어떤 상품을 원하시는지 알려주시면, 주문 절차를 안내해드리도록 하겠습니다.\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"네\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 방법 이용 2차 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_prompt = PromptTemplate.from_template(\"\"\"\n",
    "너는 고객 문의를 매우 많이 해본 숙력된 종업원이야.\n",
    "가게에서 판매하는 상품 정보를 바탕으로 고객 문의에 친절하고 자세하게 답변해줘.\n",
    "자연스럽게 주문으로 이어지도록 대화를 이어가되, 지나치게 주문을 유도하지는 말아줘.\n",
    "\n",
    "가게에서 판매하는 상품 목록.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "   \n",
    "이전 대화 내용을 고려해서 답변해야 해.\n",
    "이전 대화 내용은 다음과 같아:\n",
    "{history}\n",
    "\n",
    "고객이 문의는 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\")\n",
    "\n",
    "prompt_branch = RunnableBranch(\n",
    "  (lambda x: x[\"topic\"] == \"주문 변경\", order_change_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"주문 취소\", order_cancel_prompt),\n",
    "  general_prompt\n",
    ")\n",
    "\n",
    "order_change_prompt = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 변경을 전담하는 종업원이야.\n",
    "고객이 변경한 주문 내용을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 변경 내용이 잘못됐다면, 주문 변경 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 변경 내용을 정확히 파악할 때까지 반복해야돼.\n",
    "고객의 주문 변경을 정확히 파악했다면, 고객에게 고객이 주문을 변경한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 변경된 주문의 총 가격을 알려줘.\n",
    "이전 대화 내용을 고려해서 답변해야 해.\n",
    "\n",
    "이전 대화 내용은 다음과 같아:\n",
    "{history}\n",
    "\n",
    "\n",
    "고객의 주문 변경은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\")\n",
    "\n",
    "order_cancel_prompt = PromptTemplate.from_template(\"\"\"\n",
    "너는 주문 취소를 전담하는 종업원이야.\n",
    "고객이 취소하려는 주문을 정확하게 파악하고, 너가 파악한 내용이 맞는지 고객에게 한 번 더 확인해줘.\n",
    "너가 파악한 주문 취소 내용이 잘못됐다면, 주문 취소 내용을 정확히 파악하고 그 내용이 맞는지 고객에게 확인하는 작업을 주문 취소 내용을 정확히 파악할 때\n",
    "고객의 주문 취소 내용을 정확히 파악했다면, 고객에게 고객이 주문을 취소한 상품의 이름, 수량, 가격을 각각 알려주고, 마지막에는 취소된 주문의 총 가격을 알려줘.\n",
    "이전 대화 내용을 고려해서 답변해야 해.\n",
    "\n",
    "이전 대화 내용은 다음과 같아:\n",
    "{history}\n",
    "\n",
    "고객이 취소하려는 주문은 다음과 같아:\n",
    "{message}\n",
    "답변:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_branch = RunnableBranch(\n",
    "  (lambda x: x[\"topic\"] == \"주문 변경\", order_change_prompt),\n",
    "  (lambda x: x[\"topic\"] == \"주문 취소\", order_cancel_prompt),\n",
    "  general_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 오류 발생\n",
    "# ValidationError: 1 validation error for TopicClassifier\n",
    "# topic\n",
    "#   unexpected value; permitted: '일반 문의', '주문 변경', '주문 취소' (type=value_error.const; given=주문; permitted=('일반 문의', '주문 변경', '주문 취소'))\n",
    "\n",
    "class TopicClassifier(BaseModel):\n",
    "    \"사용자 문의의 주제를 분류해줘.\"\n",
    "    \n",
    "    topic: Literal[\"일반 문의\", \"주문 변경\", \"주문 취소\"]\n",
    "    \"사용자 문의의 주제는 '일반 문의', '주문 변경', '주문 취소' 중 하나야.\"\n",
    "\n",
    "\n",
    "classifier_function = convert_pydantic_to_openai_function(TopicClassifier)\n",
    "llm = ChatOpenAI().bind(functions=[classifier_function], function_call={\"name\": \"TopicClassifier\"}) \n",
    "parser = PydanticAttrOutputFunctionsParser(pydantic_schema=TopicClassifier, attr_name=\"topic\")\n",
    "classifier_chain = llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정\n",
    "\n",
    "class TopicClassifier(BaseModel):\n",
    "    \"사용자 문의의 주제를 분류해줘.\"\n",
    "    \n",
    "    topic: Literal[\"일반\", \"주문 변경\", \"주문 취소\"]\n",
    "    \"사용자 문의의 주제는 '일반', '주문 변경', '주문 취소' 중 하나야.\"\n",
    "\n",
    "\n",
    "classifier_function = convert_pydantic_to_openai_function(TopicClassifier)\n",
    "llm = ChatOpenAI().bind(functions=[classifier_function], function_call={\"name\": \"TopicClassifier\"}) \n",
    "parser = PydanticAttrOutputFunctionsParser(pydantic_schema=TopicClassifier, attr_name=\"topic\")\n",
    "classifier_chain = llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메모리 저장하는 과정이 추가돼야"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = (\n",
    "    RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))|\n",
    "    RunnablePassthrough.assign(topic=itemgetter(\"message\") | classifier_chain) \n",
    "    | prompt_branch \n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# final_chain.invoke({\"message\": \"주문 좀 하려고요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conversation(dict):\n",
    "    print('customer_message: ', dict[\"customer_message\"])\n",
    "    print('ai_response: ', dict[\"ai_response\"])\n",
    "    memory.save_context({\"inputs\": dict[\"customer_message\"]}, {\"output\": dict[\"ai_response\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_memory = {\"customer_message\": itemgetter(\"message\"), \"ai_response\": final_chain} |  RunnableLambda(save_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  주문 좀 하려고요\n",
      "ai_response:  안녕하세요! 주문을 도와드릴게요. 어떤 상품을 주문하시려고 하시나요? 저희 가게에서는 떡케익5호, 무지개 백설기 케익, 미니 백설기, 그리고 개별 모듬팩을 판매하고 있어요. 어떤 상품을 원하시나요?\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"주문 좀 하려고요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  상품 가격 좀 알려주세요\n",
      "ai_response:  저희 가게에서 판매하는 상품의 가격을 알려드릴게요.\n",
      "\n",
      "1. 떡케익5호의 가격은 54,000원입니다.\n",
      "2. 무지개 백설기 케익의 가격은 51,500원입니다.\n",
      "3. 미니 백설기의 가격은 31,500원입니다.\n",
      "4. 개별 모듬팩의 가격은 13,500원입니다.\n",
      "\n",
      "원하시는 상품의 가격을 알려드렸는데, 어떤 상품을 주문하실 건가요?\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"상품 가격 좀 알려주세요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  떡케익 1개랑 미니 백설기 3개 주문할게요\n",
      "ai_response:  고객님께서는 떡케익 1개와 미니 백설기 3개를 주문하시려고 하신다고 말씀하셨네요. 이 내용을 다시 한 번 확인해 볼게요. 맞나요?\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"떡케익 1개랑 미니 백설기 3개 주문할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  아 그냥 거기서 미니 백설기는 하나 뺄게요. 그럼 얼마죠?\n",
      "ai_response:  고객님께서는 떡케익 1개와 미니 백설기 2개를 주문하시려고 변경하셨네요. 변경된 주문 내용을 다시 한 번 확인해 볼게요. 맞나요?\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"아 그냥 거기서 미니 백설기는 하나 뺄게요. 그럼 얼마죠?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  네 맞아요. 그대로 주문해주세요\n",
      "ai_response:  고객의 주문 변경이 정확히 파악되었습니다. 고객께서는 떡케익 1개와 미니 백설기 2개를 주문하시려고 변경하셨습니다. 변경된 주문 내용을 확인해주신 후, 변경된 주문의 총 가격은 118,500원입니다. 원하시는 상품을 준비해드리도록 하겠습니다.\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"네 맞아요. 그대로 주문해주세요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  주문 좀 변경할게요\n",
      "ai_response:  고객님, 주문 변경을 도와드릴게요. 어떤 부분을 변경하시려고 하시나요?\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"주문 좀 변경할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  미니 백설기 1개 빼고 개별 모듬팩 1개 할게요\n",
      "ai_response:  고객님, 주문 변경 내용을 확인했습니다. 미니 백설기 1개를 제외하고 개별 모듬팩 1개를 주문하시려고 변경하셨네요. 변경된 주문 내용을 다시 한 번 확인해주신 후, 변경된 주문의 총 가격은 25,500원입니다. 원하시는 상품을 준비해드리도록 하겠습니다.\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"미니 백설기 1개 빼고 개별 모듬팩 1개 할게요\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  왜 변경된 가격이 저런 거죠?\n",
      "ai_response:  주문을 변경할 때, 주문한 상품의 수량이나 종류가 달라지면 가격도 달라질 수 있어요. 이전에 주문하신 내용에서는 떡케익 1개와 미니 백설기 2개로 주문하셨는데, 변경된 주문 내용에서는 미니 백설기 1개를 제외하고 개별 모듬팩 1개로 주문하셨어요. 그래서 가격이 달라진 거예요. 변경된 주문의 총 가격은 25,500원이에요. 원하시는 상품을 준비해드릴게요. 주문을 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"왜 변경된 가격이 저런 거죠?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_message:  현재 제 주문 내역이 어떻게 되는 건데요?\n",
      "ai_response:  고객님의 현재 주문 내역은 떡케익 1개와 미니 백설기 2개로 변경되었습니다. 이에 따라 변경된 주문의 총 가격은 25,500원입니다. 원하시는 상품을 준비해드리도록 하겠습니다. 주문을 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "chain_with_memory.invoke({\"message\": \"현재 제 주문 내역이 어떻게 되는 건데요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_memory.invoke({\"message\": \"기존 주문에서 미니 백설기 1개 빼고 개별 모듬팩 1개 추가하겠다는 게 제대로 반영되지 않았는데요\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "order_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
