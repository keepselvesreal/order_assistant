{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "print(load_dotenv(find_dotenv(), override=True))\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo-0301'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\"\n",
    "\n",
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단순 문의와 주문 관련 경우 구분 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가 필요 내용\n",
    "- 고객 아이디, 메시지 도착 시간 등도 반환 결과에 담길 수 있게 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "system_message = {'role': 'system', \n",
    " 'content':\n",
    "     \"\"\"\n",
    "     너는 온라인 주문을 받는 경험 많은 종업원이야.\n",
    "     최초 대화에는 반드시 아래와 같은 인사가 포함돼야 해.\n",
    "     인사 예시 -> 안녕하세요~ :) 무엇을 도와드릴까요?\n",
    "     가게에서 판매하는 상품과 상품 관련 정보로 고객의 질문에 친절히 답변해야 해.\n",
    "     \n",
    "     우리 가게에서 취급하는 상품과 가격은 아래와 같아.\n",
    "     떡케일5호(기본 판매 수량: 1개)-54,000원\n",
    "     무지개 백설기 케익(기본 판매 수량: 1개)-51,500원\n",
    "     미니 백설기(기본 판매 수량: 35개)-31,500원\n",
    "     개별 모듬팩(기본  판매 수량: 1개)-13,500원\n",
    "     \n",
    "     다음과 같은 경우에는 \"죄송합니다. 해당 내용은 제가 답변하기 어렵네요 ㅜㅜ\"라고 응답해줘.\n",
    "     경우1- 고객이 주문과 관련 없는 질문을 하는 경우\n",
    "     경우2- 고객이 가게에서 판매하지 않는 상품처럼 너가 확인할 수 없는 정보에 관해 질문하는 경우\n",
    "     \n",
    "     고객이 주문을 한다면, 상품별 수량과 가격을 파악하고 총액을 계산해줘.\n",
    "     맞춤법이나 띄어쓰기가 잘못된 입력은 적절히 수정한 후에 주문 내역을 기록해줘.\n",
    "     만약 메뉴에 없는 상품에 대한 주문이 들어오면, '확인 필요'라는 글자를 기록하고 그 옆에 해당 주문 내역을 그대로 표시해줘.\n",
    "     주문 내역을 모두 파악하고 정리했으면, 다음 양식에 맞게 주문 내역을 출력해줘\n",
    "     총 상품 수량은 주문 수량*기본 판매 수량으로 구하면 돼.\n",
    "     마지막에는 \"이용해주셔서 감사합니다 ^^\"라는 문구를 출력해줘.\n",
    "     상품명| 주문 수량(총 상품 수량 개수)| 해당 상품에 대한 주문 총액\n",
    "     \"\"\"}\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [system_message,\n",
    "                {\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.events import Event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 방식으로는 임포트된 모듈 정보 파악 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 클래스는 Python의 asyncio 모듈에서 제공하는 클래스로, 비동기 이벤트를 처리하기 위해 사용됩니다. 이 클래스는 이벤트 루프와 함께 사용되며, 이벤트가 발생할 때마다 콜백 함수를 호출하여 해당 이벤트를 처리합니다.\n",
      "\n",
      "Event 클래스는 다음과 같은 주요 메서드를 제공합니다:\n",
      "\n",
      "- set(): 이벤트를 설정하고 대기 중인 모든 코루틴을 깨웁니다.\n",
      "- clear(): 이벤트를 초기화하고 대기 중인 모든 코루틴을 재설정합니다.\n",
      "- is_set(): 이벤트가 설정되었는지 여부를 확인합니다.\n",
      "- wait(): 이벤트가 설정될 때까지 대기합니다.\n",
      "\n",
      "Event 클래스는 주로 다른 코루틴들 간의 동기화를 위해 사용됩니다. 예를 들어, 한 코루틴이 특정 이벤트를 설정하고 다른 코루틴이 해당 이벤트를 기다리는 경우, 이벤트가 설정될 때까지 대기하고 이벤트가 설정되면 작업을 계속할 수 있습니다. 이를 통해 비동기 코드의 실행 흐름을 제어할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "system_message = {'role': 'system', \n",
    " 'content':\n",
    "     \"\"\"\n",
    "     너는 현재 내 코딩을 도와주고 있는 AI 비서야\n",
    "\n",
    "     \"\"\"}\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [system_message,\n",
    "                {\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "지금 import된 Evnet 클래스에 관해 설명하고 요약해봐\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "print(load_dotenv(find_dotenv(), override=True))\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'customer_userid': 'vee',\n",
       " 'conversation': ['안녕하세요~',\n",
       "  '예',\n",
       "  '말씀하세요~~',\n",
       "  '떡 주문 좀 하려고요',\n",
       "  '예 뭘로 드릴까요?',\n",
       "  '미니백설기 3, 떡케익2, 시루떡 5']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'customer_userid': \"vee\",\n",
    "        'conversation': [\n",
    "            '안녕하세요~',\n",
    "            '예',\n",
    "            '말씀하세요~~',\n",
    "            '떡 주문 좀 하려고요',\n",
    "            '예 뭘로 드릴까요?',\n",
    "            '미니백설기 3, 떡케익2, 시루떡 5'\n",
    "        ]}\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1194719741.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[22], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    prompt =\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# customer_id = \"vee\"\n",
    "# text = f\"\"\"\n",
    "# customer_id: {customer_id}\n",
    "\n",
    "# 미니백설기 3\n",
    "# 떡케익 2\n",
    "# 시루떡 4\n",
    "# \"\"\"\n",
    "\n",
    "# models 폴더\n",
    "prompt = f\"\"\"\n",
    "백틱 사이에 있는 정보를 참고해 아래와 같은 dictionary 형태로 출력해줘.\n",
    "백틱 사이에 입력되는 dictionary에서 'customer_id' keyd에 담긴 value는 고객의 인스타그램 ID를 의미하고, 'conversation' key에 담긴 value는 고객과의 대화 내용을 의미해.\n",
    "리스트의 마지막 요소가 고객이 가장 최근에 보낸 메시지이고, 그 앞의 요소는 모두 이전 대화 내용이야.\n",
    "\n",
    "출력 형태 예시를 보여줄게.\n",
    "아래에서 value 값은 해당 value 값으로 들어갈 데이터에 대한 설명이야.\n",
    "{\n",
    "    \"user\": \"고객 아이디\",\n",
    "    \"order\": \"주문 내역\",\n",
    "    \"mini_baksurgi\": \"주문 내역에 포함된 미니 백설기 개수\",\n",
    "    \"rice_cake\": \"주문 내역에 포함된 떡케익 개수\",   \n",
    "}\n",
    "\n",
    "```data```\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "백틱 사이에 있는 정보를 참고해 아래와 같은 dictionary 형태로 출력해줘.\n",
    "백틱 사이에 입력되는 dictionary에서 'customer_id' keyd에 담긴 value는 고객의 인스타그램 ID를 의미하고, 'conversation' key에 담긴 value는 고객과의 대화 내용을 의미해.\n",
    "리스트의 마지막 요소가 고객이 가장 최근에 보낸 메시지이고, 그 앞의 요소는 모두 이전 대화 내용이야.\n",
    "```{data}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ } 때문에 오류 발생했던 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n백틱 사이에 있는 정보를 참고해주세요.\\n백틱 사이에 입력되는 dictionary에서 'customer_id' keyd에 담긴 value는 고객의 인스타그램 ID를 의미하고, 'conversation' key에 담긴 value는 고객과의 대화 내용을 의미해요.\\n리스트의 마지막 요소가 고객이 가장 최근에 보낸 메시지이고, 그 앞의 요소는 모두 이전 대화 내용입니다.\\n이전 대화 내용을 참고해서 리스트의 마지막 요소에 담긴 글에 적절한 답변을 부탁해요\\n\\n원하는 출력 형태는 dictionary이고 key-value 예시를 보여줄게요.\\n아래에서 value 값은 해당 value 값으로 들어갈 데이터에 대한 설명이에요.\\n'user': '고객 아이디',\\n'order': '주문 내역',\\n'mini_baksurgi': '주문 내역에 포함된 미니 백설기 개수',\\n'rice_cake': '주문 내역에 포함된 떡케익 개수',\\n\\n```{'customer_userid': 'vee', 'conversation': ['안녕하세요~', '예', '말씀하세요~~', '떡 주문 좀 하려고요', '예 뭘로 드릴까요?', '미니백설기 3, 떡케익2, 시루떡 5']}```\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = f\"\"\"\n",
    "백틱 사이에 있는 정보를 참고해주세요.\n",
    "백틱 사이에 입력되는 dictionary에서 'customer_id' keyd에 담긴 value는 고객의 인스타그램 ID를 의미하고, 'conversation' key에 담긴 value는 고객과의 대화 내용을 의미해요.\n",
    "리스트의 마지막 요소가 고객이 가장 최근에 보낸 메시지이고, 그 앞의 요소는 모두 이전 대화 내용입니다.\n",
    "이전 대화 내용을 참고해서 리스트의 마지막 요소에 담긴 글에 적절한 답변을 부탁해요\n",
    "\n",
    "원하는 출력 형태는 dictionary이고 key-value 예시를 보여줄게요.\n",
    "아래에서 value 값은 해당 value 값으로 들어갈 데이터에 대한 설명이에요.\n",
    "'user': '고객 아이디',\n",
    "'order': '주문 내역',\n",
    "'mini_baksurgi': '주문 내역에 포함된 미니 백설기 개수',\n",
    "'rice_cake': '주문 내역에 포함된 떡케익 개수',\n",
    "\n",
    "```{data}```\n",
    "\"\"\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'customer_userid': 'vee', 'conversation': ['안녕하세요~', '예', '말씀하세요~~', '떡 주문 좀 하려고요', '예 뭘로 드릴까요?', '미니백설기 3, 떡케익2, 시루떡 5']}\n",
      "\n",
      "고객 아이디: vee\n",
      "주문 내역: 미니백설기 3, 떡케익2, 시루떡 5\n",
      "\n",
      "주문 내역을 확인하고 계산해보겠습니다.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'customer_userid': 'vee', 'conversation': ['안녕하세요~', '예', '말씀하세요~~', '떡 주문 좀 하려고요', '예 뭘로 드릴까요?', '미니백설기 3, 떡케익2, 시루떡 5']}\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'customer_userid': 'vee', 'conversation': ['안녕하세요~', '예', '말씀하세요~~', '떡 주문 좀 하려고요', '예 뭘로 드릴까요?', '미니백설기 3, 떡케익2, 시루떡 5']}\n",
      "\n",
      "고객 아이디: vee\n",
      "주문 내역: 미니백설기 3, 떡케익2, 시루떡 5\n",
      "\n",
      "주문 내역을 확인하고 계산해보겠습니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# models 폴더\n",
    "format_prompt = f\"\"\"\n",
    "백틱 사이에 있는 정보를 참고해 아래와 같은 dictionary 형태로 출력해줘.\n",
    "아래에서 value 값은 해당 value 값으로 들어갈 데이터에 대한 설명이야.\n",
    "\"user\": \"고객 아이디\",\n",
    "\"order\": \"주문 내역\",\n",
    "\"mini_baksurgi\": \"주문 내역에 포함된 미니 백설기 개수\",\n",
    "\"rice_cake\": \"주문 내역에 포함된 떡케익 개수\",\n",
    "\n",
    "```{response}```\n",
    "\"\"\"\n",
    "\n",
    "result = get_completion(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chain of thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "질문\n",
    "- step 1 등에 나오는 delimiter 왜 넣는 거지? 안 넣어도 되지 않나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "system_message = f\"\"\"\n",
    "아래 단계들에 따라 고객 문의에 답변해줘.\n",
    "고객 문의는 4개의 해시태그로 구분돼. \n",
    "즉, {delimiter}로 구분돼.\n",
    "\n",
    "Step 1:{delimiter} 우선 고객 문의가 특정 상품에 관한 것인지 판단해줘.\n",
    "\n",
    "Step 2:{delimiter} 만약 고객 문의가 특정 상품에 관한 문의가 아니라 일반적인 문의라면 적절히 답변하며 상품 문의로 이어지게 해줘.\n",
    "\n",
    "Step 3:{delimiter} 만약 고객이 특정 상품에 관해 묻는다면, 고객이 문의한 상품명이 아래 목록에 포함된 상품명과 정확히 일치하는지 확인하고 \\\n",
    "정확히 일치하지 않을 경우 Step 4로 넘어가고, 정확히 일치할 경우 Step 5로 넘어가.\n",
    "\n",
    "Step 4:{delimiter} 만약 고객이 묻는 상품명이 아래 목록에 포함된 상품명과 비슷하지만 정확히 일치하지 않는다면, \\\n",
    "비슷한 상품명을 모두 고객에게 보여주고 어떤 상품에 관해 묻는 것인지 정확히 확인해줘.\n",
    "\n",
    "모든 상품 목록은 아래에 있어.\n",
    "1. 상품: 떡케익5호\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 54,000원\n",
    "2. 상품: 무지개 백설기 케익\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 51,500원\n",
    "3. 상품: 미니 백설기\n",
    "   기본 판매 수량: 35개\n",
    "   기본 판매 수량의 가격: 31,500원\n",
    "4. 상품: 개별 모듬팩\n",
    "   기본 판매 수량: 1개\n",
    "   기본 판매 수량의 가격: 13,500원\n",
    "\n",
    "Step 5:{delimiter} 고객이 문의한 상품의 이름, 기본 판매 수량, 기본 판매 수량의 가격을 고객에게 보여줘.\n",
    "\n",
    "다음 형식을 사용해줘:\n",
    "Step 1:{delimiter} <step 1 reasoning>\n",
    "Step 2:{delimiter} <step 2 reasoning>\n",
    "Step 3:{delimiter} <step 3 reasoning>\n",
    "Step 4:{delimiter} <step 4 reasoning>\n",
    "Step 5:{delimiter} <step 5 reasoning>\n",
    "Response to user:{delimiter} <response to customer>\n",
    "\n",
    "Make sure to include {delimiter} to separate every step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = f\"\"\"\n",
    "문의 좀 할게요\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step4를 깜빡했는데, step3 밑에 부분을 step4간주했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:#### 고객 문의가 특정 상품에 관한 것인지 판단해줘.\n",
      "Step 2:#### 만약 고객 문의가 특정 상품에 관한 문의가 아니라면 적절히 답변하며 상품 문의로 이어지게 해줘.\n",
      "Step 3:#### 만약 고객이 특정 상품에 관해 묻는다면, 그 상품이 아래 목록에 포함됐는지 확인해줘.\n",
      "Step 4:#### 만약 고객이 묻는 상품명이 아래 목록에 포함된 상품과 비슷하지만 정확히 일치하지 않는다면, 그 비슷한 상품명을 고객에게 보여주고 이 상품에 관해 묻는 것이 맞는지 확인해줘.\n",
      "\n",
      "Response to user:#### 어떤 문의가 있으신가요? 상품에 관한 문의인지 아니면 일반적인 문의인지 알려주세요.\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:#### 고객 문의가 특정 상품에 관한 것인지 판단해줘.\n",
      "고객 문의는 \"떡케익 가격 좀 알고 싶어서요\"라고 되어 있습니다. 이 문의는 특정 상품인 \"떡케익\"에 관한 것입니다.\n",
      "\n",
      "Step 2:#### 만약 고객 문의가 특정 상품에 관한 문의가 아니라면 적절히 답변하며 상품 문의로 이어지게 해줘.\n",
      "고객 문의가 특정 상품인 \"떡케익\"에 관한 문의이므로, 상품 문의로 이어집니다.\n",
      "\n",
      "Step 3:#### 만약 고객이 특정 상품에 관해 묻는다면, 그 상품이 아래 목록에 포함됐는지 확인해줘.\n",
      "\"떡케익\"은 아래 상품 목록에 포함되어 있습니다.\n",
      "\n",
      "1. 상품: 떡케익5호\n",
      "   기본 판매 수량: 1개\n",
      "   기본 판매 수량의 가격: 54,000원\n",
      "\n",
      "Step 4:#### <step 4 reasoning>\n",
      "고객이 묻는 상품인 \"떡케익\"은 상품 목록에 포함되어 있습니다. 따라서, 떡케익의 가격은 54,000원입니다.\n",
      "\n",
      "Response to user:#### 떡케익의 가격은 54,000원입니다. 추가로 궁금한 사항이 있으시면 언제든지 물어보세요.\n"
     ]
    }
   ],
   "source": [
    "user_message = f\"\"\"\n",
    "떡케익 가격 좀 알고 싶어서요\"\"\"\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "] \n",
    "\n",
    "response = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:#### 고객 문의가 특정 상품에 관한 것인지 판단해줘.\n",
      "고객 문의는 \"백설기는 가격이 어떻게 하나요?\"라는 문장으로 시작한다. 이 문장은 특정 상품에 관한 문의이다.\n",
      "\n",
      "Step 2:#### 만약 고객 문의가 특정 상품에 관한 문의라면 적절히 답변하며 상품 문의로 이어지게 해줘.\n",
      "고객 문의는 백설기의 가격에 관한 것이므로, 백설기에 대한 가격 정보를 제공해야 한다.\n",
      "\n",
      "Step 3:#### 만약 고객이 특정 상품에 관해 묻는다면, 그 상품이 아래 목록에 포함됐는지 확인해줘.\n",
      "백설기는 상품 목록에 포함돼 있다.\n",
      "\n",
      "Step 4:#### 만약 고객이 문의한 상품이 상품 목록에 포함돼 있다면, 해당 상품의 이름, 기본 판매 수량, 기본 판매 수량의 가격을 고객에게 보여줘.\n",
      "백설기의 이름은 \"무지개 백설기 케익\"이고, 기본 판매 수량은 1개이며, 기본 판매 수량의 가격은 51,500원이다.\n",
      "\n",
      "Response to user:#### 무지개 백설기 케익은 1개에 51,500원입니다. 어떤 점이 궁금하신가요?\n"
     ]
    }
   ],
   "source": [
    "# step4 추가 후 \n",
    "\n",
    "user_message = f\"\"\"\n",
    "백설기는 가격이 어떻게 하나요?\"\"\"\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "] \n",
    "\n",
    "response = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:#### 고객 문의가 특정 상품에 관한 것인지 판단해줘.\n",
      "고객 문의는 \"백설기는 가격이 어떻게 하나요?\"로 구성돼 있습니다. 이는 특정 상품에 관한 문의입니다.\n",
      "\n",
      "Step 2:#### 만약 고객 문의가 특정 상품에 관한 문의가 아니라면 적절히 답변하며 상품 문의로 이어지게 해줘.\n",
      "고객 문의는 특정 상품에 관한 문의이므로, Step 2는 필요하지 않습니다.\n",
      "\n",
      "Step 3:#### 만약 고객이 특정 상품에 관해 묻는다면, 고객이 문의한 상품명이 아래 목록에 포함된 상품명과 정확히 일치하는지 확인하고 정확히 일치하지 않을 경우 Step 4로 넘어가고, 정확히 일치할 경우 Step 5로 넘어가.\n",
      "고객이 문의한 상품명 \"백설기\"는 목록에 포함된 상품명 중 하나와 정확히 일치하지 않습니다. 따라서 Step 4로 넘어갑니다.\n",
      "\n",
      "Step 4:#### 만약 고객이 묻는 상품명이 아래 목록에 포함된 상품명과 비슷하지만 정확히 일치하지 않는다면, 비슷한 상품명을 모두 고객에게 보여주고 어떤 상품에 관해 묻는 것인지 정확히 확인해줘.\n",
      "고객이 문의한 상품명 \"백설기\"는 비슷한 상품명이 없\n"
     ]
    }
   ],
   "source": [
    "# 백설기 케익과 미니 백설기가 중 어떤 것인지 물어보지 않아 프롬프트 살짝 수정 후 다시 질문\n",
    "\n",
    "user_message = f\"\"\"\n",
    "백설기는 가격이 어떻게 하나요?\"\"\"\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "] \n",
    "\n",
    "response = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt parser 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0301', temperature=0.0, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gift_schema = ResponseSchema(name=\"user\",\n",
    "#                              description=\"Was the item purchased\\\n",
    "#                              as a gift for someone else? \\\n",
    "#                              Answer True if yes,\\\n",
    "#                              False if not or unknown.\")\n",
    "ricecake_cnt_schema = ResponseSchema(name=\"ricecake_cnt\",\n",
    "                                      description=\"주문한 떡케익의 수량 \\\n",
    "                                          만약 주문한 떡케익이 없다면 -1로 출력.\")\n",
    "rainbow_bagsulgi_cnt__schema = ResponseSchema(name=\"rainbow_bagsulgi_cnt\",\n",
    "                                    description=\"주문한 무지개 백설기 케익의 수량 \\\n",
    "                                        만약 주문한 무지개 백설기 케익이 없다면 -1로 출력.\")\n",
    "mini_bagsulgi_cnt__schema = ResponseSchema(name=\"mini_bagsulgi_cnt\",\n",
    "                                    description=\"주문한 미니 백설기 케익의 수량 \\\n",
    "                                        만약 주문한 미니 백설기 케익이 없다면 -1로 출력.\")\n",
    "pack_cnt__schema = ResponseSchema(name=\"pack_cnt\",\n",
    "                                    description=\"주문한 개별 모듬팩 수량 \\\n",
    "                                        만약 주문한 개별 모듬팩이 없다면 -1로 출력.\")\n",
    "\n",
    "response_schemas = [ricecake_cnt_schema , \n",
    "                    rainbow_bagsulgi_cnt__schema,\n",
    "                    mini_bagsulgi_cnt__schema,\n",
    "                    pack_cnt__schema ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredOutputParser(response_schemas=[ResponseSchema(name='ricecake_cnt', description='주문한 떡케익의 수량                                           만약 주문한 떡케익이 없다면 -1로 출력.', type='string'), ResponseSchema(name='rainbow_bagsulgi_cnt', description='주문한 무지개 백설기 케익의 수량                                         만약 주문한 무지개 백설기 케익이 없다면 -1로 출력.', type='string'), ResponseSchema(name='mini_bagsulgi_cnt', description='주문한 미니 백설기 케익의 수량                                         만약 주문한 미니 백설기 케익이 없다면 -1로 출력.', type='string'), ResponseSchema(name='pack_cnt', description='주문한 개별 모듬팩 수량                                         만약 주문한 개별 모듬팩이 없다면 -1로 출력.', type='string')])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"ricecake_cnt\": string  // 주문한 떡케익의 수량                                           만약 주문한 떡케익이 없다면 -1로 출력.\\n\\t\"rainbow_bagsulgi_cnt\": string  // 주문한 무지개 백설기 케익의 수량                                         만약 주문한 무지개 백설기 케익이 없다면 -1로 출력.\\n\\t\"mini_bagsulgi_cnt\": string  // 주문한 미니 백설기 케익의 수량                                         만약 주문한 미니 백설기 케익이 없다면 -1로 출력.\\n\\t\"pack_cnt\": string  // 주문한 개별 모듬팩 수량                                         만약 주문한 개별 모듬팩이 없다면 -1로 출력.\\n}\\n```'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_message = \"\"\"\\\n",
    "아 그럼 떡케익5호짜리 2개, 무지개 백설기 케익 1개, 미니 백설기 1, 개별 모듬팩 2개 할게요\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='뒤따르는 텍스트에서 다음 정보를 추출해주세요:\\n\\nricecake_cnt: 주문한 떡케익의 수량. 만약 주문한 떡케익이 없다면 -1로 출력.\\n\\nrainbow_bagsulgi_cnt: 주문한 무지개 백설기 케익의 수량. 만약 주문한 무지개 백설기 케익이 없다면 -1로 출력.\\n\\nmini_bagsulgi_cnt: 주문한 미니 백설기 케익의 수량. 만약 주문한 미니 백설기 케익이 없다면 -1로 출력.\\n\\npack_cnt: 주문한 개별 모듬팩 수량. 만약 주문한 개별 모듬팩이 없다면 -1로 출력.\\n\\ntext: 아 그럼 떡케익5호짜리 2개, 무지개 백설기 케익 1개, 미니 백설기 1, 개별 모듬팩 2개 할게요\\n\\n\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"riceckae_cnt\": string  // 주문한 떡케익의 수량                                           만약 주문한 떡케익이 없다면 -1로 출력.\\n\\t\"rainbow_bagsulgi_cnt_\": string  // 주문한 무지개 백설기 케익의 수량                                         만약 주문한 무지개 백설기 케익이 없다면 -1로 출력.\\n\\t\"mini_bagsulgi_cnt_\": string  // 주문한 미니 백설기 케익의 수량                                         만약 주문한 미니 백설기 케익이 없다면 -1로 출력.\\n\\t\"pack_cnt_\": string  // 주문한 개별 모듬팩 수량                                         만약 주문한 개별 모듬팩이 없다면 -1로 출력.\\n}\\n```\\n')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "뒤따르는 텍스트에서 다음 정보를 추출해주세요:\n",
    "\n",
    "ricecake_cnt: 주문한 떡케익의 수량. 만약 주문한 떡케익이 없다면 -1로 출력.\n",
    "\n",
    "rainbow_bagsulgi_cnt: 주문한 무지개 백설기 케익의 수량. 만약 주문한 무지개 백설기 케익이 없다면 -1로 출력.\n",
    "\n",
    "mini_bagsulgi_cnt: 주문한 미니 백설기 케익의 수량. 만약 주문한 미니 백설기 케익이 없다면 -1로 출력.\n",
    "\n",
    "pack_cnt: 주문한 개별 모듬팩 수량. 만약 주문한 개별 모듬팩이 없다면 -1로 출력.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_message, \n",
    "                                format_instructions=format_instructions)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\\n\\t\"ricecake_cnt\": \"2\",\\n\\t\"rainbow_bagsulgi_cnt\": \"1\",\\n\\t\"mini_bagsulgi_cnt\": \"1\",\\n\\t\"pack_cnt\": \"2\"\\n}\\n```')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"ricecake_cnt\": \"2\",\n",
      "\t\"rainbow_bagsulgi_cnt\": \"1\",\n",
      "\t\"mini_bagsulgi_cnt\": \"1\",\n",
      "\t\"pack_cnt\": \"2\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ricecake_cnt': '2',\n",
       " 'rainbow_bagsulgi_cnt': '1',\n",
       " 'mini_bagsulgi_cnt': '1',\n",
       " 'pack_cnt': '2'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "print(type(output_dict))\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type HumanMessage is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\test_231023.ipynb Cell 43\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m messages \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mformat_messages(text\u001b[39m=\u001b[39mcustomer_message, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                                 format_instructions\u001b[39m=\u001b[39mformat_instructions)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# messages =  [  \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# {'role':'system', \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m#  'content': system_message},    \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# {'role':'user', \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m#  'content': f\"{delimiter}{user_message}{delimiter}\"},  \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# ] \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m response \u001b[39m=\u001b[39m get_completion_from_messages(messages)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "\u001b[1;32md:\\Repository\\order_assistant\\workspace\\test_231023.ipynb Cell 43\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_completion_from_messages\u001b[39m(messages, \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                  model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                  temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, max_tokens\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repository/order_assistant/workspace/test_231023.ipynb#Y104sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    139\u001b[0m ):\n\u001b[0;32m    140\u001b[0m     (\n\u001b[0;32m    141\u001b[0m         deployment_id,\n\u001b[0;32m    142\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    153\u001b[0m     )\n\u001b[1;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    157\u001b[0m         url,\n\u001b[0;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\openai\\api_requestor.py:289\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    280\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[0;32m    290\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[0;32m    291\u001b[0m         url,\n\u001b[0;32m    292\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    293\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    294\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    295\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    296\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    298\u001b[0m     )\n\u001b[0;32m    299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\openai\\api_requestor.py:591\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest_raw\u001b[39m(\n\u001b[0;32m    580\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    581\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    590\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m requests\u001b[39m.\u001b[39mResponse:\n\u001b[1;32m--> 591\u001b[0m     abs_url, headers, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_request_raw(\n\u001b[0;32m    592\u001b[0m         url, supplied_headers, method, params, files, request_id\n\u001b[0;32m    593\u001b[0m     )\n\u001b[0;32m    595\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(_thread_context, \u001b[39m\"\u001b[39m\u001b[39msession\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    596\u001b[0m         _thread_context\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m _make_session()\n",
      "File \u001b[1;32md:\\Repository\\order_assistant\\lib\\site-packages\\openai\\api_requestor.py:563\u001b[0m, in \u001b[0;36mAPIRequestor._prepare_request_raw\u001b[1;34m(self, url, supplied_headers, method, params, files, request_id)\u001b[0m\n\u001b[0;32m    561\u001b[0m         data \u001b[39m=\u001b[39m params\n\u001b[0;32m    562\u001b[0m     \u001b[39mif\u001b[39;00m params \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m files:\n\u001b[1;32m--> 563\u001b[0m         data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mdumps(params)\u001b[39m.\u001b[39mencode()\n\u001b[0;32m    564\u001b[0m         headers[\u001b[39m\"\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39m# cached encoder\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m skipkeys \u001b[39mand\u001b[39;00m ensure_ascii \u001b[39mand\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     check_circular \u001b[39mand\u001b[39;00m allow_nan \u001b[39mand\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m indent \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m separators \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     default \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sort_keys \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_encoder\u001b[39m.\u001b[39;49mencode(obj)\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\json\\encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[39mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    196\u001b[0m \u001b[39m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[39m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterencode(o, _one_shot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m    201\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\json\\encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     _iterencode \u001b[39m=\u001b[39m _make_iterencode(\n\u001b[0;32m    254\u001b[0m         markers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault, _encoder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent, floatstr,\n\u001b[0;32m    255\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_keys,\n\u001b[0;32m    256\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m _iterencode(o, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\json\\encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[0;32m    161\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    180\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type HumanMessage is not JSON serializable"
     ]
    }
   ],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "뒤따르는 텍스트에서 다음 정보를 추출해주세요:\n",
    "\n",
    "ricecake_cnt: 주문한 떡케익의 수량. 만약 주문한 떡케익이 없다면 -1로 출력.\n",
    "\n",
    "rainbow_bagsulgi_cnt: 주문한 무지개 백설기 케익의 수량. 만약 주문한 무지개 백설기 케익이 없다면 -1로 출력.\n",
    "\n",
    "mini_bagsulgi_cnt: 주문한 미니 백설기 케익의 수량. 만약 주문한 미니 백설기 케익이 없다면 -1로 출력.\n",
    "\n",
    "pack_cnt: 주문한 개별 모듬팩 수량. 만약 주문한 개별 모듬팩이 없다면 -1로 출력.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_message, \n",
    "                                format_instructions=format_instructions)\n",
    "\n",
    "# messages =  [  \n",
    "# {'role':'system', \n",
    "#  'content': system_message},    \n",
    "# {'role':'user', \n",
    "#  'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "# ] \n",
    "\n",
    "response = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['format_instructions', 'text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['format_instructions', 'text'], template='뒤따르는 텍스트에서 다음 정보를 추출해주세요:\\n\\nricecake_cnt: 주문한 떡케익의 수량. 만약 주문한 떡케익이 없다면 -1로 출력.\\n\\nrainbow_bagsulgi_cnt: 주문한 무지개 백설기 케익의 수량. 만약 주문한 무지개 백설기 케익이 없다면 -1로 출력.\\n\\nmini_bagsulgi_cnt: 주문한 미니 백설기 케익의 수량. 만약 주문한 미니 백설기 케익이 없다면 -1로 출력.\\n\\npack_cnt: 주문한 개별 모듬팩 수량. 만약 주문한 개별 모듬팩이 없다면 -1로 출력.\\n\\ntext: {text}\\n\\n{format_instructions}\\n'))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"ricecake_cnt\": string  // 주문한 떡케익의 수량                                           만약 주문한 떡케익이 없다면 -1로 출력.\\n\\t\"rainbow_bagsulgi_cnt\": string  // 주문한 무지개 백설기 케익의 수량                                         만약 주문한 무지개 백설기 케익이 없다면 -1로 출력.\\n\\t\"mini_bagsulgi_cnt\": string  // 주문한 미니 백설기 케익의 수량                                         만약 주문한 미니 백설기 케익이 없다면 -1로 출력.\\n\\t\"pack_cnt\": string  // 주문한 개별 모듬팩 수량                                         만약 주문한 개별 모듬팩이 없다면 -1로 출력.\\n}\\n```'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='뒤따르는 텍스트에서 다음 정보를 추출해주세요:\\n\\nricecake_cnt: 주문한 떡케익의 수량. 만약 주문한 떡케익이 없다면 -1로 출력.\\n\\nrainbow_bagsulgi_cnt: 주문한 무지개 백설기 케익의 수량. 만약 주문한 무지개 백설기 케익이 없다면 -1로 출력.\\n\\nmini_bagsulgi_cnt: 주문한 미니 백설기 케익의 수량. 만약 주문한 미니 백설기 케익이 없다면 -1로 출력.\\n\\npack_cnt: 주문한 개별 모듬팩 수량. 만약 주문한 개별 모듬팩이 없다면 -1로 출력.\\n\\ntext: 아 그럼 떡케익5호짜리 2개, 무지개 백설기 케익 1개, 미니 백설기 1, 개별 모듬팩 2개 할게요\\n\\n\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"ricecake_cnt\": string  // 주문한 떡케익의 수량                                           만약 주문한 떡케익이 없다면 -1로 출력.\\n\\t\"rainbow_bagsulgi_cnt\": string  // 주문한 무지개 백설기 케익의 수량                                         만약 주문한 무지개 백설기 케익이 없다면 -1로 출력.\\n\\t\"mini_bagsulgi_cnt\": string  // 주문한 미니 백설기 케익의 수량                                         만약 주문한 미니 백설기 케익이 없다면 -1로 출력.\\n\\t\"pack_cnt\": string  // 주문한 개별 모듬팩 수량                                         만약 주문한 개별 모듬팩이 없다면 -1로 출력.\\n}\\n```\\n')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
